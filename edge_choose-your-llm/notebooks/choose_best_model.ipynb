{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f113298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Measuring Local Resources...\n",
      "\n",
      "---------------------------------------- CPU Info ----------------------------------------\n",
      "Physical cores: 10\n",
      "Total cores: 10\n",
      "CPU frequency information not available via psutil, trying sysctl...\n",
      "\n",
      "Unexpected output format from sysctl.\n",
      "---------------------------------------- Memory Information ----------------------------------------\n",
      "Total: 16.00GB\n",
      "Available: 6.25GB\n",
      "No NVIDIA GPU detected\n",
      "---------------------------------------- Python Version ----------------------------------------\n",
      "3.11.5\n",
      "---------------------------------------- PyTorch Version ----------------------------------------\n",
      "2.1.0\n",
      "---------------------------------------- TensorFlow Version ----------------------------------------\n",
      "2.14.0\n",
      "\n",
      "Measuring Local Resources Finish!\n",
      "==============================================================================================================\n",
      "\n",
      "\n",
      "==============================================================================================================\n",
      "Scraping HuggingFace open-llm-leaderboard (https://huggingfaceh4-open-llm-leaderboard.hf.space)...\n",
      "\n",
      "Scraping HuggingFace open-llm-leaderboard Finish!\n",
      "See best-models.txt and best-models-deduplicate.txt for best models for each size and each kind.\n",
      "==============================================================================================================\n",
      "\n",
      "\n",
      "==============================================================================================================\n",
      "Data Cleaning starts...\n",
      "\n",
      "Data Cleaning Finish!\n",
      "==============================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import get_model_memory\n",
    "import get_resources_info\n",
    "from Scraper import scrape\n",
    "\n",
    "'''\n",
    "use get_resources_info.py to get if gpu and memory\n",
    "use scrape.py to get update the leaderboard and return sorted model list\n",
    "use available memory to find the best model in range\n",
    "deploy\n",
    "'''\n",
    "\n",
    "# get_resources_info.py\n",
    "print(f'{\"=\"*110}\\nMeasuring Local Resources...\\n')\n",
    "local_resources = get_resources_info.get_resources_info()\n",
    "have_gpu = False if local_resources[7] is None else True\n",
    "if have_gpu:\n",
    "    #gpu_info.append([gpu_id, gpu_name, gpu_memory_total, gpu_memory_free, gpu_load])\n",
    "    if len(local_resources[7]) > 1:\n",
    "        pass #TODO: address multiple GPU cases\n",
    "    else:\n",
    "        gpu_name, gpu_memory_total, gpu_memory_free = local_resources[7][0][1], local_resources[7][0][2], local_resources[7][0][3]\n",
    "else:\n",
    "    mem_total, mem_available = local_resources[5], local_resources[6]\n",
    "print(f'\\nMeasuring Local Resources Finish!\\n{\"=\"*110}\\n\\n')\n",
    "    \n",
    "# scrape.py\n",
    "print(f'{\"=\"*110}\\nScraping HuggingFace open-llm-leaderboard (https://huggingfaceh4-open-llm-leaderboard.hf.space)...')\n",
    "# scrape.scrape() # update leaderboard\n",
    "print(f'\\nScraping HuggingFace open-llm-leaderboard Finish!\\nSee best-models.txt and best-models-deduplicate.txt for best models for each size and each kind.\\n{\"=\"*110}\\n\\n')\n",
    "\n",
    "print(f'{\"=\"*110}\\nData Cleaning starts...')\n",
    "# load leaderboard data\n",
    "data = pd.read_csv('open-llm-leaderboard.csv') # read the full leaderboard\n",
    "# data cleaning\n",
    "\n",
    "# fixing a mistake for microsoft's phi-1_5\n",
    "data.loc[data['Model_name_for_query']=='microsoft/phi-1_5', '#Params (B)'] = 1.3\n",
    "# fixing a mistake for roneneldan/TinyStories-1M\n",
    "data.loc[data['Model_name_for_query']=='roneneldan/TinyStories-1M', '#Params (B)'] = 0.001\n",
    "# removing models with size 0\n",
    "data = data.loc[data['#Params (B)'] != 0]\n",
    "# change the type of lamini 774M to Finetuned\n",
    "data.loc[data['Model_name_for_query']=='MBZUAI/LaMini-GPT-774M', 'Type'] = \"fine-tuned\"\n",
    "print(f'\\nData Cleaning Finish!\\n{\"=\"*110}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e14495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['Available_on_the_hub'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf940da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model_repo</th>\n",
       "      <th>Model_experiment_details</th>\n",
       "      <th>Average</th>\n",
       "      <th>ARC</th>\n",
       "      <th>HellaSwag</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>TruthfulQA</th>\n",
       "      <th>Winogrande</th>\n",
       "      <th>GSM8K</th>\n",
       "      <th>DROP</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Hub_License</th>\n",
       "      <th>#Params (B)</th>\n",
       "      <th>Hub_like</th>\n",
       "      <th>Available_on_the_hub</th>\n",
       "      <th>Model_sha</th>\n",
       "      <th>Model_name_for_query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>pretrained</td>\n",
       "      <td>https://huggingface.co/microsoft/phi-1_5</td>\n",
       "      <td>https://huggingface.co/datasets/open-llm-leade...</td>\n",
       "      <td>41.6</td>\n",
       "      <td>52.9</td>\n",
       "      <td>63.79</td>\n",
       "      <td>43.89</td>\n",
       "      <td>40.89</td>\n",
       "      <td>72.22</td>\n",
       "      <td>12.43</td>\n",
       "      <td>5.04</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>other</td>\n",
       "      <td>1.3</td>\n",
       "      <td>916.0</td>\n",
       "      <td>True</td>\n",
       "      <td>ea95720a352172db6fcbcd89032bfb1cb8481797</td>\n",
       "      <td>microsoft/phi-1_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type                                Model_repo  \\\n",
       "1025  pretrained  https://huggingface.co/microsoft/phi-1_5   \n",
       "\n",
       "                               Model_experiment_details  Average   ARC  \\\n",
       "1025  https://huggingface.co/datasets/open-llm-leade...     41.6  52.9   \n",
       "\n",
       "      HellaSwag   MMLU  TruthfulQA  Winogrande  GSM8K  DROP       Precision  \\\n",
       "1025      63.79  43.89       40.89       72.22  12.43  5.04  torch.bfloat16   \n",
       "\n",
       "     Hub_License  #Params (B)  Hub_like Available_on_the_hub  \\\n",
       "1025       other          1.3     916.0                 True   \n",
       "\n",
       "                                     Model_sha Model_name_for_query  \n",
       "1025  ea95720a352172db6fcbcd89032bfb1cb8481797    microsoft/phi-1_5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['Model_name_for_query'] == \"microsoft/phi-1_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0c2ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Please input your desired task types...\n",
      "Train or Inference: inf\n",
      "Quantization method: no_quant\n",
      "Prompt length in tokens: 300\n",
      "Output length in tokens: 300\n",
      "Batch Size: 1\n",
      "\n",
      "Based on your input, the task variables are \n",
      "      train_or_inference: inf, \n",
      "      train_method: None,\n",
      "      optimizer: None, \n",
      "      gradient_checkpointing: None,\n",
      "      quant: no_quant, \n",
      "      prompt_len: 300, \n",
      "      tokens_to_generate: 300, \n",
      "      batch_size: 1 \n",
      "      \n",
      "==============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to input something\n",
    "# TODO: Refine the logic: skip character and more while loops\n",
    "print(f'{\"=\"*110}\\nPlease input your desired task types...')\n",
    "train_or_inference = input(\"Train or Inference: \")\n",
    "while (train_or_inference not in {'inf', 'inf_vLLM', 'inf_ggml', 'trn'}):\n",
    "    train_or_inference = input(\"Invalid input. Please pick from {'inf', 'inf_vLLM', 'inf_ggml', 'trn'}.\\nTrain or Inference: \")\n",
    "if train_or_inference == 'trn':\n",
    "    train_method = input(\"Train method: \")\n",
    "    while (train_method not in {'full_trn','lora_trn','qlora'}):\n",
    "        train_method = input(\"Invalid input. Please pick from {'full_trn','lora_trn','qlora'}.\\nTrain method: \")\n",
    "    optimizer = input(\"Optimizer: \")\n",
    "    while (optimizer not in {'adam_opt', 'sgd_opt'}):\n",
    "        optimizer = input(\"Invalid input. Please pick from {'adam_opt', 'sgd_opt'}.\\nOptimizer: \")\n",
    "    gradient_checkpointing = True if input(\"Gradient checkpointing? {'y', 'n'} \")==\"y\" else False\n",
    "    quant, prompt_len, tokens_to_generate = None, None, 1\n",
    "else: # inference\n",
    "    quant = input(\"Quantization method: \")\n",
    "    while (quant not in {'no_quant', 'bnb_int8', 'bnb_q4', 'ggml_Q2_K', 'ggml_Q3_K_L','ggml_Q3_K_M', 'ggml_QK4_0','ggml_QK4_1','ggml_QK4_K_M','ggml_QK4_K_S', 'ggml_QK5_0', 'ggml_QK5_1', 'ggml_QK5_K_M', 'ggml_Q6_K', 'ggml_QK8_0'}):\n",
    "        quant = input(\"Invalid input. Please pick from {'no_quant', 'bnb_int8', 'bnb_q4', 'ggml_Q2_K', 'ggml_Q3_K_L','ggml_Q3_K_M', 'ggml_QK4_0','ggml_QK4_1','ggml_QK4_K_M','ggml_QK4_K_S', 'ggml_QK5_0', 'ggml_QK5_1', 'ggml_QK5_K_M', 'ggml_Q6_K', 'ggml_QK8_0'}.\\nQuantization method: \")\n",
    "    try:\n",
    "        prompt_len = int(input(\"Prompt length in tokens: \"))\n",
    "        tokens_to_generate = int(input(\"Output length in tokens: \"))\n",
    "    except Exception as e:\n",
    "        print(f'prompt_len and output_len should be positive int. We will proceed with default value:{300, 300}')\n",
    "        prompt_len, tokens_to_generate = 300, 300\n",
    "    train_method, optimizer, gradient_checkpointing = None, None, None\n",
    "batch_size = int(input(\"Batch Size: \")) # modify\n",
    "print(f'\\nBased on your input, the task variables are \\n\\\n",
    "      train_or_inference: {train_or_inference}, \\n\\\n",
    "      train_method: {train_method},\\n\\\n",
    "      optimizer: {optimizer}, \\n\\\n",
    "      gradient_checkpointing: {gradient_checkpointing},\\n\\\n",
    "      quant: {quant}, \\n\\\n",
    "      prompt_len: {prompt_len}, \\n\\\n",
    "      tokens_to_generate: {tokens_to_generate}, \\n\\\n",
    "      batch_size: {batch_size} \\n\\\n",
    "      \\n{\"=\"*110}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49ee39a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Data Augmentation using get_model_memory starts...\n",
      "\n",
      "Data Augmentation Finish!\n",
      "==============================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"=\"*110}\\nData Augmentation using get_model_memory starts...')\n",
    "# Wrapper function for applying get_model_memory with error handling\n",
    "# TODO: enable users to choose training or inference\n",
    "def apply_get_model_memory(row):\n",
    "    try:\n",
    "        return get_model_memory.findMemoryRequirement(row['Model_name_for_query'], train_or_inference, train_method, optimizer, quant, prompt_len, tokens_to_generate, batch_size, gradient_checkpointing)['Total']\n",
    "    except Exception as e:\n",
    "        # assign a negative value as indicator\n",
    "        return -1\n",
    "\n",
    "# Apply the function to each row and create a new column\n",
    "data['Memory'] = data.apply(apply_get_model_memory, axis=1)\n",
    "print(f'\\nData Augmentation Finish!\\n{\"=\"*110}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7f688b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['Memory'] != -1] # TODO: give estimates to models not in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e1f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(models):\n",
    "    best_model_names = set()\n",
    "    for i in range(len(seperation)+1):\n",
    "        low = seperation[i-1] if i != 0 else 0\n",
    "        high = seperation[i] if i != len(seperation) else None\n",
    "        sub_models = models[models['#Params (B)'] >= low]\n",
    "        if high: sub_models = sub_models[sub_models['#Params (B)'] < high] # if high is not None\n",
    "        if len(sub_models) == 0: continue # skip if there are no models within this size range\n",
    "        max_score_index = sub_models['Average'].idxmax()\n",
    "        print(f'({low},{high})', sub_models.loc[max_score_index][\"Model_name_for_query\"])\n",
    "        best_model_names.add(((low, high), sub_models.loc[max_score_index][\"Model_name_for_query\"]))\n",
    "    return best_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ade3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================\n",
      "Searching for best models...\n",
      "\n",
      "Best Model Found!\n",
      "==============================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"=\"*110}\\nSearching for best models...')\n",
    "if have_gpu:\n",
    "    deployable_models = data[data['Memory'] < gpu_memory_free*1024]\n",
    "else:\n",
    "    deployable_models = data[data['Memory'] < mem_available*1024]\n",
    "max_score_index = deployable_models['Average'].idxmax()\n",
    "model_name = deployable_models.loc[max_score_index][\"Model_name_for_query\"]\n",
    "print(f'\\nBest Model Found!\\n{\"=\"*110}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42bc6697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/phi-1_5'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5481254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Model_repo</th>\n",
       "      <th>Model_experiment_details</th>\n",
       "      <th>Average</th>\n",
       "      <th>ARC</th>\n",
       "      <th>HellaSwag</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>TruthfulQA</th>\n",
       "      <th>Winogrande</th>\n",
       "      <th>GSM8K</th>\n",
       "      <th>DROP</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Hub_License</th>\n",
       "      <th>#Params (B)</th>\n",
       "      <th>Hub_like</th>\n",
       "      <th>Available_on_the_hub</th>\n",
       "      <th>Model_sha</th>\n",
       "      <th>Model_name_for_query</th>\n",
       "      <th>Memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>pretrained</td>\n",
       "      <td>https://huggingface.co/microsoft/phi-1_5</td>\n",
       "      <td>https://huggingface.co/datasets/open-llm-leade...</td>\n",
       "      <td>41.6</td>\n",
       "      <td>52.9</td>\n",
       "      <td>63.79</td>\n",
       "      <td>43.89</td>\n",
       "      <td>40.89</td>\n",
       "      <td>72.22</td>\n",
       "      <td>12.43</td>\n",
       "      <td>5.04</td>\n",
       "      <td>torch.bfloat16</td>\n",
       "      <td>other</td>\n",
       "      <td>1.3</td>\n",
       "      <td>916.0</td>\n",
       "      <td>True</td>\n",
       "      <td>ea95720a352172db6fcbcd89032bfb1cb8481797</td>\n",
       "      <td>microsoft/phi-1_5</td>\n",
       "      <td>4381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Type                                Model_repo  \\\n",
       "1025  pretrained  https://huggingface.co/microsoft/phi-1_5   \n",
       "\n",
       "                               Model_experiment_details  Average   ARC  \\\n",
       "1025  https://huggingface.co/datasets/open-llm-leade...     41.6  52.9   \n",
       "\n",
       "      HellaSwag   MMLU  TruthfulQA  Winogrande  GSM8K  DROP       Precision  \\\n",
       "1025      63.79  43.89       40.89       72.22  12.43  5.04  torch.bfloat16   \n",
       "\n",
       "     Hub_License  #Params (B)  Hub_like Available_on_the_hub  \\\n",
       "1025       other          1.3     916.0                 True   \n",
       "\n",
       "                                     Model_sha Model_name_for_query  Memory  \n",
       "1025  ea95720a352172db6fcbcd89032bfb1cb8481797    microsoft/phi-1_5    4381  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['Model_name_for_query'] == \"microsoft/phi-1_5\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
