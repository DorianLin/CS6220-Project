<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Type</th>
      <th>Model_repo</th>
      <th>Model_experiment_details</th>
      <th>Average</th>
      <th>ARC</th>
      <th>HellaSwag</th>
      <th>MMLU</th>
      <th>TruthfulQA</th>
      <th>Winogrande</th>
      <th>GSM8K</th>
      <th>DROP</th>
      <th>Architecture</th>
      <th>Precision</th>
      <th>Hub_License</th>
      <th>#Params (B)</th>
      <th>Hub_like</th>
      <th>Available_on_the_hub</th>
      <th>Model_sha</th>
      <th>Model_name_for_query</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/01-ai/Yi-34B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_01-ai__Yi-34B</td>
      <td>68.68</td>
      <td>64.59</td>
      <td>85.69</td>
      <td>76.35</td>
      <td>56.23</td>
      <td>83.03</td>
      <td>50.64</td>
      <td>64.20</td>
      <td>YiForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>custom</td>
      <td>34.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>cd8d59de87ea11c6453ee287ac82e5523f08c8ec</td>
      <td>01-ai/Yi-34B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/MayaPH/GodziLLa2-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MayaPH__GodziLLa2-70B</td>
      <td>67.01</td>
      <td>71.42</td>
      <td>87.53</td>
      <td>69.88</td>
      <td>61.54</td>
      <td>83.19</td>
      <td>43.21</td>
      <td>52.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.98</td>
      <td>17.0</td>
      <td>True</td>
      <td>7b78087db07eec97f7b461d10758ece76d685543</td>
      <td>MayaPH/GodziLLa2-70B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sequelbox/StellarBright</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sequelbox__StellarBright</td>
      <td>66.98</td>
      <td>72.95</td>
      <td>87.82</td>
      <td>71.17</td>
      <td>64.46</td>
      <td>83.27</td>
      <td>39.50</td>
      <td>49.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>69.24</td>
      <td>8.0</td>
      <td>True</td>
      <td>43efad8bfdb47139934e810906c1e59c25b5e269</td>
      <td>sequelbox/StellarBright</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Platypus2-70B-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Platypus2-70B-instruct</td>
      <td>66.89</td>
      <td>71.84</td>
      <td>87.94</td>
      <td>70.48</td>
      <td>62.26</td>
      <td>82.72</td>
      <td>40.56</td>
      <td>52.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>148.0</td>
      <td>True</td>
      <td>a66378c15f89756215ccc64572ba69b161173703</td>
      <td>garage-bAInd/Platypus2-70B-instruct</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/upstage/SOLAR-0-70b-16bit</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_upstage__SOLAR-0-70b-16bit</td>
      <td>66.88</td>
      <td>71.08</td>
      <td>87.89</td>
      <td>70.58</td>
      <td>62.25</td>
      <td>83.58</td>
      <td>45.26</td>
      <td>47.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>214.0</td>
      <td>True</td>
      <td>5f9c77b2c0397cf83d2f97740483f107c7109e8c</td>
      <td>upstage/SOLAR-0-70b-16bit</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Euryale-1.3-L2-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Euryale-1.3-L2-70B</td>
      <td>66.58</td>
      <td>70.82</td>
      <td>87.92</td>
      <td>70.39</td>
      <td>59.85</td>
      <td>82.79</td>
      <td>34.19</td>
      <td>60.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>4.0</td>
      <td>True</td>
      <td>6e3ce78eb5346bf3a5ee88cd60c25dc0d73de639</td>
      <td>Sao10K/Euryale-1.3-L2-70B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_101</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_101</td>
      <td>66.55</td>
      <td>68.69</td>
      <td>86.42</td>
      <td>69.92</td>
      <td>58.85</td>
      <td>82.08</td>
      <td>44.81</td>
      <td>55.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>884c53a64a3c5faf7b0706d36a587ca1532ed8f5</td>
      <td>psmathur/model_101</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-llama2-70b-v10.1-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-llama2-70b-v10.1-bf16</td>
      <td>66.47</td>
      <td>61.86</td>
      <td>83.13</td>
      <td>67.41</td>
      <td>56.18</td>
      <td>80.11</td>
      <td>60.27</td>
      <td>56.30</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>68.76</td>
      <td>41.0</td>
      <td>True</td>
      <td>a6ee90d262ac729f90ed8de97127766df070074c</td>
      <td>OpenBuddy/openbuddy-llama2-70b-v10.1-bf16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/budecosystem/genz-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_budecosystem__genz-70b</td>
      <td>66.34</td>
      <td>71.42</td>
      <td>87.99</td>
      <td>70.78</td>
      <td>62.66</td>
      <td>83.50</td>
      <td>33.74</td>
      <td>54.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>26.0</td>
      <td>True</td>
      <td>32110b4f33e5e80073ca1f47638482fdc0e19297</td>
      <td>budecosystem/genz-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/upstage/Llama-2-70b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_upstage__Llama-2-70b-instruct</td>
      <td>66.10</td>
      <td>70.90</td>
      <td>87.48</td>
      <td>69.80</td>
      <td>60.97</td>
      <td>82.87</td>
      <td>32.22</td>
      <td>58.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>53.0</td>
      <td>True</td>
      <td>8469429924dc2e1a9394b8095753985668a4052e</td>
      <td>upstage/Llama-2-70b-instruct</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/Samantha-1.11-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Samantha-1.11-70b</td>
      <td>65.90</td>
      <td>70.05</td>
      <td>87.55</td>
      <td>67.82</td>
      <td>65.02</td>
      <td>83.27</td>
      <td>29.95</td>
      <td>57.68</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>30.0</td>
      <td>True</td>
      <td>49e5b5ee0bed2864f0b38ba8bf9e01ccc5e0ba5f</td>
      <td>ehartford/Samantha-1.11-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ICBU-NPU/FashionGPT-70B-V1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ICBU-NPU__FashionGPT-70B-V1.1</td>
      <td>65.88</td>
      <td>71.76</td>
      <td>88.20</td>
      <td>70.99</td>
      <td>65.26</td>
      <td>82.64</td>
      <td>41.47</td>
      <td>40.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>38.0</td>
      <td>True</td>
      <td>05941a3eaacff0dead79b09d2175b5d7b98c525b</td>
      <td>ICBU-NPU/FashionGPT-70B-V1.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/MelangeB-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__MelangeB-70b</td>
      <td>65.80</td>
      <td>71.67</td>
      <td>87.50</td>
      <td>70.03</td>
      <td>59.36</td>
      <td>83.50</td>
      <td>30.63</td>
      <td>57.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.98</td>
      <td>0.0</td>
      <td>True</td>
      <td>08239fb1e30b1e42b14370f23e942bc51e76027c</td>
      <td>chargoddard/MelangeB-70b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/Samantha-1.1-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Samantha-1.1-70b</td>
      <td>65.74</td>
      <td>68.77</td>
      <td>87.46</td>
      <td>68.60</td>
      <td>64.85</td>
      <td>83.27</td>
      <td>31.61</td>
      <td>55.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>4.0</td>
      <td>True</td>
      <td>a3819d186f5b4d52ced7ddeb7fa16bf66e8a2ea7</td>
      <td>ehartford/Samantha-1.1-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/AIDC-ai-business/Marcoroni-70B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AIDC-ai-business__Marcoroni-70B-v1</td>
      <td>65.52</td>
      <td>73.55</td>
      <td>87.62</td>
      <td>70.67</td>
      <td>64.41</td>
      <td>83.43</td>
      <td>33.28</td>
      <td>45.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>16.0</td>
      <td>True</td>
      <td>55a30d29db194832c0b5de1392a6598a63582144</td>
      <td>AIDC-ai-business/Marcoroni-70B-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/fangloveskari/Platypus_QLoRA_LLaMA_70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_fangloveskari__Platypus_QLoRA_LLaMA_70b</td>
      <td>65.41</td>
      <td>72.10</td>
      <td>87.46</td>
      <td>71.02</td>
      <td>61.18</td>
      <td>82.87</td>
      <td>30.78</td>
      <td>52.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>3.0</td>
      <td>True</td>
      <td>b9b8560832276f60ba6bf37ac913b230a85ac19b</td>
      <td>fangloveskari/Platypus_QLoRA_LLaMA_70b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/v2ray/LLaMA-2-Wizard-70B-QLoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_v2ray__LLaMA-2-Wizard-70B-QLoRA</td>
      <td>65.40</td>
      <td>67.58</td>
      <td>87.52</td>
      <td>69.11</td>
      <td>61.79</td>
      <td>82.32</td>
      <td>30.48</td>
      <td>59.03</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>4bff676fe29f56d31961794c062aebc36312446e</td>
      <td>v2ray/LLaMA-2-Wizard-70B-QLoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TigerResearch/tigerbot-70b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TigerResearch__tigerbot-70b-chat</td>
      <td>65.32</td>
      <td>76.79</td>
      <td>87.76</td>
      <td>66.35</td>
      <td>55.09</td>
      <td>77.58</td>
      <td>45.64</td>
      <td>47.99</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>68.95</td>
      <td>6.0</td>
      <td>False</td>
      <td>7e506c4a056821e5d151a0e46572cd74d04194be</td>
      <td>TigerResearch/tigerbot-70b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TigerResearch/tigerbot-70b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TigerResearch__tigerbot-70b-chat</td>
      <td>65.24</td>
      <td>76.79</td>
      <td>87.83</td>
      <td>66.08</td>
      <td>55.10</td>
      <td>77.90</td>
      <td>44.96</td>
      <td>48.02</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>68.95</td>
      <td>6.0</td>
      <td>False</td>
      <td>7e506c4a056821e5d151a0e46572cd74d04194be</td>
      <td>TigerResearch/tigerbot-70b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_009</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_009</td>
      <td>65.03</td>
      <td>71.59</td>
      <td>87.70</td>
      <td>69.43</td>
      <td>60.72</td>
      <td>82.32</td>
      <td>39.42</td>
      <td>44.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>5020869e6394b1ac039bf80a0a1d2bed6be6707e</td>
      <td>psmathur/model_009</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-70b-2.2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-70b-2.2.1</td>
      <td>65.01</td>
      <td>69.71</td>
      <td>87.95</td>
      <td>69.79</td>
      <td>59.49</td>
      <td>82.95</td>
      <td>44.88</td>
      <td>40.27</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>6.0</td>
      <td>True</td>
      <td>eadc78a4a9e173bccdca7dc8d12a34e80317c66c</td>
      <td>jondurbin/airoboros-l2-70b-2.2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/stabilityai/StableBeluga2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__StableBeluga2</td>
      <td>64.97</td>
      <td>71.08</td>
      <td>86.37</td>
      <td>68.79</td>
      <td>59.44</td>
      <td>82.95</td>
      <td>35.86</td>
      <td>50.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>836.0</td>
      <td>True</td>
      <td>e4944caa6ece819413b140b8dcecea79fe7e22cf</td>
      <td>stabilityai/StableBeluga2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_v3_70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_v3_70b</td>
      <td>64.90</td>
      <td>71.25</td>
      <td>87.85</td>
      <td>70.18</td>
      <td>61.27</td>
      <td>82.72</td>
      <td>40.86</td>
      <td>40.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>16.0</td>
      <td>True</td>
      <td>c1d4f997f8ed685a6efc72229523b2e56fd0774b</td>
      <td>psmathur/orca_mini_v3_70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_51</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_51</td>
      <td>64.88</td>
      <td>68.43</td>
      <td>86.71</td>
      <td>69.31</td>
      <td>57.18</td>
      <td>81.77</td>
      <td>32.37</td>
      <td>58.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>9542702011bf4d282f4b0f0bd79229f5822b6313</td>
      <td>psmathur/model_51</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/v2ray/LLaMA-2-Jannie-70B-QLoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_v2ray__LLaMA-2-Jannie-70B-QLoRA</td>
      <td>64.76</td>
      <td>68.94</td>
      <td>86.90</td>
      <td>69.37</td>
      <td>53.67</td>
      <td>82.95</td>
      <td>31.77</td>
      <td>59.75</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>70.00</td>
      <td>14.0</td>
      <td>False</td>
      <td>e552ddca841a2b86e36bbe5f99840afedfdbcd14</td>
      <td>v2ray/LLaMA-2-Jannie-70B-QLoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/fangloveskari/ORCA_LLaMA_70B_QLoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_fangloveskari__ORCA_LLaMA_70B_QLoRA</td>
      <td>64.67</td>
      <td>72.27</td>
      <td>87.74</td>
      <td>70.23</td>
      <td>63.37</td>
      <td>83.66</td>
      <td>28.35</td>
      <td>47.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>49.0</td>
      <td>True</td>
      <td>ef9b04ef02ccc4d96f1181467da92bb6b5baf835</td>
      <td>fangloveskari/ORCA_LLaMA_70B_QLoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-70B-v1.2b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-70B-v1.2b</td>
      <td>64.63</td>
      <td>68.77</td>
      <td>87.57</td>
      <td>68.81</td>
      <td>57.69</td>
      <td>83.90</td>
      <td>35.25</td>
      <td>50.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>17.0</td>
      <td>True</td>
      <td>7b687d6e4101b8bb8cc4062f8a318d639098a55d</td>
      <td>migtissera/Synthia-70B-v1.2b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-falcon-180b-v13-preview0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-falcon-180b-v13-preview0</td>
      <td>64.30</td>
      <td>65.10</td>
      <td>86.19</td>
      <td>64.60</td>
      <td>54.97</td>
      <td>82.64</td>
      <td>41.62</td>
      <td>54.98</td>
      <td>FalconForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>178.64</td>
      <td>0.0</td>
      <td>True</td>
      <td>7d7b93ffd67d1b0c39f3503050dbbcc951948120</td>
      <td>OpenBuddy/openbuddy-falcon-180b-v13-preview0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Camel-Platypus2-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Camel-Platypus2-70B</td>
      <td>64.23</td>
      <td>71.08</td>
      <td>87.60</td>
      <td>70.04</td>
      <td>58.09</td>
      <td>83.82</td>
      <td>22.90</td>
      <td>56.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>12.0</td>
      <td>True</td>
      <td>b9f8de09ab860ee8ba570db7227c5444020ea056</td>
      <td>garage-bAInd/Camel-Platypus2-70B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Platypus2-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Platypus2-70B</td>
      <td>64.16</td>
      <td>70.65</td>
      <td>87.15</td>
      <td>70.08</td>
      <td>52.37</td>
      <td>84.37</td>
      <td>33.06</td>
      <td>51.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>68.72</td>
      <td>19.0</td>
      <td>True</td>
      <td>16b6583ad58313331f86be18e531ab03f1857695</td>
      <td>garage-bAInd/Platypus2-70B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/liuxiang886/llama2-70B-qlora-gpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_liuxiang886__llama2-70B-qlora-gpt4</td>
      <td>64.13</td>
      <td>70.31</td>
      <td>86.39</td>
      <td>69.29</td>
      <td>54.02</td>
      <td>82.87</td>
      <td>28.89</td>
      <td>57.15</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>08115ee077953e9c01c6a40f5086def3ecf9f5f0</td>
      <td>liuxiang886/llama2-70B-qlora-gpt4</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Camel-Platypus2-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Camel-Platypus2-70B</td>
      <td>64.05</td>
      <td>70.14</td>
      <td>87.71</td>
      <td>69.83</td>
      <td>57.77</td>
      <td>82.95</td>
      <td>23.96</td>
      <td>55.97</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>12.0</td>
      <td>True</td>
      <td>6f958a1063fe1e6075f6e379fae621ff5a1d98c6</td>
      <td>garage-bAInd/Camel-Platypus2-70B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/s1ghhh/medllama-2-70b-qlora-1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_s1ghhh__medllama-2-70b-qlora-1.1</td>
      <td>63.58</td>
      <td>69.03</td>
      <td>87.17</td>
      <td>71.04</td>
      <td>52.41</td>
      <td>84.21</td>
      <td>32.07</td>
      <td>49.10</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>d55e05e9d67418c639933c85a5b9d17c6f531a92</td>
      <td>s1ghhh/medllama-2-70b-qlora-1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-70B-v1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-70B-v1.2</td>
      <td>63.41</td>
      <td>70.48</td>
      <td>86.98</td>
      <td>70.13</td>
      <td>58.64</td>
      <td>83.27</td>
      <td>31.92</td>
      <td>42.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>16.0</td>
      <td>True</td>
      <td>9b92ee1093b125035ba1649dca6f4ceb9d86a656</td>
      <td>migtissera/Synthia-70B-v1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_007</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_007</td>
      <td>63.20</td>
      <td>71.08</td>
      <td>87.65</td>
      <td>69.04</td>
      <td>63.12</td>
      <td>83.35</td>
      <td>37.15</td>
      <td>31.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>19.0</td>
      <td>True</td>
      <td>0f5d81b13718a866cb078bd8762ab80a41972663</td>
      <td>psmathur/model_007</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/upstage/llama-65b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_upstage__llama-65b-instruct</td>
      <td>63.10</td>
      <td>68.86</td>
      <td>86.43</td>
      <td>64.77</td>
      <td>59.70</td>
      <td>81.06</td>
      <td>26.23</td>
      <td>54.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>65.02</td>
      <td>9.0</td>
      <td>True</td>
      <td>b95668861dfb7b0abca44ccdbef2db49b2dd8917</td>
      <td>upstage/llama-65b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sequelbox/SharpBalance</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sequelbox__SharpBalance</td>
      <td>63.01</td>
      <td>69.28</td>
      <td>87.59</td>
      <td>69.51</td>
      <td>59.05</td>
      <td>84.06</td>
      <td>34.65</td>
      <td>36.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>69.24</td>
      <td>3.0</td>
      <td>True</td>
      <td>a87cb1756d7b7389cc5a6d4647cf53377e962aea</td>
      <td>sequelbox/SharpBalance</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-70B-v1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-70B-v1.1</td>
      <td>62.84</td>
      <td>70.05</td>
      <td>87.12</td>
      <td>70.34</td>
      <td>57.84</td>
      <td>83.66</td>
      <td>31.84</td>
      <td>39.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>7.0</td>
      <td>True</td>
      <td>05a13f6adfe95a713dff04dc2eaa214c77c2512a</td>
      <td>migtissera/Synthia-70B-v1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uni-tianyan/Uni-TianYan</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uni-tianyan__Uni-TianYan</td>
      <td>62.78</td>
      <td>72.10</td>
      <td>87.40</td>
      <td>69.91</td>
      <td>65.81</td>
      <td>82.32</td>
      <td>22.14</td>
      <td>39.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>48.0</td>
      <td>True</td>
      <td>46b78b9a10e78283e59c28b56cb59c2f33b0816a</td>
      <td>uni-tianyan/Uni-TianYan</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-llama-65b-v8-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-llama-65b-v8-bf16</td>
      <td>62.62</td>
      <td>62.80</td>
      <td>83.60</td>
      <td>62.01</td>
      <td>55.09</td>
      <td>79.95</td>
      <td>43.37</td>
      <td>51.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>65.07</td>
      <td>8.0</td>
      <td>True</td>
      <td>445b77821fac8e6cfb77d0399fb827400b5bb71e</td>
      <td>OpenBuddy/openbuddy-llama-65b-v8-bf16</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TigerResearch/tigerbot-70b-base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TigerResearch__tigerbot-70b-base</td>
      <td>62.10</td>
      <td>62.46</td>
      <td>83.61</td>
      <td>65.49</td>
      <td>52.76</td>
      <td>80.19</td>
      <td>37.76</td>
      <td>52.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>68.95</td>
      <td>11.0</td>
      <td>True</td>
      <td>8af85526293eb8625375f3f7a1bab69825176e48</td>
      <td>TigerResearch/tigerbot-70b-base</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_007_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_007_v2</td>
      <td>62.02</td>
      <td>71.42</td>
      <td>87.31</td>
      <td>68.58</td>
      <td>62.65</td>
      <td>84.14</td>
      <td>28.66</td>
      <td>31.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>3d95e0f3598f7a76ab97cb2cc0e4aae957d77479</td>
      <td>psmathur/model_007_v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Airoboros-L2-70B-2.1-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Airoboros-L2-70B-2.1-GPTQ</td>
      <td>61.76</td>
      <td>70.39</td>
      <td>86.54</td>
      <td>68.89</td>
      <td>55.55</td>
      <td>81.61</td>
      <td>15.24</td>
      <td>54.10</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>72.82</td>
      <td>14.0</td>
      <td>True</td>
      <td>23ed580cb77ebaee49ea11eb4538fd3ab3795b76</td>
      <td>TheBloke/Airoboros-L2-70B-2.1-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/higgs-llama-vicuna-ep25-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__higgs-llama-vicuna-ep25-70b</td>
      <td>61.54</td>
      <td>62.29</td>
      <td>86.07</td>
      <td>64.25</td>
      <td>53.75</td>
      <td>80.66</td>
      <td>34.57</td>
      <td>49.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>1da59e150f1d0bae67f66400738a01d408a8c45d</td>
      <td>luffycodes/higgs-llama-vicuna-ep25-70b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/MelangeC-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__MelangeC-70b</td>
      <td>61.22</td>
      <td>71.67</td>
      <td>87.60</td>
      <td>70.37</td>
      <td>58.13</td>
      <td>83.98</td>
      <td>0.00</td>
      <td>56.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.98</td>
      <td>0.0</td>
      <td>True</td>
      <td>e54a2b924dec135f3fa2373933ab8485178cde1b</td>
      <td>chargoddard/MelangeC-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Brillibits/Instruct_Llama70B_Dolly15k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Brillibits__Instruct_Llama70B_Dolly15k</td>
      <td>60.97</td>
      <td>68.34</td>
      <td>87.21</td>
      <td>69.52</td>
      <td>46.46</td>
      <td>84.29</td>
      <td>42.68</td>
      <td>28.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>45444ac60488594e0700e6c7313ff444b4468240</td>
      <td>Brillibits/Instruct_Llama70B_Dolly15k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-70b-gpt4-2.0</td>
      <td>60.78</td>
      <td>68.52</td>
      <td>87.89</td>
      <td>70.41</td>
      <td>49.79</td>
      <td>83.50</td>
      <td>24.72</td>
      <td>40.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>12.0</td>
      <td>True</td>
      <td>f16526d9bb814dc10adc911f94e8c7a520beb5b6</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/elinas/chronos007-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elinas__chronos007-70b</td>
      <td>60.72</td>
      <td>70.14</td>
      <td>87.52</td>
      <td>69.33</td>
      <td>57.65</td>
      <td>82.24</td>
      <td>42.61</td>
      <td>15.52</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>2.0</td>
      <td>True</td>
      <td>c775f87a56f00725de4263f8d527995d40f611c4</td>
      <td>elinas/chronos007-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-falcon-180b-v12-preview0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-falcon-180b-v12-preview0</td>
      <td>60.54</td>
      <td>62.29</td>
      <td>83.80</td>
      <td>55.92</td>
      <td>53.05</td>
      <td>82.08</td>
      <td>41.24</td>
      <td>45.41</td>
      <td>FalconForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>178.64</td>
      <td>0.0</td>
      <td>True</td>
      <td>4f1aeb136860ee3216f23faec0c598014e5c40a6</td>
      <td>OpenBuddy/openbuddy-falcon-180b-v12-preview0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Faradaylab/ARIA-70B-V3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Faradaylab__ARIA-70B-V3</td>
      <td>60.53</td>
      <td>63.91</td>
      <td>86.21</td>
      <td>64.75</td>
      <td>51.32</td>
      <td>82.08</td>
      <td>28.13</td>
      <td>47.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.98</td>
      <td>0.0</td>
      <td>True</td>
      <td>6e7fdcd20626786dd744ea86c664a3c088ced39f</td>
      <td>Faradaylab/ARIA-70B-V3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-70B</td>
      <td>60.29</td>
      <td>69.45</td>
      <td>87.11</td>
      <td>68.91</td>
      <td>59.79</td>
      <td>83.66</td>
      <td>31.39</td>
      <td>21.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>8.0</td>
      <td>True</td>
      <td>d63dfdd0baed756981f5f78f7419fd822c572362</td>
      <td>migtissera/Synthia-70B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/llm-agents/tora-70b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llm-agents__tora-70b-v1.0</td>
      <td>60.12</td>
      <td>67.75</td>
      <td>85.83</td>
      <td>69.22</td>
      <td>51.79</td>
      <td>81.93</td>
      <td>23.81</td>
      <td>40.52</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>8.0</td>
      <td>True</td>
      <td>e95fd7daf017e7c414ec07ebef4ddf013c16f9a4</td>
      <td>llm-agents/tora-70b-v1.0</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Qwen/Qwen-14B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Qwen__Qwen-14B</td>
      <td>60.07</td>
      <td>58.28</td>
      <td>83.99</td>
      <td>67.70</td>
      <td>49.43</td>
      <td>76.80</td>
      <td>58.98</td>
      <td>25.31</td>
      <td>QWenLMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>14.17</td>
      <td>162.0</td>
      <td>True</td>
      <td>5eda9482e32a8ea7ed2dc47178f3b491eb207939</td>
      <td>Qwen/Qwen-14B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-70b-gpt4-2.0</td>
      <td>60.05</td>
      <td>68.60</td>
      <td>87.53</td>
      <td>69.37</td>
      <td>48.52</td>
      <td>83.90</td>
      <td>17.66</td>
      <td>44.74</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>other</td>
      <td>68.72</td>
      <td>12.0</td>
      <td>True</td>
      <td>f16526d9bb814dc10adc911f94e8c7a520beb5b6</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/oh-yeontaek/llama-2-70B-LoRA-assemble-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_oh-yeontaek__llama-2-70B-LoRA-assemble-v2</td>
      <td>59.93</td>
      <td>71.84</td>
      <td>86.89</td>
      <td>69.37</td>
      <td>64.79</td>
      <td>81.22</td>
      <td>14.25</td>
      <td>31.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>2.0</td>
      <td>True</td>
      <td>7feeb5b665ab1ecdfd9cc4fe45fadb86b7b91b5b</td>
      <td>oh-yeontaek/llama-2-70B-LoRA-assemble-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/test_42_70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__test_42_70b</td>
      <td>59.77</td>
      <td>68.26</td>
      <td>87.65</td>
      <td>70.00</td>
      <td>48.76</td>
      <td>83.66</td>
      <td>45.94</td>
      <td>14.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>4.0</td>
      <td>True</td>
      <td>ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b</td>
      <td>psmathur/test_42_70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/meta-math/MetaMath-70B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-math__MetaMath-70B-V1.0</td>
      <td>59.35</td>
      <td>68.00</td>
      <td>86.85</td>
      <td>69.31</td>
      <td>50.98</td>
      <td>82.32</td>
      <td>44.66</td>
      <td>13.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>8.0</td>
      <td>True</td>
      <td>783a3c7d5d0a75e6e11074f2577b90dd219ef7b1</td>
      <td>meta-math/MetaMath-70B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HiTZ/alpaca-lora-65b-en-pt-es-ca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HiTZ__alpaca-lora-65b-en-pt-es-ca</td>
      <td>59.31</td>
      <td>65.02</td>
      <td>84.88</td>
      <td>62.19</td>
      <td>46.06</td>
      <td>80.51</td>
      <td>26.69</td>
      <td>49.84</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>65.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>aa5bd88bd132925cf2dd5c44eceafdb5ed5e5be4</td>
      <td>HiTZ/alpaca-lora-65b-en-pt-es-ca</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/tiiuae/falcon-180B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_tiiuae__falcon-180B</td>
      <td>59.10</td>
      <td>69.45</td>
      <td>88.86</td>
      <td>70.50</td>
      <td>45.47</td>
      <td>86.90</td>
      <td>45.94</td>
      <td>6.57</td>
      <td>?</td>
      <td>8bit</td>
      <td>unknown</td>
      <td>179.52</td>
      <td>830.0</td>
      <td>False</td>
      <td>71a1a70b629e9963f7b4601e82f3f9079d48011e</td>
      <td>tiiuae/falcon-180B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-70b-gpt4-m2.0</td>
      <td>59.08</td>
      <td>70.05</td>
      <td>87.83</td>
      <td>70.67</td>
      <td>49.79</td>
      <td>83.58</td>
      <td>25.40</td>
      <td>26.20</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>10.0</td>
      <td>True</td>
      <td>1cccd0b60a988bf6ddc4e2688895837845afa076</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lizpreciatior/lzlv_70b_fp16_hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lizpreciatior__lzlv_70b_fp16_hf</td>
      <td>59.06</td>
      <td>70.14</td>
      <td>87.54</td>
      <td>70.23</td>
      <td>60.49</td>
      <td>83.43</td>
      <td>30.93</td>
      <td>10.68</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-2.0</td>
      <td>68.98</td>
      <td>23.0</td>
      <td>True</td>
      <td>b366c0bb318ae592023cca894cc6b4421a607a0d</td>
      <td>lizpreciatior/lzlv_70b_fp16_hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lvkaokao/mistral-7b-finetuned-orca-dpo-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lvkaokao__mistral-7b-finetuned-orca-dpo-v2</td>
      <td>59.06</td>
      <td>66.21</td>
      <td>83.64</td>
      <td>62.37</td>
      <td>59.65</td>
      <td>78.14</td>
      <td>19.56</td>
      <td>43.84</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5c1daaec60a480e8c81b265135583034054be2b</td>
      <td>lvkaokao/mistral-7b-finetuned-orca-dpo-v2</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/openbmb/UltraLM-65b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openbmb__UltraLM-65b</td>
      <td>58.99</td>
      <td>67.06</td>
      <td>84.98</td>
      <td>63.48</td>
      <td>53.51</td>
      <td>81.14</td>
      <td>32.75</td>
      <td>30.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>65.02</td>
      <td>6.0</td>
      <td>False</td>
      <td></td>
      <td>openbmb/UltraLM-65b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddyEA__openbuddy-llama-30b-v7.1-bf16</td>
      <td>58.89</td>
      <td>62.37</td>
      <td>82.29</td>
      <td>58.18</td>
      <td>52.60</td>
      <td>77.51</td>
      <td>31.61</td>
      <td>47.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.35</td>
      <td>6.0</td>
      <td>True</td>
      <td>85f7ad9d6ff016312262a47d45ffd07dee54aab0</td>
      <td>OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Xwin-LM__Xwin-LM-70B-V0.1</td>
      <td>58.87</td>
      <td>70.22</td>
      <td>87.25</td>
      <td>69.77</td>
      <td>59.86</td>
      <td>82.87</td>
      <td>27.22</td>
      <td>14.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>159.0</td>
      <td>True</td>
      <td>d6c803a180e3d46c371f8d3cb3848b861596ccbc</td>
      <td>Xwin-LM/Xwin-LM-70B-V0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lilloukas/GPlatty-30B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lilloukas__GPlatty-30B</td>
      <td>58.87</td>
      <td>65.78</td>
      <td>84.79</td>
      <td>63.49</td>
      <td>52.45</td>
      <td>80.98</td>
      <td>13.87</td>
      <td>50.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>18.0</td>
      <td>True</td>
      <td>836cf4dcd60ebe2ff09415c72f809d94639e8d35</td>
      <td>lilloukas/GPlatty-30B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ICBU-NPU/FashionGPT-70B-V1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ICBU-NPU__FashionGPT-70B-V1</td>
      <td>58.85</td>
      <td>71.08</td>
      <td>87.32</td>
      <td>70.70</td>
      <td>63.92</td>
      <td>83.66</td>
      <td>28.13</td>
      <td>7.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>4.0</td>
      <td>True</td>
      <td>060c096af49700760f734c0102250a524d46b3eb</td>
      <td>ICBU-NPU/FashionGPT-70B-V1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ICBU-NPU/FashionGPT-70B-V1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ICBU-NPU__FashionGPT-70B-V1.2</td>
      <td>58.85</td>
      <td>73.04</td>
      <td>88.15</td>
      <td>70.11</td>
      <td>65.15</td>
      <td>82.56</td>
      <td>24.03</td>
      <td>8.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>2.0</td>
      <td>True</td>
      <td>990a1664fc058de6ee2406af62c0a817d7047304</td>
      <td>ICBU-NPU/FashionGPT-70B-V1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddyEA__openbuddy-llama-30b-v7.1-bf16</td>
      <td>58.83</td>
      <td>62.46</td>
      <td>82.30</td>
      <td>58.15</td>
      <td>52.57</td>
      <td>77.82</td>
      <td>30.93</td>
      <td>47.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>32.35</td>
      <td>6.0</td>
      <td>True</td>
      <td>85f7ad9d6ff016312262a47d45ffd07dee54aab0</td>
      <td>OpenBuddyEA/openbuddy-llama-30b-v7.1-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-1.3</td>
      <td>58.57</td>
      <td>66.13</td>
      <td>85.99</td>
      <td>63.89</td>
      <td>51.32</td>
      <td>79.95</td>
      <td>13.65</td>
      <td>49.07</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>4373e66135c6fb4a6063777c4270a34509e7e932</td>
      <td>jondurbin/airoboros-65b-gpt4-1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/upstage/llama-30b-instruct-2048</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_upstage__llama-30b-instruct-2048</td>
      <td>58.56</td>
      <td>64.93</td>
      <td>84.94</td>
      <td>61.90</td>
      <td>56.30</td>
      <td>79.56</td>
      <td>17.82</td>
      <td>44.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>102.0</td>
      <td>True</td>
      <td>be44a37814a20e790063086703f570732597887a</td>
      <td>upstage/llama-30b-instruct-2048</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jarradh/llama2_70b_chat_uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jarradh__llama2_70b_chat_uncensored</td>
      <td>58.48</td>
      <td>68.43</td>
      <td>86.77</td>
      <td>68.76</td>
      <td>52.50</td>
      <td>82.56</td>
      <td>30.25</td>
      <td>20.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>37.0</td>
      <td>True</td>
      <td>34b23982a9a996adc8f45c4c2eac7245c4e251b3</td>
      <td>jarradh/llama2_70b_chat_uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Doctor-Shotgun/mythospice-limarp-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Doctor-Shotgun__mythospice-limarp-70b</td>
      <td>58.45</td>
      <td>69.20</td>
      <td>87.46</td>
      <td>70.14</td>
      <td>55.86</td>
      <td>82.72</td>
      <td>32.22</td>
      <td>11.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>agpl-3.0</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>ff29fed2a33fc050fd20d0e25b5b23c4a101b074</td>
      <td>Doctor-Shotgun/mythospice-limarp-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_420</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_420</td>
      <td>58.41</td>
      <td>70.14</td>
      <td>87.73</td>
      <td>70.35</td>
      <td>54.00</td>
      <td>83.74</td>
      <td>28.58</td>
      <td>14.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>13c7b5f403c0f2af9bf7fce2d4a32deb9054c083</td>
      <td>psmathur/model_420</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__llama-2-70b-Guanaco-QLoRA-fp16</td>
      <td>58.32</td>
      <td>68.26</td>
      <td>88.32</td>
      <td>70.23</td>
      <td>55.69</td>
      <td>83.98</td>
      <td>29.80</td>
      <td>11.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>53.0</td>
      <td>True</td>
      <td>54b0e39d5e9aee7b323f50b0a26db15295c3d5c9</td>
      <td>TheBloke/llama-2-70b-Guanaco-QLoRA-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/augtoma/qCammel-70</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_augtoma__qCammel-70</td>
      <td>58.32</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>10.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>20.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>augtoma/qCammel-70</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/augtoma/qCammel-70-x</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_augtoma__qCammel-70-x</td>
      <td>58.32</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>10.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>20.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>augtoma/qCammel-70-x</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/augtoma/qCammel-70x</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_augtoma__qCammel-70x</td>
      <td>58.32</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>10.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>20.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>augtoma/qCammel-70x</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/augtoma/qCammel70</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_augtoma__qCammel70</td>
      <td>58.32</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>10.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>20.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>augtoma/qCammel70</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/augtoma/qCammel-70v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_augtoma__qCammel-70v1</td>
      <td>58.32</td>
      <td>68.34</td>
      <td>87.87</td>
      <td>70.18</td>
      <td>57.47</td>
      <td>84.29</td>
      <td>29.72</td>
      <td>10.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>20.0</td>
      <td>True</td>
      <td>cf1e917e42fd1e56ee1edef7ee1a98cbe705c18c</td>
      <td>augtoma/qCammel-70v1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/pankajmathur/Lima_Unchained_70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pankajmathur__Lima_Unchained_70b</td>
      <td>58.20</td>
      <td>68.26</td>
      <td>87.65</td>
      <td>70.00</td>
      <td>48.76</td>
      <td>83.66</td>
      <td>34.72</td>
      <td>14.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>4.0</td>
      <td>True</td>
      <td>7dadf059a03bdfec2eb4f4a47666545875c68e49</td>
      <td>pankajmathur/Lima_Unchained_70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_42_70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_42_70b</td>
      <td>58.20</td>
      <td>68.26</td>
      <td>87.65</td>
      <td>70.00</td>
      <td>48.76</td>
      <td>83.66</td>
      <td>34.72</td>
      <td>14.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>4.0</td>
      <td>True</td>
      <td>ca3789cd6b683e97dcd6a5f0367f90a63d7a4e7b</td>
      <td>psmathur/model_42_70b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Aeala/Alpaca-elina-65b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aeala__Alpaca-elina-65b</td>
      <td>57.98</td>
      <td>65.27</td>
      <td>85.75</td>
      <td>63.42</td>
      <td>47.32</td>
      <td>81.37</td>
      <td>29.04</td>
      <td>33.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>65.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>51ce30a69b3c3363c8cfcd6395bf1df974ba2977</td>
      <td>Aeala/Alpaca-elina-65b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aeala/GPT4-x-AlpacaDente-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aeala__GPT4-x-AlpacaDente-30b</td>
      <td>57.98</td>
      <td>62.12</td>
      <td>82.78</td>
      <td>56.19</td>
      <td>52.68</td>
      <td>78.69</td>
      <td>30.10</td>
      <td>43.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>3.0</td>
      <td>True</td>
      <td>ee76c821f861f0ab0276f9f429dd06565f1f2051</td>
      <td>Aeala/GPT4-x-AlpacaDente-30b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/internlm/internlm-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_internlm__internlm-20b</td>
      <td>57.98</td>
      <td>60.49</td>
      <td>82.13</td>
      <td>61.85</td>
      <td>52.61</td>
      <td>76.72</td>
      <td>23.50</td>
      <td>48.53</td>
      <td>InternLMForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>47.0</td>
      <td>True</td>
      <td>b8825fe3394608fe84f0f5eb6471454384fb83aa</td>
      <td>internlm/internlm-20b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ariellee/SuperPlatty-30B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ariellee__SuperPlatty-30B</td>
      <td>57.89</td>
      <td>65.78</td>
      <td>83.95</td>
      <td>62.57</td>
      <td>53.52</td>
      <td>80.35</td>
      <td>9.63</td>
      <td>49.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>9.0</td>
      <td>True</td>
      <td>017e1c32bca060107337dbf26db2044a7caa56f2</td>
      <td>ariellee/SuperPlatty-30B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MetaIX__GPT4-X-Alpasta-30b</td>
      <td>57.85</td>
      <td>63.05</td>
      <td>83.56</td>
      <td>57.71</td>
      <td>51.52</td>
      <td>78.22</td>
      <td>30.48</td>
      <td>40.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>64.0</td>
      <td>True</td>
      <td>1a0d1d72a40946463fb4a9780207da19bfecc38b</td>
      <td>MetaIX/GPT4-X-Alpasta-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_teknium__OpenHermes-2.5-Mistral-7B</td>
      <td>57.82</td>
      <td>64.93</td>
      <td>84.30</td>
      <td>63.82</td>
      <td>52.31</td>
      <td>77.90</td>
      <td>25.47</td>
      <td>35.99</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>115.0</td>
      <td>True</td>
      <td>2a54cad766bc90828354db5c4199795aecfd0df1</td>
      <td>teknium/OpenHermes-2.5-Mistral-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/quantumaikr/llama-2-70b-fb16-orca-chat-10k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__llama-2-70b-fb16-orca-chat-10k</td>
      <td>57.73</td>
      <td>68.09</td>
      <td>87.07</td>
      <td>69.21</td>
      <td>61.56</td>
      <td>84.14</td>
      <td>26.91</td>
      <td>7.11</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>2.0</td>
      <td>True</td>
      <td>697aaeb8eb9905c9b25bebb736d1905444c774a6</td>
      <td>quantumaikr/llama-2-70b-fb16-orca-chat-10k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Doctor-Shotgun/mythospice-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Doctor-Shotgun__mythospice-70b</td>
      <td>57.71</td>
      <td>69.28</td>
      <td>87.53</td>
      <td>70.10</td>
      <td>56.76</td>
      <td>83.27</td>
      <td>30.10</td>
      <td>6.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>b00992c26604c9cd496bc41472a05e4c01cd2008</td>
      <td>Doctor-Shotgun/mythospice-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-1.4.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-70b-gpt4-1.4.1</td>
      <td>57.69</td>
      <td>70.39</td>
      <td>87.82</td>
      <td>70.31</td>
      <td>55.20</td>
      <td>83.58</td>
      <td>22.52</td>
      <td>14.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.72</td>
      <td>46.0</td>
      <td>True</td>
      <td>ea98153fa721ed7110c77e73388e3b6f3996f2bb</td>
      <td>jondurbin/airoboros-l2-70b-gpt4-1.4.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__llama2-70b-oasst-sft-v10</td>
      <td>57.58</td>
      <td>67.06</td>
      <td>86.38</td>
      <td>67.70</td>
      <td>56.45</td>
      <td>82.00</td>
      <td>27.22</td>
      <td>16.28</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>64.0</td>
      <td>True</td>
      <td>e68a8a2888097def3c7f4fe5d443866a18d05c6c</td>
      <td>OpenAssistant/llama2-70b-oasst-sft-v10</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dfurman/llama-2-70b-dolphin-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dfurman__llama-2-70b-dolphin-peft</td>
      <td>57.34</td>
      <td>69.62</td>
      <td>86.82</td>
      <td>69.18</td>
      <td>57.43</td>
      <td>83.90</td>
      <td>27.37</td>
      <td>7.03</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>70.00</td>
      <td>12.0</td>
      <td>False</td>
      <td>a1190dee60b5854e80d340958dc3cc956bc56f68</td>
      <td>dfurman/llama-2-70b-dolphin-peft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Intel/neural-chat-7b-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Intel__neural-chat-7b-v3</td>
      <td>57.31</td>
      <td>67.15</td>
      <td>83.29</td>
      <td>62.26</td>
      <td>58.77</td>
      <td>78.06</td>
      <td>1.21</td>
      <td>50.43</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>7a05c8a2151f7d32252d9ef5db10445c13ae1f20</td>
      <td>Intel/neural-chat-7b-v3</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__oasst-sft-6-llama-33b-xor-MERGED-16bit</td>
      <td>57.21</td>
      <td>61.52</td>
      <td>83.50</td>
      <td>57.43</td>
      <td>50.70</td>
      <td>79.08</td>
      <td>30.48</td>
      <td>37.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>3.0</td>
      <td>True</td>
      <td>62f92ddab8b37eaeda15cf5ecb5605141a0525eb</td>
      <td>TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardLM-70B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardLM-70B-V1.0</td>
      <td>57.17</td>
      <td>65.44</td>
      <td>84.41</td>
      <td>64.05</td>
      <td>54.81</td>
      <td>80.82</td>
      <td>17.97</td>
      <td>32.71</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>112.0</td>
      <td>True</td>
      <td>6dae38060d70b82dcfe787a612d04aaf0adf0738</td>
      <td>WizardLM/WizardLM-70B-V1.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Platypus-30B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Platypus-30B</td>
      <td>57.12</td>
      <td>64.59</td>
      <td>84.26</td>
      <td>64.23</td>
      <td>45.35</td>
      <td>81.37</td>
      <td>14.40</td>
      <td>45.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>16.0</td>
      <td>True</td>
      <td>c5d21054f8dd71099696bd7790df07ac54990f29</td>
      <td>garage-bAInd/Platypus-30B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lilloukas/Platypus-30B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lilloukas__Platypus-30B</td>
      <td>57.12</td>
      <td>64.59</td>
      <td>84.24</td>
      <td>64.19</td>
      <td>45.35</td>
      <td>81.37</td>
      <td>14.40</td>
      <td>45.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>16.0</td>
      <td>True</td>
      <td>979ad39b58a8e4a9419b7bc7a0dc8419f3912e71</td>
      <td>lilloukas/Platypus-30B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/tiiuae/falcon-180B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_tiiuae__falcon-180B</td>
      <td>57.11</td>
      <td>69.20</td>
      <td>88.89</td>
      <td>69.59</td>
      <td>45.16</td>
      <td>86.74</td>
      <td>33.21</td>
      <td>7.02</td>
      <td>?</td>
      <td>4bit</td>
      <td>unknown</td>
      <td>179.52</td>
      <td>830.0</td>
      <td>False</td>
      <td>71a1a70b629e9963f7b4601e82f3f9079d48011e</td>
      <td>tiiuae/falcon-180B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/SynthIA-7B-v1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__SynthIA-7B-v1.3</td>
      <td>57.11</td>
      <td>62.12</td>
      <td>83.45</td>
      <td>62.65</td>
      <td>51.37</td>
      <td>78.85</td>
      <td>17.59</td>
      <td>43.76</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>115.0</td>
      <td>True</td>
      <td>8e6d0b18be876e0ebfff47d6c4f33d776f189971</td>
      <td>migtissera/SynthIA-7B-v1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jordiclive/Llama-2-70b-oasst-1-200</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jordiclive__Llama-2-70b-oasst-1-200</td>
      <td>57.11</td>
      <td>67.66</td>
      <td>87.24</td>
      <td>69.95</td>
      <td>51.28</td>
      <td>84.14</td>
      <td>32.75</td>
      <td>6.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>68.72</td>
      <td>2.0</td>
      <td>True</td>
      <td>153b209007e688d713cd670c9972f2827c597b45</td>
      <td>jordiclive/Llama-2-70b-oasst-1-200</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/OpenLemur/lemur-70b-chat-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenLemur__lemur-70b-chat-v1</td>
      <td>57.10</td>
      <td>66.98</td>
      <td>85.73</td>
      <td>65.99</td>
      <td>56.58</td>
      <td>81.69</td>
      <td>35.33</td>
      <td>7.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>46.0</td>
      <td>True</td>
      <td>33da87ba6d90662c6a00535bd628e5b39b3afd3b</td>
      <td>OpenLemur/lemur-70b-chat-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/fiction.live-Kimiko-V2-70B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__fiction.live-Kimiko-V2-70B-fp16</td>
      <td>57.08</td>
      <td>67.66</td>
      <td>87.65</td>
      <td>69.82</td>
      <td>49.28</td>
      <td>83.90</td>
      <td>34.57</td>
      <td>6.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>3.0</td>
      <td>True</td>
      <td>6b0c2cb654133cad2d4920e7da2e3f6cb1c4f7fd</td>
      <td>TheBloke/fiction.live-Kimiko-V2-70B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aeala/GPT4-x-AlpacaDente2-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aeala__GPT4-x-AlpacaDente2-30b</td>
      <td>57.05</td>
      <td>60.58</td>
      <td>81.81</td>
      <td>56.63</td>
      <td>48.38</td>
      <td>78.14</td>
      <td>26.76</td>
      <td>47.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>30.0</td>
      <td>True</td>
      <td>9fe5a8dada738f44e7ee9293b2140ae0be021787</td>
      <td>Aeala/GPT4-x-AlpacaDente2-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/quantumaikr/llama-2-70b-fb16-korean</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__llama-2-70b-fb16-korean</td>
      <td>56.97</td>
      <td>67.15</td>
      <td>86.78</td>
      <td>69.29</td>
      <td>56.50</td>
      <td>82.64</td>
      <td>29.04</td>
      <td>7.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.98</td>
      <td>28.0</td>
      <td>True</td>
      <td>fd57855006c15c4121feccab1cbeee8107de5b5a</td>
      <td>quantumaikr/llama-2-70b-fb16-korean</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-1.2</td>
      <td>56.96</td>
      <td>65.87</td>
      <td>86.08</td>
      <td>63.37</td>
      <td>52.72</td>
      <td>79.56</td>
      <td>26.54</td>
      <td>24.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>21.0</td>
      <td>True</td>
      <td>50ab86e198e1c82ec81aefc628f23501c101d390</td>
      <td>jondurbin/airoboros-65b-gpt4-1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-TotSirocco-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-TotSirocco-7b</td>
      <td>56.92</td>
      <td>62.03</td>
      <td>84.23</td>
      <td>64.19</td>
      <td>46.49</td>
      <td>78.69</td>
      <td>13.27</td>
      <td>49.54</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>824e3a4738818142374721306ce85b83770de24b</td>
      <td>PocketDoc/Dans-TotSirocco-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-TotSirocco-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-TotSirocco-7b</td>
      <td>56.90</td>
      <td>62.20</td>
      <td>84.28</td>
      <td>63.80</td>
      <td>46.04</td>
      <td>79.48</td>
      <td>13.19</td>
      <td>49.30</td>
      <td>MistralForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>824e3a4738818142374721306ce85b83770de24b</td>
      <td>PocketDoc/Dans-TotSirocco-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/bhenrym14/mistral-7b-platypus-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bhenrym14__mistral-7b-platypus-fp16</td>
      <td>56.89</td>
      <td>63.05</td>
      <td>84.15</td>
      <td>64.11</td>
      <td>45.07</td>
      <td>78.53</td>
      <td>17.36</td>
      <td>45.92</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>d836a261afa0871d3734a7dfd1a28dc23c173ea7</td>
      <td>bhenrym14/mistral-7b-platypus-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/scarlett-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__scarlett-33b</td>
      <td>56.68</td>
      <td>67.75</td>
      <td>85.48</td>
      <td>58.98</td>
      <td>61.05</td>
      <td>76.80</td>
      <td>2.81</td>
      <td>43.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>32.32</td>
      <td>19.0</td>
      <td>True</td>
      <td>305eea72fb9fe2ac5929a62483ea51f152bcc060</td>
      <td>ajibawa-2023/scarlett-33b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-33b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-33b-instruct</td>
      <td>56.64</td>
      <td>63.05</td>
      <td>85.00</td>
      <td>58.32</td>
      <td>52.10</td>
      <td>78.85</td>
      <td>11.14</td>
      <td>47.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>32.32</td>
      <td>5.0</td>
      <td>True</td>
      <td>9c2b558b888e0ef8b4a72e0771db72a06a5c8474</td>
      <td>bofenghuang/vigogne-33b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Nous-Puffin-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Nous-Puffin-70B</td>
      <td>56.58</td>
      <td>67.41</td>
      <td>87.37</td>
      <td>69.77</td>
      <td>46.77</td>
      <td>83.90</td>
      <td>34.27</td>
      <td>6.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>[mit]</td>
      <td>68.72</td>
      <td>16.0</td>
      <td>True</td>
      <td>129e0af93d04b1b9cc85ea48bbb300f1ccb44210</td>
      <td>NousResearch/Nous-Puffin-70B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-PersonalityEngine-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-PersonalityEngine-30b</td>
      <td>56.42</td>
      <td>63.48</td>
      <td>84.37</td>
      <td>58.99</td>
      <td>46.98</td>
      <td>80.98</td>
      <td>15.54</td>
      <td>44.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>4.0</td>
      <td>True</td>
      <td>1990b46a2e2ac1f6282d961bce691ceceafed514</td>
      <td>PocketDoc/Dans-PersonalityEngine-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Euryale-L2-70B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Euryale-L2-70B</td>
      <td>56.39</td>
      <td>68.94</td>
      <td>87.07</td>
      <td>68.84</td>
      <td>54.49</td>
      <td>82.08</td>
      <td>26.54</td>
      <td>6.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>8.0</td>
      <td>True</td>
      <td>6589310a57ce5d9d6877f353f3d00cda8fa9101c</td>
      <td>Sao10K/Euryale-L2-70B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yulan-team/YuLan-Chat-2-13b-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yulan-team__YuLan-Chat-2-13b-fp16</td>
      <td>56.36</td>
      <td>59.04</td>
      <td>80.66</td>
      <td>56.72</td>
      <td>52.18</td>
      <td>79.64</td>
      <td>13.80</td>
      <td>52.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.95</td>
      <td>7.0</td>
      <td>True</td>
      <td>2d439187efd6edd91a0c0146f08dff52d92aa7bc</td>
      <td>yulan-team/YuLan-Chat-2-13b-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/hippogriff-30b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__hippogriff-30b-chat</td>
      <td>56.32</td>
      <td>64.51</td>
      <td>85.20</td>
      <td>59.09</td>
      <td>48.42</td>
      <td>80.82</td>
      <td>10.24</td>
      <td>45.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>21.0</td>
      <td>True</td>
      <td>64c10edf5312cd13704925b07413882d9e94c7a0</td>
      <td>openaccess-ai-collective/hippogriff-30b-chat</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/meta-llama/Llama-2-70b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-70b-hf</td>
      <td>56.25</td>
      <td>67.32</td>
      <td>87.33</td>
      <td>69.83</td>
      <td>44.92</td>
      <td>83.74</td>
      <td>33.97</td>
      <td>6.62</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.98</td>
      <td>623.0</td>
      <td>False</td>
      <td>ed7b07231238f836b99bf45701b9a0063576b194</td>
      <td>meta-llama/Llama-2-70b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Llama-2-70B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Llama-2-70B-fp16</td>
      <td>56.25</td>
      <td>67.32</td>
      <td>87.33</td>
      <td>69.83</td>
      <td>44.92</td>
      <td>83.74</td>
      <td>33.97</td>
      <td>6.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>68.98</td>
      <td>40.0</td>
      <td>True</td>
      <td>b25061ef1b440e970d15d4ac99bc42937cd442a2</td>
      <td>TheBloke/Llama-2-70B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-m-7b-3.1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-m-7b-3.1.2</td>
      <td>56.24</td>
      <td>61.86</td>
      <td>83.51</td>
      <td>61.91</td>
      <td>53.75</td>
      <td>77.58</td>
      <td>13.87</td>
      <td>41.20</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>20.0</td>
      <td>True</td>
      <td>e9a7f0271fa442d65bf6be87feeb3f4de2f5760e</td>
      <td>jondurbin/airoboros-m-7b-3.1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elinas/chronos-70b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elinas__chronos-70b-v2</td>
      <td>56.21</td>
      <td>68.09</td>
      <td>86.50</td>
      <td>68.28</td>
      <td>53.70</td>
      <td>81.22</td>
      <td>28.66</td>
      <td>7.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.72</td>
      <td>9.0</td>
      <td>True</td>
      <td>373af41ca0b2855972b8d471fd63e72b63e4c9fc</td>
      <td>elinas/chronos-70b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/trurl-2-13b-pl-instruct_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__trurl-2-13b-pl-instruct_unload</td>
      <td>56.20</td>
      <td>59.90</td>
      <td>79.99</td>
      <td>78.66</td>
      <td>45.56</td>
      <td>74.35</td>
      <td>12.21</td>
      <td>42.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>17f57642165e30a4025d6817bd47dcd80d0c5c4d</td>
      <td>Aspik101/trurl-2-13b-pl-instruct_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/WizardLM-30B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-30B-fp16</td>
      <td>56.19</td>
      <td>62.54</td>
      <td>83.28</td>
      <td>59.03</td>
      <td>52.49</td>
      <td>77.51</td>
      <td>22.21</td>
      <td>36.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>10.0</td>
      <td>True</td>
      <td>465f87a243969963f25ae6cf8f8d2de6c0898bbe</td>
      <td>TheBloke/WizardLM-30B-fp16</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/LLMs/WizardLM-30B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LLMs__WizardLM-30B-V1.0</td>
      <td>56.13</td>
      <td>62.54</td>
      <td>83.27</td>
      <td>59.05</td>
      <td>52.49</td>
      <td>77.51</td>
      <td>21.83</td>
      <td>36.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>32.32</td>
      <td>2.0</td>
      <td>True</td>
      <td>75318440dba949804d6263d368e1f29a94ea7c5f</td>
      <td>LLMs/WizardLM-30B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__OpenAssistant-SFT-7-Llama-30B-HF</td>
      <td>56.12</td>
      <td>60.58</td>
      <td>82.17</td>
      <td>57.93</td>
      <td>46.94</td>
      <td>78.61</td>
      <td>29.80</td>
      <td>36.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>13.0</td>
      <td>True</td>
      <td>a7a2306b9a63de2c545f35b24735f4540baf5903</td>
      <td>TheBloke/OpenAssistant-SFT-7-Llama-30B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama-30b</td>
      <td>56.06</td>
      <td>64.25</td>
      <td>83.64</td>
      <td>58.23</td>
      <td>53.20</td>
      <td>77.43</td>
      <td>15.85</td>
      <td>39.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>7f035eabd1d0e7b38ace395847a623f475d90da8</td>
      <td>YeungNLP/firefly-llama-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/42MARU/sitebunny-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_42MARU__sitebunny-13b</td>
      <td>56.03</td>
      <td>63.14</td>
      <td>83.64</td>
      <td>59.91</td>
      <td>56.21</td>
      <td>76.72</td>
      <td>9.40</td>
      <td>43.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>67107327d09c2f9bf3e4b316d97767c97f5a0804</td>
      <td>42MARU/sitebunny-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_420_preview</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_420_preview</td>
      <td>55.99</td>
      <td>67.06</td>
      <td>87.26</td>
      <td>69.85</td>
      <td>44.57</td>
      <td>83.35</td>
      <td>33.21</td>
      <td>6.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>True</td>
      <td>5095384f1b7bb6e23a987f95589e66e21ae854ef</td>
      <td>psmathur/model_420_preview</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/gpt4-alpaca-lora_mlp-65B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__gpt4-alpaca-lora_mlp-65B-HF</td>
      <td>55.94</td>
      <td>65.02</td>
      <td>86.13</td>
      <td>62.73</td>
      <td>59.16</td>
      <td>80.66</td>
      <td>28.28</td>
      <td>9.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.02</td>
      <td>7.0</td>
      <td>True</td>
      <td>664ff8e3e1d446971a16a6c9018ab24de7664684</td>
      <td>TheBloke/gpt4-alpaca-lora_mlp-65B-HF</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/MelangeA-70b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__MelangeA-70b</td>
      <td>55.92</td>
      <td>71.25</td>
      <td>87.30</td>
      <td>70.56</td>
      <td>60.61</td>
      <td>81.53</td>
      <td>5.69</td>
      <td>14.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.98</td>
      <td>0.0</td>
      <td>True</td>
      <td>d48cf79d1ead50154b1e70120779ae91bc5fafb4</td>
      <td>chargoddard/MelangeA-70b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/dolphin-2.0-mistral-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__dolphin-2.0-mistral-7b</td>
      <td>55.85</td>
      <td>59.22</td>
      <td>80.26</td>
      <td>56.90</td>
      <td>61.09</td>
      <td>75.37</td>
      <td>18.65</td>
      <td>39.49</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>58.0</td>
      <td>True</td>
      <td>c673387016c622fd0a707426953c03957398bc37</td>
      <td>ehartford/dolphin-2.0-mistral-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Zephyrus-L1-33B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Zephyrus-L1-33B</td>
      <td>55.79</td>
      <td>64.51</td>
      <td>84.15</td>
      <td>57.37</td>
      <td>53.87</td>
      <td>80.19</td>
      <td>23.58</td>
      <td>26.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.53</td>
      <td>2.0</td>
      <td>True</td>
      <td>679aae34440d576456b283070371b2a15dbb948b</td>
      <td>Sao10K/Zephyrus-L1-33B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/tulu-30B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__tulu-30B-fp16</td>
      <td>55.74</td>
      <td>59.98</td>
      <td>83.40</td>
      <td>56.10</td>
      <td>45.14</td>
      <td>80.82</td>
      <td>19.71</td>
      <td>45.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>5.0</td>
      <td>True</td>
      <td>37c3655676c37662f60c68dacfce3f0e861be846</td>
      <td>TheBloke/tulu-30B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Faradaylab/ARIA-70B-V2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Faradaylab__ARIA-70B-V2</td>
      <td>55.66</td>
      <td>62.12</td>
      <td>85.68</td>
      <td>63.49</td>
      <td>49.80</td>
      <td>81.69</td>
      <td>28.81</td>
      <td>18.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>7.0</td>
      <td>True</td>
      <td>2bf026af438d522268533484a85a3e54178e7809</td>
      <td>Faradaylab/ARIA-70B-V2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/SkunkworksAI/Mistralic-7B-1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_SkunkworksAI__Mistralic-7B-1</td>
      <td>55.44</td>
      <td>60.84</td>
      <td>82.29</td>
      <td>60.80</td>
      <td>52.38</td>
      <td>77.03</td>
      <td>11.07</td>
      <td>43.71</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.11</td>
      <td>17.0</td>
      <td>True</td>
      <td>ebf138de4fb7a57f0d187ad0ab43abd6b35bfb62</td>
      <td>SkunkworksAI/Mistralic-7B-1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/llama-2-70b-IA3-guanaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__llama-2-70b-IA3-guanaco</td>
      <td>55.42</td>
      <td>68.52</td>
      <td>85.67</td>
      <td>67.03</td>
      <td>43.47</td>
      <td>82.24</td>
      <td>28.73</td>
      <td>12.27</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.72</td>
      <td>1.0</td>
      <td>True</td>
      <td>e3230df22d065b6699096494d1151fa337dde9e8</td>
      <td>yeontaek/llama-2-70b-IA3-guanaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-code-mistral-orca-7b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-code-mistral-orca-7b-v1.0</td>
      <td>55.33</td>
      <td>59.64</td>
      <td>82.25</td>
      <td>61.33</td>
      <td>48.45</td>
      <td>77.51</td>
      <td>8.26</td>
      <td>49.89</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>4.0</td>
      <td>True</td>
      <td>f7db67fe6c82657b35d0ffcf8b7ff1568d979482</td>
      <td>uukuguy/speechless-code-mistral-orca-7b-v1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-llama2-13b-v8.1-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-llama2-13b-v8.1-fp16</td>
      <td>55.31</td>
      <td>55.97</td>
      <td>79.79</td>
      <td>54.95</td>
      <td>51.16</td>
      <td>74.35</td>
      <td>30.33</td>
      <td>40.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.88</td>
      <td>62.0</td>
      <td>True</td>
      <td>b51c6b29abdf7c420cb5e5f4f309ff83179c7bb8</td>
      <td>OpenBuddy/openbuddy-llama2-13b-v8.1-fp16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-2.1</td>
      <td>55.30</td>
      <td>63.65</td>
      <td>84.97</td>
      <td>57.37</td>
      <td>52.17</td>
      <td>78.22</td>
      <td>6.60</td>
      <td>44.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>8.0</td>
      <td>True</td>
      <td>12ccd0e6c9ef12c7d3c2eab8266cd32c0b2f7683</td>
      <td>jondurbin/airoboros-33b-2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/WizardLM-70B-V1.0-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-70B-V1.0-GPTQ</td>
      <td>55.28</td>
      <td>63.82</td>
      <td>83.85</td>
      <td>63.68</td>
      <td>54.54</td>
      <td>78.61</td>
      <td>18.50</td>
      <td>23.97</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>72.82</td>
      <td>26.0</td>
      <td>True</td>
      <td>c234d7c9c0fd26efb55757fdbfb604d549539fe0</td>
      <td>TheBloke/WizardLM-70B-V1.0-GPTQ</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/CalderaAI/30B-Epsilon</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CalderaAI__30B-Epsilon</td>
      <td>55.25</td>
      <td>63.05</td>
      <td>83.59</td>
      <td>56.89</td>
      <td>59.03</td>
      <td>77.66</td>
      <td>10.69</td>
      <td>35.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>9.0</td>
      <td>True</td>
      <td>6962638c2b0368ad496af6e20e46e3de97a7772b</td>
      <td>CalderaAI/30B-Epsilon</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/manticore-30b-chat-pyg-alpha</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__manticore-30b-chat-pyg-alpha</td>
      <td>55.20</td>
      <td>64.16</td>
      <td>84.38</td>
      <td>57.49</td>
      <td>51.57</td>
      <td>79.48</td>
      <td>16.07</td>
      <td>33.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.53</td>
      <td>13.0</td>
      <td>True</td>
      <td>0cff8e9718e57202171003d556d2e6630061879d</td>
      <td>openaccess-ai-collective/manticore-30b-chat-pyg-alpha</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewdBoros-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewdBoros-L2-13B</td>
      <td>55.10</td>
      <td>62.54</td>
      <td>83.90</td>
      <td>56.57</td>
      <td>48.14</td>
      <td>76.95</td>
      <td>10.99</td>
      <td>46.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>11.0</td>
      <td>True</td>
      <td>a3033ac5825662f1c66418d7543648dc76980185</td>
      <td>Undi95/MLewdBoros-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/carl-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__carl-33b</td>
      <td>55.01</td>
      <td>64.59</td>
      <td>85.27</td>
      <td>58.38</td>
      <td>45.32</td>
      <td>76.24</td>
      <td>6.37</td>
      <td>48.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>32.32</td>
      <td>8.0</td>
      <td>True</td>
      <td>5f80b372b493d901cab4490b4f23c71499023615</td>
      <td>ajibawa-2023/carl-33b</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/meta-llama/Llama-2-70b-chat-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-70b-chat-hf</td>
      <td>54.98</td>
      <td>64.59</td>
      <td>85.88</td>
      <td>63.91</td>
      <td>52.80</td>
      <td>80.51</td>
      <td>26.69</td>
      <td>10.50</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>68.98</td>
      <td>1453.0</td>
      <td>False</td>
      <td>7f54101c0fbb67a8143ca23eb8bd09b71f269c74</td>
      <td>meta-llama/Llama-2-70b-chat-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/willyninja30/ARIA-70B-French</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_willyninja30__ARIA-70B-French</td>
      <td>54.96</td>
      <td>64.51</td>
      <td>85.87</td>
      <td>63.88</td>
      <td>52.80</td>
      <td>80.51</td>
      <td>26.69</td>
      <td>10.50</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>0.0</td>
      <td>False</td>
      <td>d8580d360c51e71fddd27897445e2aa9d1888585</td>
      <td>willyninja30/ARIA-70B-French</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/alpaca-lora-65B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__alpaca-lora-65B-HF</td>
      <td>54.86</td>
      <td>64.85</td>
      <td>85.59</td>
      <td>63.11</td>
      <td>45.15</td>
      <td>81.22</td>
      <td>28.05</td>
      <td>16.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>113b61b37a2862b950ada68620e57acafbcefe13</td>
      <td>TheBloke/alpaca-lora-65B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CobraMamba/mamba-gpt-7b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CobraMamba__mamba-gpt-7b-v2</td>
      <td>54.85</td>
      <td>61.95</td>
      <td>83.83</td>
      <td>61.74</td>
      <td>46.63</td>
      <td>78.45</td>
      <td>17.29</td>
      <td>34.07</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>6439444e2c0b61253d3e61ae04fe0436717acc2f</td>
      <td>CobraMamba/mamba-gpt-7b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-1.2-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-1.2-L2-13B</td>
      <td>54.80</td>
      <td>60.75</td>
      <td>83.67</td>
      <td>56.27</td>
      <td>50.32</td>
      <td>74.98</td>
      <td>10.92</td>
      <td>46.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e76f35fe771ef142d6629092bd4a93301fd6cd4a</td>
      <td>Sao10K/Stheno-1.2-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/SynthIA-7B-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__SynthIA-7B-v1.5</td>
      <td>54.80</td>
      <td>62.71</td>
      <td>83.37</td>
      <td>63.48</td>
      <td>51.32</td>
      <td>79.24</td>
      <td>17.44</td>
      <td>26.01</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a9912ef90a0efc1aaea327e5cf3e9554c8bd897</td>
      <td>migtissera/SynthIA-7B-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CobraMamba/mamba-gpt-7b-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CobraMamba__mamba-gpt-7b-v1</td>
      <td>54.77</td>
      <td>61.26</td>
      <td>84.10</td>
      <td>63.46</td>
      <td>46.34</td>
      <td>79.16</td>
      <td>17.36</td>
      <td>31.67</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>e64d658b397748e409d9633fd24fc5a6df429600</td>
      <td>CobraMamba/mamba-gpt-7b-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/VicUnlocked-alpaca-65B-QLoRA-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__VicUnlocked-alpaca-65B-QLoRA-fp16</td>
      <td>54.74</td>
      <td>65.61</td>
      <td>85.15</td>
      <td>63.13</td>
      <td>52.47</td>
      <td>81.29</td>
      <td>27.82</td>
      <td>7.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.02</td>
      <td>9.0</td>
      <td>True</td>
      <td>6cdacfda96970aa144e316b108ab9bc17c99a573</td>
      <td>TheBloke/VicUnlocked-alpaca-65B-QLoRA-fp16</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/lmsys/vicuna-33b-v1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-33b-v1.3</td>
      <td>54.74</td>
      <td>62.12</td>
      <td>83.00</td>
      <td>59.22</td>
      <td>56.16</td>
      <td>77.03</td>
      <td>13.72</td>
      <td>31.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>230.0</td>
      <td>True</td>
      <td>ef8d6becf883fb3ce52e3706885f761819477ab4</td>
      <td>lmsys/vicuna-33b-v1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/guanaco-65B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__guanaco-65B-HF</td>
      <td>54.68</td>
      <td>65.44</td>
      <td>86.47</td>
      <td>62.92</td>
      <td>52.81</td>
      <td>82.40</td>
      <td>26.00</td>
      <td>6.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.02</td>
      <td>26.0</td>
      <td>True</td>
      <td>7f83ae526f8b83705ca8434535da8fd8c692f9d0</td>
      <td>TheBloke/guanaco-65B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewd-v2.4-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewd-v2.4-13B</td>
      <td>54.65</td>
      <td>61.69</td>
      <td>83.83</td>
      <td>55.10</td>
      <td>53.34</td>
      <td>74.51</td>
      <td>9.78</td>
      <td>44.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>8.0</td>
      <td>True</td>
      <td>6f6ec6024ee054020e49fd96f149919692848f0b</td>
      <td>Undi95/MLewd-v2.4-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-AdventurousWinds-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-AdventurousWinds-7b</td>
      <td>54.63</td>
      <td>61.01</td>
      <td>83.47</td>
      <td>63.69</td>
      <td>42.65</td>
      <td>78.22</td>
      <td>15.69</td>
      <td>37.65</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>ddc7e4fcbbb5c666a3fe1bbe4a47b4477151b699</td>
      <td>PocketDoc/Dans-AdventurousWinds-7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/huggyllama/llama-65b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huggyllama__llama-65b</td>
      <td>54.62</td>
      <td>63.48</td>
      <td>86.09</td>
      <td>63.93</td>
      <td>43.43</td>
      <td>82.56</td>
      <td>37.23</td>
      <td>5.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.29</td>
      <td>64.0</td>
      <td>True</td>
      <td>49707c5313d34d1c5a846e29cf2a2a650c22c8ee</td>
      <td>huggyllama/llama-65b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/ehartford/samantha-1.1-llama-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__samantha-1.1-llama-33b</td>
      <td>54.59</td>
      <td>67.83</td>
      <td>85.55</td>
      <td>58.79</td>
      <td>61.19</td>
      <td>76.48</td>
      <td>4.02</td>
      <td>28.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>13.0</td>
      <td>True</td>
      <td>ad8892a17be1372f611203a4cf71560cc337e458</td>
      <td>ehartford/samantha-1.1-llama-33b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NeverSleep/Mistral-11B-SynthIAirOmniMix</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NeverSleep__Mistral-11B-SynthIAirOmniMix</td>
      <td>54.56</td>
      <td>62.46</td>
      <td>83.13</td>
      <td>63.47</td>
      <td>55.69</td>
      <td>76.40</td>
      <td>11.90</td>
      <td>28.88</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>19694dc88e74a018d54bac6070cf521dff6d4397</td>
      <td>NeverSleep/Mistral-11B-SynthIAirOmniMix</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Stable-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Stable-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>54.53</td>
      <td>62.29</td>
      <td>82.46</td>
      <td>57.09</td>
      <td>51.41</td>
      <td>76.56</td>
      <td>3.56</td>
      <td>48.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>0c15b8540335b3e21a976a5fc5c33b47927fea6c</td>
      <td>TFLai/Stable-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__Mistral-7B-OpenOrca</td>
      <td>54.51</td>
      <td>64.08</td>
      <td>83.99</td>
      <td>62.24</td>
      <td>53.05</td>
      <td>77.74</td>
      <td>19.94</td>
      <td>20.53</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>314.0</td>
      <td>True</td>
      <td>7233ac83317946d05c474b71cc1379f49eb74c14</td>
      <td>Open-Orca/Mistral-7B-OpenOrca</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-2.0</td>
      <td>54.46</td>
      <td>66.64</td>
      <td>86.66</td>
      <td>63.18</td>
      <td>49.11</td>
      <td>80.74</td>
      <td>20.85</td>
      <td>14.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8</td>
      <td>jondurbin/airoboros-65b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-1.1-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-1.1-L2-13B</td>
      <td>54.43</td>
      <td>60.75</td>
      <td>83.64</td>
      <td>56.39</td>
      <td>50.30</td>
      <td>75.22</td>
      <td>7.96</td>
      <td>46.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>0f45a9f834dd216ce25ffa606b3b1ef2c99e7acd</td>
      <td>Sao10K/Stheno-1.1-L2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardMath-70B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardMath-70B-V1.0</td>
      <td>54.41</td>
      <td>68.17</td>
      <td>86.49</td>
      <td>68.89</td>
      <td>52.69</td>
      <td>82.32</td>
      <td>3.94</td>
      <td>18.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>95.0</td>
      <td>True</td>
      <td>e85b43e53c5379e35393b970c66d76c2d1060381</td>
      <td>WizardLM/WizardMath-70B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/upstage/llama-30b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_upstage__llama-30b-instruct</td>
      <td>54.41</td>
      <td>62.46</td>
      <td>86.23</td>
      <td>59.37</td>
      <td>52.78</td>
      <td>80.51</td>
      <td>12.13</td>
      <td>27.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>21.0</td>
      <td>True</td>
      <td>fea4312379557e8a1e8073965f560798de369edd</td>
      <td>upstage/llama-30b-instruct</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/ehartford/WizardLM-33B-V1.0-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-33B-V1.0-Uncensored</td>
      <td>54.41</td>
      <td>63.65</td>
      <td>83.84</td>
      <td>59.36</td>
      <td>56.80</td>
      <td>77.66</td>
      <td>18.65</td>
      <td>20.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>44.0</td>
      <td>True</td>
      <td>3eca9fdee0ce28d6a4a635a6f19d9a413caee3e7</td>
      <td>ehartford/WizardLM-33B-V1.0-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-m2.0</td>
      <td>54.40</td>
      <td>63.14</td>
      <td>85.19</td>
      <td>57.28</td>
      <td>48.07</td>
      <td>78.45</td>
      <td>9.70</td>
      <td>38.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>96af3dc6c9f2248d964cf14cef6e5f2e5894583a</td>
      <td>jondurbin/airoboros-33b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-30B-Uncensored-GPTQ</td>
      <td>54.39</td>
      <td>61.09</td>
      <td>82.40</td>
      <td>56.46</td>
      <td>49.90</td>
      <td>77.66</td>
      <td>23.28</td>
      <td>29.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>380.0</td>
      <td>True</td>
      <td>56a82ece7a9309189561a590e8f4d2fe0d4be92b</td>
      <td>TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sauce1337/AppleSauce-L2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sauce1337__AppleSauce-L2-13b</td>
      <td>54.37</td>
      <td>61.01</td>
      <td>83.61</td>
      <td>57.07</td>
      <td>47.81</td>
      <td>75.93</td>
      <td>10.01</td>
      <td>45.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba253c52eb85e24987c81e5d36b5a9a00e276ce7</td>
      <td>sauce1337/AppleSauce-L2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/JosephusCheung/Pwen-14B-Chat-20_30</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__Pwen-14B-Chat-20_30</td>
      <td>54.35</td>
      <td>56.14</td>
      <td>79.78</td>
      <td>60.01</td>
      <td>47.02</td>
      <td>76.48</td>
      <td>26.99</td>
      <td>33.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>14.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e878e1f1f7b533c32beb8e06ebcf0cfa23f3fe9b</td>
      <td>JosephusCheung/Pwen-14B-Chat-20_30</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardMath-70B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardMath-70B-V1.0</td>
      <td>54.34</td>
      <td>67.92</td>
      <td>86.46</td>
      <td>68.92</td>
      <td>52.77</td>
      <td>82.32</td>
      <td>4.09</td>
      <td>17.87</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>95.0</td>
      <td>True</td>
      <td>e85b43e53c5379e35393b970c66d76c2d1060381</td>
      <td>WizardLM/WizardMath-70B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-m2.0</td>
      <td>54.27</td>
      <td>63.40</td>
      <td>85.19</td>
      <td>57.46</td>
      <td>48.15</td>
      <td>78.37</td>
      <td>9.63</td>
      <td>37.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>84a89dee5bf3447079f115a3ef4d58ef8f924798</td>
      <td>jondurbin/airoboros-33b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-2.0</td>
      <td>54.27</td>
      <td>66.81</td>
      <td>86.66</td>
      <td>63.41</td>
      <td>49.17</td>
      <td>80.27</td>
      <td>20.55</td>
      <td>13.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea4bdd0221f77de9b0343cd8291cbd0fd6033ca8</td>
      <td>jondurbin/airoboros-65b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Stable-Platypus2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Stable-Platypus2-13B</td>
      <td>54.25</td>
      <td>62.71</td>
      <td>82.29</td>
      <td>58.30</td>
      <td>52.52</td>
      <td>76.87</td>
      <td>1.82</td>
      <td>45.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>12.85</td>
      <td>19.0</td>
      <td>True</td>
      <td>0e54aa49c24617e30a23a20c0c5da61419b9fe68</td>
      <td>garage-bAInd/Stable-Platypus2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-m2.0</td>
      <td>54.19</td>
      <td>65.02</td>
      <td>86.35</td>
      <td>64.37</td>
      <td>46.66</td>
      <td>80.19</td>
      <td>22.14</td>
      <td>14.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fa081d52619b35d7016fb40ce855187d6a8e7e4c</td>
      <td>jondurbin/airoboros-65b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-orca-platypus-coig-lite-2k-0.6e-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-orca-platypus-coig-lite-2k-0.6e-13b</td>
      <td>54.18</td>
      <td>59.90</td>
      <td>80.76</td>
      <td>58.34</td>
      <td>47.97</td>
      <td>77.90</td>
      <td>7.51</td>
      <td>46.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>65214c9923d55795ecd6e7f9e0fcee5ba5f26929</td>
      <td>uukuguy/speechless-orca-platypus-coig-lite-2k-0.6e-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/mistral-7b-slimorcaboros</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__mistral-7b-slimorcaboros</td>
      <td>54.10</td>
      <td>63.65</td>
      <td>83.70</td>
      <td>63.46</td>
      <td>55.81</td>
      <td>77.03</td>
      <td>23.43</td>
      <td>11.62</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>c06e1a6b6c0fe764117f9ec7611ce31e796e602a</td>
      <td>openaccess-ai-collective/mistral-7b-slimorcaboros</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__OrcaMini-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>54.08</td>
      <td>60.84</td>
      <td>82.56</td>
      <td>56.42</td>
      <td>53.32</td>
      <td>75.93</td>
      <td>2.27</td>
      <td>47.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1f81c0439f60d848e3cbc7f06fcd58b5161a8557</td>
      <td>TFLai/OrcaMini-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenLemur/lemur-70b-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenLemur__lemur-70b-v1</td>
      <td>54.03</td>
      <td>64.33</td>
      <td>85.72</td>
      <td>65.85</td>
      <td>44.78</td>
      <td>83.03</td>
      <td>28.73</td>
      <td>5.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>31.0</td>
      <td>True</td>
      <td>74432ae16ef50207fe17fb88b2f1c1d32ef3b481</td>
      <td>OpenLemur/lemur-70b-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-2.0</td>
      <td>54.01</td>
      <td>63.91</td>
      <td>85.67</td>
      <td>57.95</td>
      <td>45.54</td>
      <td>77.98</td>
      <td>11.07</td>
      <td>35.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>3.0</td>
      <td>True</td>
      <td>a4e1b721add286900c5a6f529c3d7a3e0049b2e0</td>
      <td>jondurbin/airoboros-33b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-m2.0</td>
      <td>53.94</td>
      <td>65.10</td>
      <td>86.34</td>
      <td>64.32</td>
      <td>46.63</td>
      <td>80.11</td>
      <td>21.61</td>
      <td>13.47</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fa081d52619b35d7016fb40ce855187d6a8e7e4c</td>
      <td>jondurbin/airoboros-65b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/Nebula-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__Nebula-7B</td>
      <td>53.93</td>
      <td>59.30</td>
      <td>83.46</td>
      <td>57.00</td>
      <td>45.56</td>
      <td>76.40</td>
      <td>14.86</td>
      <td>40.96</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>569f848698a468fb03d37033c67f3734bbaec127</td>
      <td>PulsarAI/Nebula-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-m2.0</td>
      <td>53.88</td>
      <td>64.68</td>
      <td>84.95</td>
      <td>57.77</td>
      <td>47.44</td>
      <td>77.74</td>
      <td>10.39</td>
      <td>34.17</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>57bd88e24d603dc4bbe4016ed0871db7c0e529d5</td>
      <td>jondurbin/airoboros-33b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Enno-Ai/ennodata-13b-8bit-raw-15epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Enno-Ai__ennodata-13b-8bit-raw-15epoch</td>
      <td>53.87</td>
      <td>61.60</td>
      <td>82.20</td>
      <td>57.55</td>
      <td>53.58</td>
      <td>77.51</td>
      <td>1.44</td>
      <td>43.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ee2ceaae9cb806bc30df84ba4d598fdf32e53b17</td>
      <td>Enno-Ai/ennodata-13b-8bit-raw-15epoch</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/teknium/CollectiveCognition-v1.1-Mistral-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_teknium__CollectiveCognition-v1.1-Mistral-7B</td>
      <td>53.87</td>
      <td>62.12</td>
      <td>84.17</td>
      <td>62.35</td>
      <td>57.62</td>
      <td>75.37</td>
      <td>15.62</td>
      <td>19.85</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>45.0</td>
      <td>True</td>
      <td>5f57f70ec99450c70da2540e94dd7fd67be4b23c</td>
      <td>teknium/CollectiveCognition-v1.1-Mistral-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openBuddy/openbuddy-llama2-34b-v11.1-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openBuddy__openbuddy-llama2-34b-v11.1-bf16</td>
      <td>53.87</td>
      <td>50.00</td>
      <td>71.19</td>
      <td>55.71</td>
      <td>53.01</td>
      <td>70.80</td>
      <td>34.57</td>
      <td>41.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>33.53</td>
      <td>6.0</td>
      <td>True</td>
      <td>21ac0d26c0097e5ac5b4a757493574b156da7731</td>
      <td>openBuddy/openbuddy-llama2-34b-v11.1-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-codellama2-34b-v11.1-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-codellama2-34b-v11.1-bf16</td>
      <td>53.87</td>
      <td>50.00</td>
      <td>71.19</td>
      <td>55.71</td>
      <td>53.01</td>
      <td>70.80</td>
      <td>34.57</td>
      <td>41.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>33.53</td>
      <td>6.0</td>
      <td>True</td>
      <td>1b361b3634bf59913b47c9dad1b138e99833472b</td>
      <td>OpenBuddy/openbuddy-codellama2-34b-v11.1-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sauce1337/BerrySauce-L2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sauce1337__BerrySauce-L2-13b</td>
      <td>53.86</td>
      <td>62.29</td>
      <td>83.78</td>
      <td>57.10</td>
      <td>48.30</td>
      <td>76.09</td>
      <td>11.75</td>
      <td>37.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>c8788874b78c84bc5593586d16fbd8ae7b5b2991</td>
      <td>sauce1337/BerrySauce-L2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-2.0</td>
      <td>53.82</td>
      <td>63.82</td>
      <td>85.65</td>
      <td>58.44</td>
      <td>45.57</td>
      <td>77.90</td>
      <td>10.69</td>
      <td>34.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>3.0</td>
      <td>True</td>
      <td>ddc598f492f5098a8e308f51a82834f98f29a4ce</td>
      <td>jondurbin/airoboros-33b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/Uncensored-Frank-33B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__Uncensored-Frank-33B</td>
      <td>53.79</td>
      <td>62.12</td>
      <td>83.30</td>
      <td>57.57</td>
      <td>54.03</td>
      <td>76.56</td>
      <td>16.68</td>
      <td>26.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>32.32</td>
      <td>2.0</td>
      <td>True</td>
      <td>1c1f4e9256ac2be145a9106863ee9f2e9d701e74</td>
      <td>ajibawa-2023/Uncensored-Frank-33B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PulsarAI/CollectiveCognition-v1.1-Nebula-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__CollectiveCognition-v1.1-Nebula-7B</td>
      <td>53.79</td>
      <td>58.11</td>
      <td>82.39</td>
      <td>57.03</td>
      <td>53.53</td>
      <td>73.72</td>
      <td>9.55</td>
      <td>42.17</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c41d373a2d49b79236d6c4d0dfc4086e709c07eb</td>
      <td>PulsarAI/CollectiveCognition-v1.1-Nebula-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-1.4</td>
      <td>53.78</td>
      <td>65.53</td>
      <td>85.77</td>
      <td>61.95</td>
      <td>52.43</td>
      <td>79.79</td>
      <td>18.04</td>
      <td>12.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>16.0</td>
      <td>True</td>
      <td>ae256799615c16443f9c423c653ed9f60577e99e</td>
      <td>jondurbin/airoboros-65b-gpt4-1.4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/model_007_13b_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__model_007_13b_v2</td>
      <td>53.78</td>
      <td>61.95</td>
      <td>82.48</td>
      <td>57.32</td>
      <td>53.50</td>
      <td>75.85</td>
      <td>1.36</td>
      <td>43.97</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>1c959d4b5d5b8683b051f07475bb5c1ab24c8bb0</td>
      <td>psmathur/model_007_13b_v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BELLE-2/BELLE-Llama2-13B-chat-0.4M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BELLE-2__BELLE-Llama2-13B-chat-0.4M</td>
      <td>53.77</td>
      <td>60.67</td>
      <td>82.31</td>
      <td>55.94</td>
      <td>50.85</td>
      <td>75.53</td>
      <td>14.40</td>
      <td>36.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>27.0</td>
      <td>True</td>
      <td>1776feacbf1052cff02eb3d7531a854555d3f6dc</td>
      <td>BELLE-2/BELLE-Llama2-13B-chat-0.4M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/concedo/Vicuzard-30B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_concedo__Vicuzard-30B-Uncensored</td>
      <td>53.76</td>
      <td>62.97</td>
      <td>83.68</td>
      <td>58.16</td>
      <td>52.27</td>
      <td>77.11</td>
      <td>15.39</td>
      <td>26.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>11.0</td>
      <td>True</td>
      <td>e2329c05a6e59660ba3cbcc01adf30a78f852594</td>
      <td>concedo/Vicuzard-30B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Enno-Ai/ennodata-raw-pankajmathur-13b-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Enno-Ai__ennodata-raw-pankajmathur-13b-peft</td>
      <td>53.72</td>
      <td>61.95</td>
      <td>82.21</td>
      <td>57.44</td>
      <td>53.57</td>
      <td>75.93</td>
      <td>1.29</td>
      <td>43.65</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>206553873db96a6730d36477837335dbbcc906fc</td>
      <td>Enno-Ai/ennodata-raw-pankajmathur-13b-peft</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-1.4</td>
      <td>53.68</td>
      <td>65.78</td>
      <td>85.83</td>
      <td>62.27</td>
      <td>52.45</td>
      <td>79.64</td>
      <td>18.04</td>
      <td>11.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.02</td>
      <td>16.0</td>
      <td>True</td>
      <td>ae256799615c16443f9c423c653ed9f60577e99e</td>
      <td>jondurbin/airoboros-65b-gpt4-1.4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-65b-gpt4-1.4-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-65b-gpt4-1.4-peft</td>
      <td>53.68</td>
      <td>65.78</td>
      <td>85.83</td>
      <td>62.27</td>
      <td>52.45</td>
      <td>79.64</td>
      <td>18.04</td>
      <td>11.76</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>65.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>85ae3b595c6b8415df87000c22bc14ea18c174f5</td>
      <td>jondurbin/airoboros-65b-gpt4-1.4-peft</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/akjindal53244/Mistral-7B-v0.1-Open-Platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_akjindal53244__Mistral-7B-v0.1-Open-Platypus</td>
      <td>53.64</td>
      <td>62.37</td>
      <td>85.08</td>
      <td>63.79</td>
      <td>47.33</td>
      <td>77.66</td>
      <td>17.29</td>
      <td>21.93</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>2.0</td>
      <td>True</td>
      <td>aa2c84e89c4c8a10e0569e45021b59e6d1c08bda</td>
      <td>akjindal53244/Mistral-7B-v0.1-Open-Platypus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/robin-65b-v2-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__robin-65b-v2-fp16</td>
      <td>53.61</td>
      <td>61.95</td>
      <td>84.60</td>
      <td>62.51</td>
      <td>52.31</td>
      <td>80.51</td>
      <td>26.99</td>
      <td>6.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>40edb31ba93045d673735361bc98f56125bbc77b</td>
      <td>TheBloke/robin-65b-v2-fp16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardMath-70B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardMath-70B-V1.0</td>
      <td>53.60</td>
      <td>67.49</td>
      <td>86.03</td>
      <td>68.44</td>
      <td>52.23</td>
      <td>81.77</td>
      <td>2.88</td>
      <td>16.36</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>llama2</td>
      <td>68.72</td>
      <td>95.0</td>
      <td>True</td>
      <td>97e5913edd2c593c3eef12070024674e7ee4e16c</td>
      <td>WizardLM/WizardMath-70B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FlagAlpha__Llama2-Chinese-13b-Chat</td>
      <td>53.57</td>
      <td>55.97</td>
      <td>82.05</td>
      <td>54.74</td>
      <td>48.90</td>
      <td>76.16</td>
      <td>12.59</td>
      <td>44.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>206.0</td>
      <td>True</td>
      <td>cb69cda10a72bc9736b1c10181ac41f28b69ff9b</td>
      <td>FlagAlpha/Llama2-Chinese-13b-Chat</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Secbone/llama-33B-instructed</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Secbone__llama-33B-instructed</td>
      <td>53.56</td>
      <td>64.59</td>
      <td>86.17</td>
      <td>60.50</td>
      <td>44.12</td>
      <td>79.32</td>
      <td>14.40</td>
      <td>25.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>7c40caaea4fe3264fd469dac428b0f9450e574a6</td>
      <td>Secbone/llama-33B-instructed</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/maywell/Synatra-V0.1-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_maywell__Synatra-V0.1-7B</td>
      <td>53.54</td>
      <td>55.29</td>
      <td>76.63</td>
      <td>55.29</td>
      <td>55.76</td>
      <td>72.77</td>
      <td>19.41</td>
      <td>39.63</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.11</td>
      <td>12.0</td>
      <td>True</td>
      <td>7ee3416f31a3c7e8d5ab4295ac1b641075f36345</td>
      <td>maywell/Synatra-V0.1-7B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/maywell/Synatra-V0.1-7B-Instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_maywell__Synatra-V0.1-7B-Instruct</td>
      <td>53.54</td>
      <td>55.29</td>
      <td>76.63</td>
      <td>55.29</td>
      <td>55.76</td>
      <td>72.77</td>
      <td>19.41</td>
      <td>39.63</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.11</td>
      <td>12.0</td>
      <td>True</td>
      <td>7ee3416f31a3c7e8d5ab4295ac1b641075f36345</td>
      <td>maywell/Synatra-V0.1-7B-Instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/jackalope-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__jackalope-7b</td>
      <td>53.53</td>
      <td>63.40</td>
      <td>83.29</td>
      <td>63.50</td>
      <td>50.06</td>
      <td>78.06</td>
      <td>28.66</td>
      <td>7.79</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>16.0</td>
      <td>True</td>
      <td>5ba23522319a51d0af23b336a6a83c72ae3780e7</td>
      <td>openaccess-ai-collective/jackalope-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-llama2-13b-v11.1-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-llama2-13b-v11.1-bf16</td>
      <td>53.51</td>
      <td>51.79</td>
      <td>76.23</td>
      <td>56.13</td>
      <td>49.70</td>
      <td>73.48</td>
      <td>24.34</td>
      <td>42.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.88</td>
      <td>16.0</td>
      <td>True</td>
      <td>76fb7d00836eb2f1d9c9605d8881d73b782cf324</td>
      <td>OpenBuddy/openbuddy-llama2-13b-v11.1-bf16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/CalderaAI/13B-Thorns-l2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CalderaAI__13B-Thorns-l2</td>
      <td>53.50</td>
      <td>62.88</td>
      <td>83.57</td>
      <td>56.95</td>
      <td>49.52</td>
      <td>74.51</td>
      <td>0.91</td>
      <td>46.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>adc5e7befcc3d0a26f46198fdda4a098a2742fe6</td>
      <td>CalderaAI/13B-Thorns-l2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-L2-13B</td>
      <td>53.48</td>
      <td>61.01</td>
      <td>83.95</td>
      <td>56.33</td>
      <td>50.18</td>
      <td>75.14</td>
      <td>11.98</td>
      <td>35.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>c4e7b771e30fdbfd6bd2e66a6928024bd5692bbd</td>
      <td>Sao10K/Stheno-L2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/dolphin-2.1-mistral-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__dolphin-2.1-mistral-7b</td>
      <td>53.47</td>
      <td>64.42</td>
      <td>84.92</td>
      <td>63.32</td>
      <td>55.56</td>
      <td>77.74</td>
      <td>20.77</td>
      <td>7.56</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>54.0</td>
      <td>True</td>
      <td>aa5bd48c8b3040d1155a8fd59328df160aa63680</td>
      <td>ehartford/dolphin-2.1-mistral-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-code-mistral-7b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-code-mistral-7b-v1.0</td>
      <td>53.47</td>
      <td>60.58</td>
      <td>83.75</td>
      <td>62.98</td>
      <td>47.90</td>
      <td>78.69</td>
      <td>19.18</td>
      <td>21.19</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>753852b8cb52dc5f0411568e98c0cb445a7835dc</td>
      <td>uukuguy/speechless-code-mistral-7b-v1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/based-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__based-30b</td>
      <td>53.46</td>
      <td>63.91</td>
      <td>85.67</td>
      <td>58.28</td>
      <td>35.70</td>
      <td>80.11</td>
      <td>0.30</td>
      <td>50.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>32.32</td>
      <td>39.0</td>
      <td>True</td>
      <td>5818a6344f48dc5a324589b57cb288a9d54c0b79</td>
      <td>ehartford/based-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-30B-Uncensored-fp16</td>
      <td>53.44</td>
      <td>62.12</td>
      <td>83.45</td>
      <td>58.24</td>
      <td>50.81</td>
      <td>78.45</td>
      <td>14.25</td>
      <td>26.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>16.0</td>
      <td>True</td>
      <td>c7b7cecb5a314fc66deebabcb67c230a3fbe84f7</td>
      <td>TheBloke/Wizard-Vicuna-30B-Uncensored-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/Wizard-Vicuna-30B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Wizard-Vicuna-30B-Uncensored</td>
      <td>53.44</td>
      <td>62.12</td>
      <td>83.45</td>
      <td>58.24</td>
      <td>50.81</td>
      <td>78.45</td>
      <td>14.25</td>
      <td>26.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>72.0</td>
      <td>True</td>
      <td>6374baef4cedd41f85c111b8eec3eb38ee24c4b9</td>
      <td>ehartford/Wizard-Vicuna-30B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Weyaxi/SlimOpenOrca-Mistral-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__SlimOpenOrca-Mistral-7B</td>
      <td>53.43</td>
      <td>62.97</td>
      <td>83.49</td>
      <td>62.30</td>
      <td>57.39</td>
      <td>77.43</td>
      <td>21.46</td>
      <td>9.01</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b0134a7512444dfbb60a2e2d81469a5bbbb18026</td>
      <td>Weyaxi/SlimOpenOrca-Mistral-7B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/Chat-AYB-Platypus2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__Chat-AYB-Platypus2-13B</td>
      <td>53.39</td>
      <td>60.49</td>
      <td>84.03</td>
      <td>57.83</td>
      <td>54.52</td>
      <td>75.77</td>
      <td>2.96</td>
      <td>38.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a54eb9d5a66df4720ec52422f5627ccd94d5fd6</td>
      <td>PulsarAI/Chat-AYB-Platypus2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-mistral-six-in-one-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-mistral-six-in-one-7b</td>
      <td>53.38</td>
      <td>62.97</td>
      <td>84.60</td>
      <td>63.29</td>
      <td>57.77</td>
      <td>77.51</td>
      <td>18.42</td>
      <td>9.13</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>41e912e0f79094a80687f88ca5555f84aa9d307f</td>
      <td>uukuguy/speechless-mistral-six-in-one-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/dolphin-2.1-mistral-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__dolphin-2.1-mistral-7b</td>
      <td>53.37</td>
      <td>63.99</td>
      <td>85.00</td>
      <td>63.44</td>
      <td>55.57</td>
      <td>77.90</td>
      <td>20.09</td>
      <td>7.61</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>54.0</td>
      <td>True</td>
      <td>aa5bd48c8b3040d1155a8fd59328df160aa63680</td>
      <td>ehartford/dolphin-2.1-mistral-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/Mistral-7B-SlimOrca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__Mistral-7B-SlimOrca</td>
      <td>53.34</td>
      <td>62.54</td>
      <td>83.86</td>
      <td>62.77</td>
      <td>54.23</td>
      <td>77.43</td>
      <td>21.38</td>
      <td>11.20</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>4.0</td>
      <td>True</td>
      <td>a9744d8cf9ce4230678a891bcf8bba7cbc0aaece</td>
      <td>Open-Orca/Mistral-7B-SlimOrca</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-mistral-dolphin-orca-platypus-samantha-7b</td>
      <td>53.34</td>
      <td>64.33</td>
      <td>84.40</td>
      <td>63.72</td>
      <td>52.52</td>
      <td>78.37</td>
      <td>21.38</td>
      <td>8.66</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>7.0</td>
      <td>True</td>
      <td>d4039b40e842df7f6b8de50532444c8944ea5791</td>
      <td>uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CalderaAI/30B-Lazarus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CalderaAI__30B-Lazarus</td>
      <td>53.33</td>
      <td>64.93</td>
      <td>84.27</td>
      <td>56.47</td>
      <td>58.65</td>
      <td>78.37</td>
      <td>7.73</td>
      <td>22.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>116.0</td>
      <td>True</td>
      <td>24da9e88f2b2b7946bc6fe9412d6728b9adc2c3d</td>
      <td>CalderaAI/30B-Lazarus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewd-ReMM-L2-Chat-20B</td>
      <td>53.33</td>
      <td>62.46</td>
      <td>85.62</td>
      <td>59.13</td>
      <td>55.63</td>
      <td>77.19</td>
      <td>10.92</td>
      <td>22.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>6.0</td>
      <td>True</td>
      <td>cda06630a1d8173541431e5ce8bc17dcfaa37e5e</td>
      <td>Undi95/MLewd-ReMM-L2-Chat-20B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-hermes-coig-lite-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-hermes-coig-lite-13b</td>
      <td>53.31</td>
      <td>59.47</td>
      <td>82.28</td>
      <td>55.18</td>
      <td>47.60</td>
      <td>78.61</td>
      <td>10.77</td>
      <td>39.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>[mit]</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ee11d9c7acaefb723796227e2ad099b165f0dd9</td>
      <td>uukuguy/speechless-hermes-coig-lite-13b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llama-65b</td>
      <td>53.31</td>
      <td>63.48</td>
      <td>86.09</td>
      <td>63.93</td>
      <td>43.43</td>
      <td>82.56</td>
      <td>27.67</td>
      <td>5.98</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.29</td>
      <td>0.0</td>
      <td>False</td>
      <td>4ae2e56610e8b9b9a78472708390668e9096b4f9</td>
      <td>huggingface/llama-65b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mistralai__Mistral-7B-Instruct-v0.1</td>
      <td>53.27</td>
      <td>54.52</td>
      <td>75.63</td>
      <td>55.38</td>
      <td>56.28</td>
      <td>73.72</td>
      <td>14.25</td>
      <td>43.10</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>698.0</td>
      <td>True</td>
      <td>7961f5aa9b736bf8e364b2e6f201190f97a27931</td>
      <td>mistralai/Mistral-7B-Instruct-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/royallab/Pygmalion-2-13b-SuperCOT</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_royallab__Pygmalion-2-13b-SuperCOT</td>
      <td>53.27</td>
      <td>63.23</td>
      <td>83.68</td>
      <td>54.90</td>
      <td>53.14</td>
      <td>77.51</td>
      <td>6.29</td>
      <td>34.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>7.0</td>
      <td>True</td>
      <td>763b3fd5afc3e7fb6c7c8768d40f06901c8d5913</td>
      <td>royallab/Pygmalion-2-13b-SuperCOT</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/OpenRP-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__OpenRP-13B</td>
      <td>53.25</td>
      <td>62.12</td>
      <td>82.60</td>
      <td>57.50</td>
      <td>48.29</td>
      <td>76.01</td>
      <td>12.89</td>
      <td>33.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>d11815287c51ef51485fb003f8f72773cf6f19a4</td>
      <td>Undi95/OpenRP-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-hermes-coig-lite-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-hermes-coig-lite-13b</td>
      <td>53.22</td>
      <td>59.56</td>
      <td>82.26</td>
      <td>55.30</td>
      <td>47.56</td>
      <td>78.53</td>
      <td>9.86</td>
      <td>39.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>[mit]</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ee11d9c7acaefb723796227e2ad099b165f0dd9</td>
      <td>uukuguy/speechless-hermes-coig-lite-13b</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yhyu13__oasst-rlhf-2-llama-30b-7k-steps-hf</td>
      <td>53.18</td>
      <td>61.35</td>
      <td>83.80</td>
      <td>57.89</td>
      <td>51.18</td>
      <td>78.77</td>
      <td>31.46</td>
      <td>7.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>6.0</td>
      <td>True</td>
      <td>e04207847429af03c4780f5ac85c726536217981</td>
      <td>Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/minlik/chinese-alpaca-33b-merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_minlik__chinese-alpaca-33b-merged</td>
      <td>53.09</td>
      <td>59.30</td>
      <td>78.43</td>
      <td>57.69</td>
      <td>52.45</td>
      <td>76.09</td>
      <td>8.04</td>
      <td>39.67</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.44</td>
      <td>10.0</td>
      <td>True</td>
      <td>fc2535104c0b48afc42575f9fe10bbcbb7612ec3</td>
      <td>minlik/chinese-alpaca-33b-merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Mistral-11B-TestBench9</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Mistral-11B-TestBench9</td>
      <td>53.06</td>
      <td>64.08</td>
      <td>84.24</td>
      <td>64.00</td>
      <td>56.19</td>
      <td>78.45</td>
      <td>16.15</td>
      <td>8.35</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>4ff48527af8c3907129c06160c7f7b7b786a5a79</td>
      <td>Undi95/Mistral-11B-TestBench9</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/01-ai/Yi-6B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_01-ai__Yi-6B</td>
      <td>53.06</td>
      <td>55.55</td>
      <td>76.42</td>
      <td>63.85</td>
      <td>41.86</td>
      <td>73.80</td>
      <td>12.66</td>
      <td>47.32</td>
      <td>YiForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>custom</td>
      <td>6.00</td>
      <td>15.0</td>
      <td>True</td>
      <td>d8029c814d8faa68e1aef2e488f668a3af5d1a8a</td>
      <td>01-ai/Yi-6B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ausboss/llama-30b-supercot</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ausboss__llama-30b-supercot</td>
      <td>53.06</td>
      <td>64.85</td>
      <td>85.08</td>
      <td>56.56</td>
      <td>53.96</td>
      <td>80.03</td>
      <td>11.90</td>
      <td>19.07</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>124.0</td>
      <td>True</td>
      <td>dc9d81f454d286ea040c5cd45b058aecaa51c13e</td>
      <td>ausboss/llama-30b-supercot</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/posicube/Llama-chat-AY-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_posicube__Llama-chat-AY-13B</td>
      <td>53.04</td>
      <td>62.80</td>
      <td>83.23</td>
      <td>60.01</td>
      <td>55.95</td>
      <td>75.93</td>
      <td>12.13</td>
      <td>21.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>66037b5ee553f7b878d796d2b2d5ada5734cc164</td>
      <td>posicube/Llama-chat-AY-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/LlongOrca-13B-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__LlongOrca-13B-16k</td>
      <td>53.02</td>
      <td>62.46</td>
      <td>82.75</td>
      <td>55.54</td>
      <td>50.11</td>
      <td>76.40</td>
      <td>12.28</td>
      <td>31.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>11.0</td>
      <td>True</td>
      <td>8ea1fb205553cadbc90069d80a7e58281b6281c3</td>
      <td>Open-Orca/LlongOrca-13B-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Mistral-11B-TestBench11</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Mistral-11B-TestBench11</td>
      <td>53.01</td>
      <td>64.42</td>
      <td>83.93</td>
      <td>63.82</td>
      <td>56.68</td>
      <td>77.74</td>
      <td>14.94</td>
      <td>9.57</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>10.73</td>
      <td>7.0</td>
      <td>True</td>
      <td>9aae2b156b24557bb98e515f3a90c7865529d2e9</td>
      <td>Undi95/Mistral-11B-TestBench11</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/posicube/Llama2-chat-AYB-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_posicube__Llama2-chat-AYB-13B</td>
      <td>53.01</td>
      <td>63.40</td>
      <td>84.79</td>
      <td>59.34</td>
      <td>55.62</td>
      <td>76.24</td>
      <td>11.30</td>
      <td>20.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>10.0</td>
      <td>True</td>
      <td>cc7ca1b8f906b9f62ace094540f4ff4124dd581a</td>
      <td>posicube/Llama2-chat-AYB-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Weyaxi/Dolphin2.1-OpenOrca-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__Dolphin2.1-OpenOrca-7B</td>
      <td>53.00</td>
      <td>63.91</td>
      <td>84.26</td>
      <td>62.66</td>
      <td>53.84</td>
      <td>78.22</td>
      <td>19.94</td>
      <td>8.17</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>076c0f7de93307e8fb3ad3bd820fb5f73325ca70</td>
      <td>Weyaxi/Dolphin2.1-OpenOrca-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ziqingyang/chinese-alpaca-2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ziqingyang__chinese-alpaca-2-13b</td>
      <td>53.00</td>
      <td>58.70</td>
      <td>79.74</td>
      <td>55.10</td>
      <td>50.22</td>
      <td>75.69</td>
      <td>10.46</td>
      <td>41.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.97</td>
      <td>64.0</td>
      <td>True</td>
      <td>576094cbf4988baf88b3bb66678be1db70bd720a</td>
      <td>ziqingyang/chinese-alpaca-2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-FP16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-13b-FP16</td>
      <td>52.97</td>
      <td>60.58</td>
      <td>82.53</td>
      <td>53.71</td>
      <td>54.46</td>
      <td>73.72</td>
      <td>4.32</td>
      <td>41.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>11.0</td>
      <td>True</td>
      <td>69615d9a8e1547f2407afd3380868a99f780e008</td>
      <td>The-Face-Of-Goonery/Huginn-13b-FP16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PulsarAI/SlimOpenOrca-Mistral-7B-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__SlimOpenOrca-Mistral-7B-v2</td>
      <td>52.96</td>
      <td>62.88</td>
      <td>83.41</td>
      <td>62.05</td>
      <td>56.65</td>
      <td>77.58</td>
      <td>18.95</td>
      <td>9.19</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>7cd030ccdb169c2685fe028bb4380b91ad74920f</td>
      <td>PulsarAI/SlimOpenOrca-Mistral-7B-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Weyaxi/Dolphin2.1-OpenOrca-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__Dolphin2.1-OpenOrca-7B</td>
      <td>52.92</td>
      <td>64.16</td>
      <td>84.25</td>
      <td>62.70</td>
      <td>53.83</td>
      <td>77.66</td>
      <td>19.71</td>
      <td>8.15</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>076c0f7de93307e8fb3ad3bd820fb5f73325ca70</td>
      <td>Weyaxi/Dolphin2.1-OpenOrca-7B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/MythoMix-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__MythoMix-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>52.91</td>
      <td>60.32</td>
      <td>83.72</td>
      <td>55.74</td>
      <td>52.18</td>
      <td>75.53</td>
      <td>0.91</td>
      <td>41.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>3d91f63d82abd598d5b80d24d74feb6b00b7d80f</td>
      <td>TFLai/MythoMix-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>52.89</td>
      <td>59.90</td>
      <td>83.29</td>
      <td>56.69</td>
      <td>51.08</td>
      <td>75.22</td>
      <td>1.44</td>
      <td>42.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>6e49d3d205e7f2e15c01ace0901da8931bbaab3b</td>
      <td>TFLai/Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Weyaxi/Samantha-Nebula-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__Samantha-Nebula-7B</td>
      <td>52.87</td>
      <td>57.00</td>
      <td>82.25</td>
      <td>54.21</td>
      <td>49.58</td>
      <td>73.09</td>
      <td>11.37</td>
      <td>42.57</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>a7d4b8a1683e33dd3c60064d7dd9d5c35691323f</td>
      <td>Weyaxi/Samantha-Nebula-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openbmb/UltraLM-13b-v2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openbmb__UltraLM-13b-v2.0</td>
      <td>52.85</td>
      <td>62.63</td>
      <td>81.49</td>
      <td>56.17</td>
      <td>49.48</td>
      <td>76.48</td>
      <td>10.99</td>
      <td>32.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>a452045c96ae62379a98ef0d85666616a66e78a6</td>
      <td>openbmb/UltraLM-13b-v2.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Weyaxi/TekniumAiroboros-Nebula-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__TekniumAiroboros-Nebula-7B</td>
      <td>52.82</td>
      <td>57.17</td>
      <td>81.72</td>
      <td>55.25</td>
      <td>51.64</td>
      <td>73.24</td>
      <td>9.40</td>
      <td>41.33</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>ef964d514cc25a600b0de78fc469d1acbec34591</td>
      <td>Weyaxi/TekniumAiroboros-Nebula-7B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Envoid/Libra-19B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Envoid__Libra-19B</td>
      <td>52.80</td>
      <td>60.58</td>
      <td>82.04</td>
      <td>55.57</td>
      <td>48.41</td>
      <td>76.32</td>
      <td>0.08</td>
      <td>46.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.20</td>
      <td>1.0</td>
      <td>True</td>
      <td>a4e1f8f62740d676c25eedb4f29f4e776dcc0c22</td>
      <td>Envoid/Libra-19B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r8-q_k_v_o</td>
      <td>52.79</td>
      <td>57.25</td>
      <td>81.73</td>
      <td>55.72</td>
      <td>41.53</td>
      <td>77.58</td>
      <td>14.03</td>
      <td>41.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>209da26cff560ab34064f277190ab63f8c970b93</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-Mistral-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-Mistral-13B</td>
      <td>52.76</td>
      <td>62.20</td>
      <td>83.82</td>
      <td>55.43</td>
      <td>53.32</td>
      <td>74.51</td>
      <td>12.05</td>
      <td>27.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>a5ef9385d9430a81778183d71b58eb2b869d6a7e</td>
      <td>Undi95/ReMM-Mistral-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/llama-30b-2048-instruct-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__llama-30b-2048-instruct-PL-lora_unload</td>
      <td>52.75</td>
      <td>63.82</td>
      <td>84.70</td>
      <td>61.49</td>
      <td>52.49</td>
      <td>79.79</td>
      <td>17.89</td>
      <td>9.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>b15f4310ea37fef99e4f16372a4b1f2342e27613</td>
      <td>Aspik101/llama-30b-2048-instruct-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-2-13b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-2-13b-instruct</td>
      <td>52.74</td>
      <td>61.18</td>
      <td>83.25</td>
      <td>55.92</td>
      <td>51.08</td>
      <td>77.35</td>
      <td>2.05</td>
      <td>38.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>11.0</td>
      <td>True</td>
      <td>ac1f326ea75a28197c4b8e7c015071e8eef64485</td>
      <td>bofenghuang/vigogne-2-13b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewd-Chat-v2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewd-Chat-v2-13B</td>
      <td>52.72</td>
      <td>61.86</td>
      <td>83.81</td>
      <td>57.00</td>
      <td>54.51</td>
      <td>75.77</td>
      <td>10.46</td>
      <td>25.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>f6181961a6a2f9ca534e1a8907b4a4459be6b6bd</td>
      <td>Undi95/MLewd-Chat-v2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/llama-2-13b-Beluga-QLoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__llama-2-13b-Beluga-QLoRA</td>
      <td>52.70</td>
      <td>59.22</td>
      <td>81.92</td>
      <td>56.67</td>
      <td>48.23</td>
      <td>77.19</td>
      <td>1.29</td>
      <td>44.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c0d3c0a5d4e9001ea933c6b71ca3adc99d1f71a2</td>
      <td>yeontaek/llama-2-13b-Beluga-QLoRA</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Voicelab/trurl-2-13b-academic</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Voicelab__trurl-2-13b-academic</td>
      <td>52.70</td>
      <td>57.94</td>
      <td>79.55</td>
      <td>55.20</td>
      <td>43.46</td>
      <td>76.56</td>
      <td>10.92</td>
      <td>45.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2e95049edf02368bbd4b4f6ffb50bc8821e919bb</td>
      <td>Voicelab/trurl-2-13b-academic</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/IkariDev/Athena-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_IkariDev__Athena-v1</td>
      <td>52.68</td>
      <td>60.07</td>
      <td>82.64</td>
      <td>55.61</td>
      <td>46.58</td>
      <td>74.82</td>
      <td>4.93</td>
      <td>44.11</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>11.0</td>
      <td>True</td>
      <td>8f96e561c8c795e383ca0faeb1696fa1e33e87de</td>
      <td>IkariDev/Athena-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-orca-platypus-coig-lite-4k-0.6e-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-orca-platypus-coig-lite-4k-0.6e-13b</td>
      <td>52.65</td>
      <td>58.79</td>
      <td>79.93</td>
      <td>56.77</td>
      <td>48.29</td>
      <td>75.93</td>
      <td>4.25</td>
      <td>44.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>6bf4cf6211489bdbea70585a4a5c0f39deefb4e5</td>
      <td>uukuguy/speechless-orca-platypus-coig-lite-4k-0.6e-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigostral-7b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigostral-7b-chat</td>
      <td>52.62</td>
      <td>62.63</td>
      <td>84.34</td>
      <td>63.53</td>
      <td>49.24</td>
      <td>78.61</td>
      <td>16.76</td>
      <td>13.26</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>969fbfc7a91f53c8562a2c48a3c24dd3745d5a97</td>
      <td>bofenghuang/vigostral-7b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-L2-13B</td>
      <td>52.58</td>
      <td>59.73</td>
      <td>83.10</td>
      <td>54.11</td>
      <td>49.94</td>
      <td>74.51</td>
      <td>2.96</td>
      <td>43.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>c4710577003a23ca8e9040d16dfb8f3e9bc5d636</td>
      <td>Undi95/ReMM-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-L2-13B-PIPPA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-L2-13B-PIPPA</td>
      <td>52.58</td>
      <td>59.73</td>
      <td>83.12</td>
      <td>54.10</td>
      <td>49.94</td>
      <td>74.51</td>
      <td>2.96</td>
      <td>43.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>79e711178c6881496ae1f5635b08bc193f370709</td>
      <td>Undi95/ReMM-L2-13B-PIPPA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/UndiMix-v1-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__UndiMix-v1-13b</td>
      <td>52.56</td>
      <td>59.47</td>
      <td>82.45</td>
      <td>55.83</td>
      <td>49.78</td>
      <td>75.45</td>
      <td>10.01</td>
      <td>34.95</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fd311f52648825d6988d2f945918468ceb32289f</td>
      <td>Undi95/UndiMix-v1-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Llama-2-70B-chat-GPTQ</td>
      <td>52.56</td>
      <td>62.63</td>
      <td>84.81</td>
      <td>62.74</td>
      <td>50.98</td>
      <td>78.69</td>
      <td>18.65</td>
      <td>9.40</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>72.82</td>
      <td>204.0</td>
      <td>True</td>
      <td>054fbf6f65e7ab7691ec07ec9ad366acf2dd90bf</td>
      <td>TheBloke/Llama-2-70B-chat-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/llama-30b-instruct-2048-PL-lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__llama-30b-instruct-2048-PL-lora</td>
      <td>52.55</td>
      <td>63.31</td>
      <td>84.66</td>
      <td>61.66</td>
      <td>53.35</td>
      <td>79.08</td>
      <td>16.83</td>
      <td>8.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>other</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>1a076bce564f03bd47951eecab628c541fb1a6ad</td>
      <td>Aspik101/llama-30b-instruct-2048-PL-lora</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/teknium/CollectiveCognition-v1-Mistral-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_teknium__CollectiveCognition-v1-Mistral-7B</td>
      <td>52.55</td>
      <td>62.37</td>
      <td>85.50</td>
      <td>62.76</td>
      <td>54.48</td>
      <td>77.58</td>
      <td>17.89</td>
      <td>7.22</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>4.0</td>
      <td>True</td>
      <td>58777f0563610fa770c4fa252c0350de71d4ab9d</td>
      <td>teknium/CollectiveCognition-v1-Mistral-7B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4</td>
      <td>52.54</td>
      <td>63.74</td>
      <td>84.87</td>
      <td>58.54</td>
      <td>47.06</td>
      <td>77.03</td>
      <td>12.66</td>
      <td>23.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>7.0</td>
      <td>True</td>
      <td>5b6bd680b1c008e52521dc8c663dbc87820da3d0</td>
      <td>jondurbin/airoboros-33b-gpt4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/speechlessai/speechless-llama2-dolphin-orca-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_speechlessai__speechless-llama2-dolphin-orca-platypus-13b</td>
      <td>52.54</td>
      <td>59.64</td>
      <td>82.65</td>
      <td>57.90</td>
      <td>43.44</td>
      <td>77.19</td>
      <td>9.70</td>
      <td>37.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>fd23b7d052eb7c18ecd2acc1be77c66b7b8d6dad</td>
      <td>speechlessai/speechless-llama2-dolphin-orca-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Brouz/Slerpeno</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Brouz__Slerpeno</td>
      <td>52.50</td>
      <td>61.69</td>
      <td>84.10</td>
      <td>56.77</td>
      <td>48.05</td>
      <td>76.40</td>
      <td>12.51</td>
      <td>28.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-4.0</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>7ff32abd17851a769a031659e91e660f219be363</td>
      <td>Brouz/Slerpeno</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HuggingFaceH4__zephyr-7b-alpha</td>
      <td>52.40</td>
      <td>61.01</td>
      <td>84.04</td>
      <td>61.39</td>
      <td>57.90</td>
      <td>78.61</td>
      <td>14.03</td>
      <td>9.82</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>mit</td>
      <td>7.11</td>
      <td>377.0</td>
      <td>True</td>
      <td>2cd2cd16a6ab22585d643cf264fac73b18e7852a</td>
      <td>HuggingFaceH4/zephyr-7b-alpha</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/boomerchan/magpie-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_boomerchan__magpie-13b</td>
      <td>52.37</td>
      <td>63.31</td>
      <td>84.25</td>
      <td>58.15</td>
      <td>49.15</td>
      <td>76.48</td>
      <td>14.48</td>
      <td>20.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>a58124cdc9f39ccd59d4290a8bdfda93ff3690dc</td>
      <td>boomerchan/magpie-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Nous-Hermes-Llama2-13b</td>
      <td>52.35</td>
      <td>61.52</td>
      <td>83.29</td>
      <td>55.11</td>
      <td>50.38</td>
      <td>75.45</td>
      <td>10.08</td>
      <td>30.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>[mit]</td>
      <td>12.85</td>
      <td>221.0</td>
      <td>True</td>
      <td>8f95aa9cd207db7b24179fc779c2b8973e71bee2</td>
      <td>NousResearch/Nous-Hermes-Llama2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/WizardLM-30B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-30B-Uncensored</td>
      <td>52.32</td>
      <td>60.24</td>
      <td>82.93</td>
      <td>56.80</td>
      <td>51.57</td>
      <td>74.35</td>
      <td>12.89</td>
      <td>27.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>118.0</td>
      <td>True</td>
      <td>761783745fcb97831ad8035d3cbd5de484aca3ce</td>
      <td>ehartford/WizardLM-30B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj</td>
      <td>52.29</td>
      <td>59.73</td>
      <td>81.06</td>
      <td>54.53</td>
      <td>38.64</td>
      <td>78.14</td>
      <td>14.03</td>
      <td>39.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aeeded8db9eea97e2e6a2e19a006ce1acd110a82</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w-q_k_v_o_proj</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama2-13b</td>
      <td>52.25</td>
      <td>59.13</td>
      <td>81.99</td>
      <td>55.49</td>
      <td>51.57</td>
      <td>74.66</td>
      <td>11.22</td>
      <td>31.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>20.0</td>
      <td>True</td>
      <td>6e918dc8beb1e764def5938fdb8e3f64ba40a456</td>
      <td>YeungNLP/firefly-llama2-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/pankajmathur/orca_mini_v3_13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pankajmathur__orca_mini_v3_13b</td>
      <td>52.23</td>
      <td>63.14</td>
      <td>82.35</td>
      <td>56.52</td>
      <td>51.81</td>
      <td>76.48</td>
      <td>13.12</td>
      <td>22.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>27.0</td>
      <td>True</td>
      <td>72eec98f68d240a71d3da8a266917b6e754ae831</td>
      <td>pankajmathur/orca_mini_v3_13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_v3_13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_v3_13b</td>
      <td>52.23</td>
      <td>63.14</td>
      <td>82.35</td>
      <td>56.52</td>
      <td>51.81</td>
      <td>76.48</td>
      <td>13.12</td>
      <td>22.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>27.0</td>
      <td>True</td>
      <td>99904e4119575f2c1606ca1e31d288f38a9f20b5</td>
      <td>psmathur/orca_mini_v3_13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Gryphe/MythoLogic-L2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Gryphe__MythoLogic-L2-13b</td>
      <td>52.22</td>
      <td>61.01</td>
      <td>83.93</td>
      <td>55.70</td>
      <td>48.64</td>
      <td>76.09</td>
      <td>11.75</td>
      <td>28.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>15.0</td>
      <td>True</td>
      <td>665948fc79acc2bcce3e9e7d2b0689ca43ae62d4</td>
      <td>Gryphe/MythoLogic-L2-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/platypus-2-22b-relora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__platypus-2-22b-relora</td>
      <td>52.21</td>
      <td>57.68</td>
      <td>82.44</td>
      <td>55.33</td>
      <td>43.61</td>
      <td>77.35</td>
      <td>6.60</td>
      <td>42.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>21.83</td>
      <td>0.0</td>
      <td>True</td>
      <td>15bca3e9b25cc2f280fec21686ef3bc445217503</td>
      <td>chargoddard/platypus-2-22b-relora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o</td>
      <td>52.21</td>
      <td>57.68</td>
      <td>81.91</td>
      <td>54.95</td>
      <td>41.31</td>
      <td>76.48</td>
      <td>12.05</td>
      <td>41.07</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f76f93dad8408523e69c59abbb96ce6b1b9b9f69</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r16-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r16-q_k_v_o</td>
      <td>52.20</td>
      <td>58.70</td>
      <td>81.66</td>
      <td>53.87</td>
      <td>43.02</td>
      <td>76.72</td>
      <td>13.80</td>
      <td>37.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>33fd8a46a711ab8c45698dae9601678dfd7b3d33</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r16-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-1.2</td>
      <td>52.20</td>
      <td>64.42</td>
      <td>84.93</td>
      <td>60.35</td>
      <td>49.18</td>
      <td>77.51</td>
      <td>9.78</td>
      <td>19.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>8.0</td>
      <td>True</td>
      <td>b3254a827fb1dfe0d4e428bf5ab1c3a2bac82d68</td>
      <td>jondurbin/airoboros-33b-gpt4-1.2</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/digitous/13B-Chimera</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__13B-Chimera</td>
      <td>52.19</td>
      <td>57.59</td>
      <td>81.50</td>
      <td>49.86</td>
      <td>52.59</td>
      <td>77.27</td>
      <td>10.69</td>
      <td>35.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>85cfe8e6db2bee804873cfdb48955696cc5b0689</td>
      <td>digitous/13B-Chimera</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MayaPH/GodziLLa-30B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MayaPH__GodziLLa-30B</td>
      <td>52.17</td>
      <td>61.52</td>
      <td>82.13</td>
      <td>54.21</td>
      <td>55.91</td>
      <td>76.16</td>
      <td>0.38</td>
      <td>34.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>8.0</td>
      <td>True</td>
      <td>aa9912a2ac60abeac28b4566731cd903dcc582ac</td>
      <td>MayaPH/GodziLLa-30B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/samantha-1.2-mistral-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__samantha-1.2-mistral-7b</td>
      <td>52.16</td>
      <td>64.08</td>
      <td>85.08</td>
      <td>63.91</td>
      <td>50.40</td>
      <td>78.53</td>
      <td>16.98</td>
      <td>6.13</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>8.0</td>
      <td>True</td>
      <td>5574a021f55a446a756dcbc776f1765aefc280a1</td>
      <td>ehartford/samantha-1.2-mistral-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Camel-Platypus2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Camel-Platypus2-13B</td>
      <td>52.12</td>
      <td>60.75</td>
      <td>83.61</td>
      <td>56.51</td>
      <td>49.60</td>
      <td>75.37</td>
      <td>0.08</td>
      <td>38.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>0480a52799cb8e8de73bb41994df8b6b793937c7</td>
      <td>garage-bAInd/Camel-Platypus2-13B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TigerResearch/tigerbot-13b-base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TigerResearch__tigerbot-13b-base</td>
      <td>52.11</td>
      <td>53.84</td>
      <td>77.05</td>
      <td>53.57</td>
      <td>44.06</td>
      <td>74.98</td>
      <td>17.06</td>
      <td>44.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>7.0</td>
      <td>True</td>
      <td>2df5ed76be7eff0962f2d816a64eca1e78e1cbf3</td>
      <td>TigerResearch/tigerbot-13b-base</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/platypus2-22b-relora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__platypus2-22b-relora</td>
      <td>52.11</td>
      <td>57.51</td>
      <td>82.36</td>
      <td>54.94</td>
      <td>43.62</td>
      <td>77.11</td>
      <td>6.29</td>
      <td>42.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>21.83</td>
      <td>0.0</td>
      <td>True</td>
      <td>15bca3e9b25cc2f280fec21686ef3bc445217503</td>
      <td>chargoddard/platypus2-22b-relora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-orca-platypus-coig-lite-4k-0.5e-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-orca-platypus-coig-lite-4k-0.5e-13b</td>
      <td>52.09</td>
      <td>58.02</td>
      <td>80.15</td>
      <td>57.26</td>
      <td>48.04</td>
      <td>75.45</td>
      <td>5.84</td>
      <td>39.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>081d1da5cfa2f6ad43abdf4fb5e41f8ec5846224</td>
      <td>uukuguy/speechless-orca-platypus-coig-lite-4k-0.5e-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o</td>
      <td>52.08</td>
      <td>59.30</td>
      <td>81.20</td>
      <td>55.58</td>
      <td>38.13</td>
      <td>76.80</td>
      <td>13.50</td>
      <td>40.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>71224344025dbfada6821c6a89cade1d8358dad1</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/CreativityEngine</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__CreativityEngine</td>
      <td>52.07</td>
      <td>59.30</td>
      <td>82.42</td>
      <td>53.55</td>
      <td>52.46</td>
      <td>74.19</td>
      <td>9.55</td>
      <td>32.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>7870cc50b82b5cbebfa9935b6d73a9d20170299a</td>
      <td>Undi95/CreativityEngine</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o</td>
      <td>52.06</td>
      <td>56.06</td>
      <td>81.89</td>
      <td>55.04</td>
      <td>40.12</td>
      <td>76.56</td>
      <td>14.25</td>
      <td>40.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f907fffbb08698040325b3f2e47200a1b48b3ed9</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Qwen/Qwen-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Qwen__Qwen-7B</td>
      <td>52.05</td>
      <td>51.37</td>
      <td>78.47</td>
      <td>59.84</td>
      <td>47.79</td>
      <td>72.69</td>
      <td>44.96</td>
      <td>9.25</td>
      <td>QWenLMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.72</td>
      <td>288.0</td>
      <td>True</td>
      <td>c9bdb955021a80ae26fa6978891996dbe4951d8d</td>
      <td>Qwen/Qwen-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Nous-Hermes-Llama2-13b</td>
      <td>52.03</td>
      <td>61.26</td>
      <td>83.26</td>
      <td>55.04</td>
      <td>50.41</td>
      <td>75.37</td>
      <td>9.17</td>
      <td>29.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>[mit]</td>
      <td>12.85</td>
      <td>221.0</td>
      <td>True</td>
      <td>8f95aa9cd207db7b24179fc779c2b8973e71bee2</td>
      <td>NousResearch/Nous-Hermes-Llama2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama2-13b-v1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama2-13b-v1.2</td>
      <td>52.03</td>
      <td>60.67</td>
      <td>80.46</td>
      <td>56.51</td>
      <td>51.03</td>
      <td>74.82</td>
      <td>11.75</td>
      <td>28.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>97279d20a8c7e2d0576c9ff4b2e15a421c40d58a</td>
      <td>YeungNLP/firefly-llama2-13b-v1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-1.8-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-1.8-L2-13B</td>
      <td>52.00</td>
      <td>63.48</td>
      <td>84.12</td>
      <td>58.57</td>
      <td>52.86</td>
      <td>76.40</td>
      <td>13.27</td>
      <td>15.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>fe054ab749a69375285df40913a88bd40f1e2bf6</td>
      <td>Sao10K/Stheno-1.8-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/circulus/Llama-2-13b-orca-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_circulus__Llama-2-13b-orca-v1</td>
      <td>51.99</td>
      <td>62.20</td>
      <td>82.32</td>
      <td>57.67</td>
      <td>49.60</td>
      <td>76.80</td>
      <td>12.89</td>
      <td>22.47</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>e77ec90f432bdffa210a0e4310d117e5d1c662df</td>
      <td>circulus/Llama-2-13b-orca-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Nous-Hermes-13B-Code</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Nous-Hermes-13B-Code</td>
      <td>51.98</td>
      <td>61.18</td>
      <td>83.21</td>
      <td>55.13</td>
      <td>50.56</td>
      <td>75.14</td>
      <td>10.39</td>
      <td>28.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>5a45cb2a6442581ce32cc19c561c49cec1db4ebb</td>
      <td>Undi95/Nous-Hermes-13B-Code</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_Fintune_1_17w</td>
      <td>51.96</td>
      <td>59.47</td>
      <td>81.00</td>
      <td>54.31</td>
      <td>38.17</td>
      <td>77.27</td>
      <td>13.27</td>
      <td>40.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa5b161b39900c5e80d5bb39d098f6333ad964f7</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Doctor-Shotgun/CalliopeDS-v2-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Doctor-Shotgun__CalliopeDS-v2-L2-13B</td>
      <td>51.95</td>
      <td>62.80</td>
      <td>84.14</td>
      <td>56.14</td>
      <td>51.06</td>
      <td>76.01</td>
      <td>12.59</td>
      <td>20.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>e63d24870c840d47e82b029e7f405baa10ad9ea4</td>
      <td>Doctor-Shotgun/CalliopeDS-v2-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/circulus/Llama-2-13b-orca-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_circulus__Llama-2-13b-orca-v1</td>
      <td>51.94</td>
      <td>62.03</td>
      <td>82.27</td>
      <td>57.71</td>
      <td>49.61</td>
      <td>76.87</td>
      <td>13.80</td>
      <td>21.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>e77ec90f432bdffa210a0e4310d117e5d1c662df</td>
      <td>circulus/Llama-2-13b-orca-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/stabilityai/StableBeluga-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__StableBeluga-13B</td>
      <td>51.94</td>
      <td>62.03</td>
      <td>82.27</td>
      <td>57.71</td>
      <td>49.61</td>
      <td>76.87</td>
      <td>13.80</td>
      <td>21.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>107.0</td>
      <td>True</td>
      <td>1d6eef4cc2b73f39600a568803ad8183f2da4514</td>
      <td>stabilityai/StableBeluga-13B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/gpt4-alpaca-lora-30b-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__gpt4-alpaca-lora-30b-HF</td>
      <td>51.93</td>
      <td>64.85</td>
      <td>85.72</td>
      <td>58.51</td>
      <td>52.24</td>
      <td>80.19</td>
      <td>15.54</td>
      <td>6.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>9.0</td>
      <td>True</td>
      <td>3c8007467a081dc72ae09b9d358416b056b38920</td>
      <td>TheBloke/gpt4-alpaca-lora-30b-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WhoTookMyAmogusNickname/NewHope_HF_not_official</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WhoTookMyAmogusNickname__NewHope_HF_not_official</td>
      <td>51.90</td>
      <td>61.09</td>
      <td>84.03</td>
      <td>55.73</td>
      <td>44.96</td>
      <td>74.98</td>
      <td>15.85</td>
      <td>26.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f587f4a31de6818f4200d9cdc7f116ca8ba1cdc2</td>
      <td>WhoTookMyAmogusNickname/NewHope_HF_not_official</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj</td>
      <td>51.90</td>
      <td>58.53</td>
      <td>82.47</td>
      <td>53.90</td>
      <td>37.92</td>
      <td>76.80</td>
      <td>12.81</td>
      <td>40.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d74752b931bfddaa063a292e7ea85dfb1d7a4998</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w-q_k_v_o_proj</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o</td>
      <td>51.89</td>
      <td>56.23</td>
      <td>81.98</td>
      <td>55.87</td>
      <td>39.76</td>
      <td>76.72</td>
      <td>11.52</td>
      <td>41.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>cc3c5e5a874cf4ff4f94ea919e819f8a914c8acb</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down</td>
      <td>51.89</td>
      <td>57.25</td>
      <td>81.49</td>
      <td>55.90</td>
      <td>39.79</td>
      <td>75.77</td>
      <td>12.05</td>
      <td>40.95</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a12fb5937e6904977e8123b0d5ef21283b6895d4</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/mncai/Mistral-7B-OpenOrca-1k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mncai__Mistral-7B-OpenOrca-1k</td>
      <td>51.88</td>
      <td>62.97</td>
      <td>84.66</td>
      <td>62.20</td>
      <td>52.96</td>
      <td>78.61</td>
      <td>11.98</td>
      <td>9.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.11</td>
      <td>1.0</td>
      <td>True</td>
      <td>ae9e37811a54ffe45f41a572c7e68363aa11b062</td>
      <td>mncai/Mistral-7B-OpenOrca-1k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gaodrew/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gaodrew__gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps</td>
      <td>51.87</td>
      <td>61.52</td>
      <td>84.06</td>
      <td>60.23</td>
      <td>51.05</td>
      <td>80.82</td>
      <td>17.66</td>
      <td>7.75</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>1114ff08ed15ef417502da58f0237d2f6650c9ce</td>
      <td>gaodrew/gaodrew-llama-30b-instruct-2048-Open-Platypus-100steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Emerhyst-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Emerhyst-20B</td>
      <td>51.85</td>
      <td>61.69</td>
      <td>84.98</td>
      <td>56.98</td>
      <td>54.16</td>
      <td>76.09</td>
      <td>8.49</td>
      <td>20.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>16.0</td>
      <td>True</td>
      <td>e4c23af4f5dd88cb27d245e2bfc3b81db652632c</td>
      <td>Undi95/Emerhyst-20B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-llama2-hermes-orca-platypus-wizardlm-13b</td>
      <td>51.85</td>
      <td>59.64</td>
      <td>82.70</td>
      <td>58.30</td>
      <td>56.00</td>
      <td>75.37</td>
      <td>13.12</td>
      <td>17.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>24.0</td>
      <td>True</td>
      <td>4410d8a20871927e9fe981c01bc8314b451b2fcd</td>
      <td>uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o</td>
      <td>51.83</td>
      <td>59.04</td>
      <td>81.15</td>
      <td>53.00</td>
      <td>40.16</td>
      <td>76.48</td>
      <td>11.90</td>
      <td>41.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ac40ecf48cf5f7168e8c3929632c654bc834c3d7</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gradientputri/MegaMix-T1-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gradientputri__MegaMix-T1-13B</td>
      <td>51.82</td>
      <td>61.35</td>
      <td>83.44</td>
      <td>58.49</td>
      <td>48.19</td>
      <td>76.09</td>
      <td>24.11</td>
      <td>11.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>55d31300f8972b56320855bb40efb5e3d1e1a6fc</td>
      <td>gradientputri/MegaMix-T1-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-34b-v1.9</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-34b-v1.9</td>
      <td>51.80</td>
      <td>54.27</td>
      <td>75.20</td>
      <td>56.12</td>
      <td>43.92</td>
      <td>73.56</td>
      <td>24.79</td>
      <td>34.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>0.0</td>
      <td>True</td>
      <td>68aad9f8452b2abf7d5415d48c09bd55d5b7ca05</td>
      <td>uukuguy/speechless-codellama-34b-v1.9</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Enno-Ai/vigogne2-enno-13b-sft-lora-4bit</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Enno-Ai__vigogne2-enno-13b-sft-lora-4bit</td>
      <td>51.79</td>
      <td>62.03</td>
      <td>82.65</td>
      <td>54.11</td>
      <td>42.98</td>
      <td>76.95</td>
      <td>0.15</td>
      <td>43.65</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2a1b03977395eee44742abda63a4787ea5371d06</td>
      <td>Enno-Ai/vigogne2-enno-13b-sft-lora-4bit</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-2.1</td>
      <td>51.78</td>
      <td>59.47</td>
      <td>82.47</td>
      <td>54.83</td>
      <td>44.65</td>
      <td>75.06</td>
      <td>3.56</td>
      <td>42.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>172e30e56e939f73d7d00a165c2d49cbd284481f</td>
      <td>jondurbin/airoboros-l2-13b-2.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/totally-not-an-llm/EverythingLM-13b-V3-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_totally-not-an-llm__EverythingLM-13b-V3-peft</td>
      <td>51.77</td>
      <td>58.36</td>
      <td>81.03</td>
      <td>54.70</td>
      <td>52.98</td>
      <td>72.85</td>
      <td>5.53</td>
      <td>36.94</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>7a2eed5038addcf4fa3b8dd358b45eb96134e749</td>
      <td>totally-not-an-llm/EverythingLM-13b-V3-peft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/UndiMix-v4-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__UndiMix-v4-13B</td>
      <td>51.77</td>
      <td>61.95</td>
      <td>83.88</td>
      <td>56.90</td>
      <td>48.96</td>
      <td>76.16</td>
      <td>13.72</td>
      <td>20.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>6dd97c74cfe1d22432d5c993814e230f333ba401</td>
      <td>Undi95/UndiMix-v4-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CalderaAI/13B-BlueMethod</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CalderaAI__13B-BlueMethod</td>
      <td>51.76</td>
      <td>59.64</td>
      <td>82.07</td>
      <td>50.34</td>
      <td>47.74</td>
      <td>77.11</td>
      <td>7.81</td>
      <td>37.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>315aa0924dd42840b8cced581c9db1240f9bae1d</td>
      <td>CalderaAI/13B-BlueMethod</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/pe-nlp/llama-2-13b-platypus-vicuna-wizard</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pe-nlp__llama-2-13b-platypus-vicuna-wizard</td>
      <td>51.76</td>
      <td>61.26</td>
      <td>82.31</td>
      <td>55.21</td>
      <td>41.91</td>
      <td>75.77</td>
      <td>0.91</td>
      <td>44.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>71aa919fc15fa9d9def9185791b15a3f76e7bd8d</td>
      <td>pe-nlp/llama-2-13b-platypus-vicuna-wizard</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bavest/fin-llama-33b-merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bavest__fin-llama-33b-merged</td>
      <td>51.76</td>
      <td>65.02</td>
      <td>86.20</td>
      <td>58.73</td>
      <td>49.75</td>
      <td>80.03</td>
      <td>16.22</td>
      <td>6.36</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl</td>
      <td>32.32</td>
      <td>11.0</td>
      <td>True</td>
      <td>17114520801da7b9599fe7a9fdf238915713a59b</td>
      <td>bavest/fin-llama-33b-merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Airoboros-L2-13B-2.1-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Airoboros-L2-13B-2.1-GPTQ</td>
      <td>51.71</td>
      <td>58.96</td>
      <td>81.72</td>
      <td>53.16</td>
      <td>44.68</td>
      <td>74.35</td>
      <td>5.99</td>
      <td>43.14</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>16.23</td>
      <td>10.0</td>
      <td>True</td>
      <td>d90d96e40b9359cb5c35e6b6c8f0eb24896e827b</td>
      <td>TheBloke/Airoboros-L2-13B-2.1-GPTQ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Norquinal/Mistral-7B-claude-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Norquinal__Mistral-7B-claude-instruct</td>
      <td>51.71</td>
      <td>63.23</td>
      <td>84.99</td>
      <td>63.84</td>
      <td>47.47</td>
      <td>78.14</td>
      <td>17.97</td>
      <td>6.35</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>faff0de73681ad1f0500169ae18d7a5ff424eb7f</td>
      <td>Norquinal/Mistral-7B-claude-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r4-q_k_v_o</td>
      <td>51.69</td>
      <td>58.36</td>
      <td>81.10</td>
      <td>54.53</td>
      <td>37.02</td>
      <td>76.64</td>
      <td>12.28</td>
      <td>41.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>5cbcd9c0a6b9a19f0d099e653cde18e11bf95303</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/ypotryll-22b-epoch2-qlora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__ypotryll-22b-epoch2-qlora</td>
      <td>51.68</td>
      <td>59.22</td>
      <td>80.66</td>
      <td>54.52</td>
      <td>40.42</td>
      <td>76.32</td>
      <td>5.38</td>
      <td>45.24</td>
      <td>?</td>
      <td>4bit</td>
      <td>?</td>
      <td>22.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>26fdd8fa420d72ed835c7d17086f0441db0985d4</td>
      <td>chargoddard/ypotryll-22b-epoch2-qlora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Nous-Hermes-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Nous-Hermes-13b</td>
      <td>51.68</td>
      <td>56.57</td>
      <td>82.11</td>
      <td>50.44</td>
      <td>51.50</td>
      <td>75.30</td>
      <td>8.34</td>
      <td>37.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl</td>
      <td>12.85</td>
      <td>388.0</td>
      <td>True</td>
      <td>24e8c03148ffd1f3e469744dfc24ad2ad82848f8</td>
      <td>NousResearch/Nous-Hermes-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/JosephusCheung/Pwen-7B-Chat-20_30</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__Pwen-7B-Chat-20_30</td>
      <td>51.68</td>
      <td>51.45</td>
      <td>73.99</td>
      <td>62.08</td>
      <td>47.01</td>
      <td>68.43</td>
      <td>20.62</td>
      <td>38.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>e6c38a7d2f4ba7b867fff421c08c02ba1908224e</td>
      <td>JosephusCheung/Pwen-7B-Chat-20_30</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-llama2-13b</td>
      <td>51.67</td>
      <td>62.03</td>
      <td>81.85</td>
      <td>58.52</td>
      <td>55.70</td>
      <td>76.56</td>
      <td>13.95</td>
      <td>13.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>c6362c4fc0dc03420e3c08454b2e7689e4e32d3a</td>
      <td>uukuguy/speechless-llama2-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PeanutJar/Mistral-v0.1-PeanutButter-v0.0.0-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PeanutJar__Mistral-v0.1-PeanutButter-v0.0.0-7B</td>
      <td>51.62</td>
      <td>62.20</td>
      <td>84.10</td>
      <td>64.14</td>
      <td>46.94</td>
      <td>78.69</td>
      <td>18.50</td>
      <td>6.76</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>9609a969ba6429b84e538d96afac55eb133a9983</td>
      <td>PeanutJar/Mistral-v0.1-PeanutButter-v0.0.0-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/unaidedelf87777/wizard-mistral-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_unaidedelf87777__wizard-mistral-v0.1</td>
      <td>51.58</td>
      <td>61.77</td>
      <td>83.51</td>
      <td>63.99</td>
      <td>47.46</td>
      <td>78.30</td>
      <td>19.03</td>
      <td>7.01</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>b66724f8195e7b76289f8f3f72a98392557c46ad</td>
      <td>unaidedelf87777/wizard-mistral-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardLM-13B-V1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardLM-13B-V1.1</td>
      <td>51.58</td>
      <td>60.24</td>
      <td>81.39</td>
      <td>50.92</td>
      <td>54.56</td>
      <td>75.06</td>
      <td>8.11</td>
      <td>30.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>69.0</td>
      <td>True</td>
      <td>badd80f8a6f46fb15310fedf6d4db54959854897</td>
      <td>WizardLM/WizardLM-13B-V1.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/airoboros-2.1-llama-2-13B-QLoRa</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__airoboros-2.1-llama-2-13B-QLoRa</td>
      <td>51.57</td>
      <td>59.73</td>
      <td>82.91</td>
      <td>54.77</td>
      <td>45.14</td>
      <td>74.03</td>
      <td>2.81</td>
      <td>41.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ebf991c8d34314caab6ccc6b078c681d20bac39a</td>
      <td>yeontaek/airoboros-2.1-llama-2-13B-QLoRa</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-13B-v1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-13B-v1.2</td>
      <td>51.56</td>
      <td>61.26</td>
      <td>82.93</td>
      <td>56.47</td>
      <td>47.27</td>
      <td>76.48</td>
      <td>10.99</td>
      <td>25.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>60d4937ac3c4dcb84c40bbf7265c5cc7f5f3d4f9</td>
      <td>migtissera/Synthia-13B-v1.2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/GenAI-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__GenAI-Nova-13B</td>
      <td>51.53</td>
      <td>62.29</td>
      <td>83.27</td>
      <td>59.47</td>
      <td>51.79</td>
      <td>77.35</td>
      <td>7.73</td>
      <td>18.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ce62a64ca53cd5feb18f523a96dd3be86e6513d</td>
      <td>PulsarAI/GenAI-Nova-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PulsarAI/2x-LoRA-Assemble-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__2x-LoRA-Assemble-13B</td>
      <td>51.52</td>
      <td>63.65</td>
      <td>83.47</td>
      <td>59.82</td>
      <td>55.94</td>
      <td>76.48</td>
      <td>9.25</td>
      <td>12.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1aca45d37eade21eb381aaefc9245b58ec3b7b26</td>
      <td>PulsarAI/2x-LoRA-Assemble-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gradientputri/MegaMix-A1-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gradientputri__MegaMix-A1-13B</td>
      <td>51.51</td>
      <td>61.60</td>
      <td>83.49</td>
      <td>58.26</td>
      <td>47.48</td>
      <td>76.16</td>
      <td>24.11</td>
      <td>9.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>14e0756c210bcf420fbf825e6b8087ee5c716e7f</td>
      <td>gradientputri/MegaMix-A1-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ValiantLabs/ShiningValiantXS</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ValiantLabs__ShiningValiantXS</td>
      <td>51.47</td>
      <td>63.48</td>
      <td>83.56</td>
      <td>59.81</td>
      <td>55.96</td>
      <td>76.48</td>
      <td>8.79</td>
      <td>12.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>8c1f86bd2e646408eed2ed3a2634b38ea4e5c599</td>
      <td>ValiantLabs/ShiningValiantXS</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sequelbox/DaringFortitude</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sequelbox__DaringFortitude</td>
      <td>51.47</td>
      <td>63.48</td>
      <td>83.56</td>
      <td>59.81</td>
      <td>55.96</td>
      <td>76.48</td>
      <td>8.79</td>
      <td>12.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>0c463888cd83b7acebd7b6fb961562e11402e47d</td>
      <td>sequelbox/DaringFortitude</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-13b-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-13b-v1.5</td>
      <td>51.46</td>
      <td>57.08</td>
      <td>81.24</td>
      <td>56.67</td>
      <td>51.51</td>
      <td>74.66</td>
      <td>11.30</td>
      <td>27.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>100.0</td>
      <td>True</td>
      <td>3deb0106f72a3a433f0c6ea0cb978bdf14bcd3a6</td>
      <td>lmsys/vicuna-13b-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Henk717/chronoboros-33B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Henk717__chronoboros-33B</td>
      <td>51.45</td>
      <td>63.91</td>
      <td>85.00</td>
      <td>59.44</td>
      <td>49.83</td>
      <td>80.35</td>
      <td>15.01</td>
      <td>6.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>9.0</td>
      <td>True</td>
      <td>a4deca117c5fa48f2cdc49ed2e2596046201d688</td>
      <td>Henk717/chronoboros-33B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Henk717/airochronos-33B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Henk717__airochronos-33B</td>
      <td>51.43</td>
      <td>64.42</td>
      <td>85.21</td>
      <td>59.79</td>
      <td>50.59</td>
      <td>79.32</td>
      <td>13.72</td>
      <td>6.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.53</td>
      <td>6.0</td>
      <td>True</td>
      <td>06843c6693cc265dabb464c818a3d3713239721a</td>
      <td>Henk717/airochronos-33B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/posicube/Llama2-chat-AYT-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_posicube__Llama2-chat-AYT-13B</td>
      <td>51.41</td>
      <td>63.31</td>
      <td>83.53</td>
      <td>59.67</td>
      <td>55.80</td>
      <td>76.09</td>
      <td>8.87</td>
      <td>12.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>True</td>
      <td>dd12dced8076a959c03b8b5c4a4266f234d6639a</td>
      <td>posicube/Llama2-chat-AYT-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Riiid/sheep-duck-llama-2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Riiid__sheep-duck-llama-2-13b</td>
      <td>51.41</td>
      <td>63.14</td>
      <td>84.52</td>
      <td>59.89</td>
      <td>55.48</td>
      <td>76.95</td>
      <td>9.17</td>
      <td>10.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>71edf22c49677d0239caf5f87d8139dd9cc79078</td>
      <td>Riiid/sheep-duck-llama-2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Emerald-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Emerald-13B</td>
      <td>51.39</td>
      <td>62.29</td>
      <td>83.69</td>
      <td>55.70</td>
      <td>50.94</td>
      <td>75.93</td>
      <td>12.81</td>
      <td>18.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7696299463d8ec402a4e1eb001f3a447f1c5552</td>
      <td>Undi95/Emerald-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/oh-yeontaek/llama-2-13B-LoRA-assemble</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_oh-yeontaek__llama-2-13B-LoRA-assemble</td>
      <td>51.36</td>
      <td>63.57</td>
      <td>83.51</td>
      <td>59.82</td>
      <td>55.96</td>
      <td>76.16</td>
      <td>8.42</td>
      <td>12.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>85bb49d333dba4a08b051418663d16853ce30cee</td>
      <td>oh-yeontaek/llama-2-13B-LoRA-assemble</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Henk717/airochronos-33B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Henk717__airochronos-33B</td>
      <td>51.36</td>
      <td>64.25</td>
      <td>85.20</td>
      <td>59.83</td>
      <td>50.56</td>
      <td>79.08</td>
      <td>13.57</td>
      <td>7.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>other</td>
      <td>32.53</td>
      <td>6.0</td>
      <td>True</td>
      <td>06843c6693cc265dabb464c818a3d3713239721a</td>
      <td>Henk717/airochronos-33B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple</td>
      <td>51.33</td>
      <td>59.13</td>
      <td>80.64</td>
      <td>56.12</td>
      <td>51.29</td>
      <td>74.66</td>
      <td>10.54</td>
      <td>26.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>848ef91ab46a72260542283918a971347c6bfa93</td>
      <td>luffycodes/nash-vicuna-13b-v1dot5-ep2-w-rag-w-simple</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-llama2-13b-v11-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-llama2-13b-v11-bf16</td>
      <td>51.32</td>
      <td>52.99</td>
      <td>75.38</td>
      <td>51.36</td>
      <td>47.94</td>
      <td>71.03</td>
      <td>18.88</td>
      <td>41.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.88</td>
      <td>2.0</td>
      <td>True</td>
      <td>4d4e72c553e9d60fdc208663b0a1c0364caa2f30</td>
      <td>OpenBuddy/openbuddy-llama2-13b-v11-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/minotaur-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__minotaur-13b</td>
      <td>51.31</td>
      <td>56.40</td>
      <td>79.13</td>
      <td>49.61</td>
      <td>49.62</td>
      <td>76.56</td>
      <td>12.51</td>
      <td>35.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>9.0</td>
      <td>True</td>
      <td>b5ae4519d4c8f4559a0aa80b6efe2008413ece01</td>
      <td>openaccess-ai-collective/minotaur-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewd-L2-Chat-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewd-L2-Chat-13B</td>
      <td>51.29</td>
      <td>62.03</td>
      <td>84.19</td>
      <td>58.75</td>
      <td>52.84</td>
      <td>77.43</td>
      <td>11.30</td>
      <td>12.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>13.0</td>
      <td>True</td>
      <td>6c66622a99c1bc73498aa6a15a59da825d875310</td>
      <td>Undi95/MLewd-L2-Chat-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MXLewd-L2-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MXLewd-L2-20B</td>
      <td>51.29</td>
      <td>63.23</td>
      <td>85.33</td>
      <td>57.36</td>
      <td>51.65</td>
      <td>76.09</td>
      <td>10.92</td>
      <td>14.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>7.0</td>
      <td>True</td>
      <td>ac279478abd9ddb8d1f5adcc548be0287b963adf</td>
      <td>Undi95/MXLewd-L2-20B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/quantumaikr/QuantumLM-70B-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__QuantumLM-70B-hf</td>
      <td>51.29</td>
      <td>59.47</td>
      <td>83.02</td>
      <td>62.25</td>
      <td>53.39</td>
      <td>78.77</td>
      <td>14.78</td>
      <td>7.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>68.98</td>
      <td>2.0</td>
      <td>True</td>
      <td>e13dd23ae5e611e959b6c8d5bc47bf4fd37cd9d7</td>
      <td>quantumaikr/QuantumLM-70B-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/samantha-mistral-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__samantha-mistral-7b</td>
      <td>51.28</td>
      <td>63.40</td>
      <td>84.10</td>
      <td>61.36</td>
      <td>46.08</td>
      <td>76.80</td>
      <td>16.00</td>
      <td>11.22</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>17.0</td>
      <td>True</td>
      <td>7f9e40543fdff8c3e58eca0390c8a631829c1206</td>
      <td>ehartford/samantha-mistral-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down</td>
      <td>51.27</td>
      <td>55.03</td>
      <td>81.97</td>
      <td>56.64</td>
      <td>38.07</td>
      <td>77.19</td>
      <td>12.21</td>
      <td>37.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>555486843f613276b6edb480f6d37b9203daa226</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down</td>
      <td>51.25</td>
      <td>56.31</td>
      <td>81.43</td>
      <td>55.30</td>
      <td>39.11</td>
      <td>76.80</td>
      <td>10.46</td>
      <td>39.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>0d8d502e4e5ef89592dd0d3bc7223eaf7f77f78b</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-30b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-30b-instruct</td>
      <td>51.24</td>
      <td>58.45</td>
      <td>84.31</td>
      <td>49.15</td>
      <td>38.05</td>
      <td>75.14</td>
      <td>15.31</td>
      <td>38.28</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-3.0</td>
      <td>29.96</td>
      <td>93.0</td>
      <td>True</td>
      <td>2abf1163dd8c9b11f07d805c06e6ec90a1f2037e</td>
      <td>mosaicml/mpt-30b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down</td>
      <td>51.23</td>
      <td>57.94</td>
      <td>81.19</td>
      <td>53.43</td>
      <td>40.48</td>
      <td>76.72</td>
      <td>10.84</td>
      <td>37.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>15f1b122d60631091419cb8e668a28737b92a0e0</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PeanutJar/Mistral-v0.1-PeanutButter-v0.0.2-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PeanutJar__Mistral-v0.1-PeanutButter-v0.0.2-7B</td>
      <td>51.22</td>
      <td>61.77</td>
      <td>84.11</td>
      <td>64.38</td>
      <td>45.92</td>
      <td>78.37</td>
      <td>17.44</td>
      <td>6.53</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f4d471d7a9447d0969a58d5b3146d50cfa3005b3</td>
      <td>PeanutJar/Mistral-v0.1-PeanutButter-v0.0.2-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Amethyst-13B-Mistral</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Amethyst-13B-Mistral</td>
      <td>51.20</td>
      <td>62.63</td>
      <td>83.17</td>
      <td>55.91</td>
      <td>52.43</td>
      <td>74.74</td>
      <td>10.84</td>
      <td>18.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>4328809e568f01e3f0a05764e3bb58e901310415</td>
      <td>Undi95/Amethyst-13B-Mistral</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Amethyst-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Amethyst-13B</td>
      <td>51.20</td>
      <td>62.63</td>
      <td>83.17</td>
      <td>55.91</td>
      <td>52.43</td>
      <td>74.74</td>
      <td>10.84</td>
      <td>18.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>d4a85b1006f0b9439e64f0e7400533a7b867c24d</td>
      <td>Undi95/Amethyst-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o</td>
      <td>51.19</td>
      <td>54.78</td>
      <td>81.40</td>
      <td>54.73</td>
      <td>41.02</td>
      <td>76.64</td>
      <td>10.54</td>
      <td>39.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>8702b433008a62e9f8bf15e70ba15fa7100e991c</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-q_k_v_o</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/dromedary-65b-lora-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__dromedary-65b-lora-HF</td>
      <td>51.19</td>
      <td>61.60</td>
      <td>82.53</td>
      <td>63.08</td>
      <td>38.82</td>
      <td>78.93</td>
      <td>27.45</td>
      <td>5.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.02</td>
      <td>19.0</td>
      <td>True</td>
      <td>3fa4546259d6bbd6b5d637484c325ab19181a73c</td>
      <td>TheBloke/dromedary-65b-lora-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-3.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-3.0</td>
      <td>51.18</td>
      <td>59.81</td>
      <td>83.71</td>
      <td>54.86</td>
      <td>47.79</td>
      <td>76.16</td>
      <td>8.95</td>
      <td>26.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>7.0</td>
      <td>True</td>
      <td>2fcef275782b2c1061cf671d889aea652d13236c</td>
      <td>jondurbin/airoboros-l2-13b-3.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/BrainDerp2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__BrainDerp2</td>
      <td>51.18</td>
      <td>60.92</td>
      <td>81.94</td>
      <td>58.90</td>
      <td>57.19</td>
      <td>75.93</td>
      <td>9.02</td>
      <td>14.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>948ee7af94a8b092807df4becfc0a8c1cd042878</td>
      <td>Sao10K/BrainDerp2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Airboros2.1-Platypus2-13B-QLora-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Airboros2.1-Platypus2-13B-QLora-0.80-epoch</td>
      <td>51.17</td>
      <td>58.96</td>
      <td>82.46</td>
      <td>54.62</td>
      <td>47.71</td>
      <td>75.14</td>
      <td>0.00</td>
      <td>39.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>45bd1e47218ba2e075e03f6407980eb839e67eb3</td>
      <td>TFLai/Airboros2.1-Platypus2-13B-QLora-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/tulu-13B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__tulu-13B-fp16</td>
      <td>51.17</td>
      <td>53.92</td>
      <td>80.66</td>
      <td>53.19</td>
      <td>43.84</td>
      <td>75.61</td>
      <td>14.25</td>
      <td>36.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>532aeb363b0ceee155b3cf9479ef635b797cee7c</td>
      <td>TheBloke/tulu-13B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dfurman/falcon-40b-openassistant-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dfurman__falcon-40b-openassistant-peft</td>
      <td>51.17</td>
      <td>62.63</td>
      <td>85.59</td>
      <td>57.77</td>
      <td>51.02</td>
      <td>81.45</td>
      <td>13.34</td>
      <td>6.36</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>40.00</td>
      <td>38.0</td>
      <td>False</td>
      <td>3d5084b6fbcb9f9f36493d9fd1e3795b0b9860f0</td>
      <td>dfurman/falcon-40b-openassistant-peft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/BrainDerp</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__BrainDerp</td>
      <td>51.16</td>
      <td>60.75</td>
      <td>82.10</td>
      <td>58.81</td>
      <td>56.90</td>
      <td>75.85</td>
      <td>8.26</td>
      <td>15.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba21a7ed5458b3fa2b05ce6aab431acd1f857516</td>
      <td>Sao10K/BrainDerp</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Weyaxi/Luban-Marcoroni-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__Luban-Marcoroni-13B</td>
      <td>51.16</td>
      <td>63.65</td>
      <td>82.92</td>
      <td>58.70</td>
      <td>55.55</td>
      <td>77.03</td>
      <td>10.01</td>
      <td>10.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>bf152c36935acd67a9029c017f0c1ff2d7a92314</td>
      <td>Weyaxi/Luban-Marcoroni-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MM-ReMM-L2-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MM-ReMM-L2-20B</td>
      <td>51.14</td>
      <td>60.84</td>
      <td>85.18</td>
      <td>56.45</td>
      <td>53.33</td>
      <td>75.77</td>
      <td>7.73</td>
      <td>18.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>1.0</td>
      <td>True</td>
      <td>37869800c15fb37d017ea83bb50fec6d6141f6ba</td>
      <td>Undi95/MM-ReMM-L2-20B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/2x-LoRA-Assemble-Platypus2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__2x-LoRA-Assemble-Platypus2-13B</td>
      <td>51.13</td>
      <td>60.58</td>
      <td>82.56</td>
      <td>58.25</td>
      <td>54.77</td>
      <td>74.90</td>
      <td>0.91</td>
      <td>25.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>f147bf8428c174d1dc0332da626d4b039690ceab</td>
      <td>PulsarAI/2x-LoRA-Assemble-Platypus2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Weyaxi/Luban-Marcoroni-13B-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__Luban-Marcoroni-13B-v3</td>
      <td>51.13</td>
      <td>63.74</td>
      <td>82.88</td>
      <td>58.64</td>
      <td>55.56</td>
      <td>76.87</td>
      <td>9.93</td>
      <td>10.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>9b68680ed8351ef8ef6948169e69a888af40002e</td>
      <td>Weyaxi/Luban-Marcoroni-13B-v3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-gpt4-2.0</td>
      <td>51.12</td>
      <td>59.04</td>
      <td>82.82</td>
      <td>54.71</td>
      <td>36.47</td>
      <td>74.19</td>
      <td>7.73</td>
      <td>42.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>14.0</td>
      <td>True</td>
      <td>ec556571acc6783fea4414e4ca72d291c563b6dc</td>
      <td>jondurbin/airoboros-l2-13b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Weyaxi/Luban-Marcoroni-13B-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__Luban-Marcoroni-13B-v2</td>
      <td>51.11</td>
      <td>63.48</td>
      <td>82.89</td>
      <td>58.72</td>
      <td>55.56</td>
      <td>76.95</td>
      <td>9.93</td>
      <td>10.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>d7c704a08218dcc03963bc08e9113e281c056f53</td>
      <td>Weyaxi/Luban-Marcoroni-13B-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Gryphe/MythoMix-L2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Gryphe__MythoMix-L2-13b</td>
      <td>51.10</td>
      <td>61.09</td>
      <td>83.86</td>
      <td>55.42</td>
      <td>52.08</td>
      <td>75.45</td>
      <td>9.93</td>
      <td>19.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>15.0</td>
      <td>True</td>
      <td>eca790fb9394c9c61be27ef709080b3b92783a45</td>
      <td>Gryphe/MythoMix-L2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down</td>
      <td>51.10</td>
      <td>55.89</td>
      <td>81.38</td>
      <td>53.77</td>
      <td>40.25</td>
      <td>76.72</td>
      <td>12.28</td>
      <td>37.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a8b15badead658df6ec5b884b813962b9fd29cfb</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/BrainDerp3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__BrainDerp3</td>
      <td>51.10</td>
      <td>60.92</td>
      <td>82.10</td>
      <td>58.91</td>
      <td>57.18</td>
      <td>75.61</td>
      <td>8.04</td>
      <td>14.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>0b575b9245406cca92942ce2ababb5b868109bed</td>
      <td>Sao10K/BrainDerp3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/30B-Lazarus-instruct-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__30B-Lazarus-instruct-PL-lora_unload</td>
      <td>51.08</td>
      <td>62.80</td>
      <td>84.13</td>
      <td>56.87</td>
      <td>55.49</td>
      <td>79.08</td>
      <td>11.37</td>
      <td>7.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>eeb29b35ceb6dd5c532f1e4e1235f1cdd3f51f23</td>
      <td>Aspik101/30B-Lazarus-instruct-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-dolphin-orca-platypus-34b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-dolphin-orca-platypus-34b</td>
      <td>51.07</td>
      <td>52.47</td>
      <td>74.13</td>
      <td>53.47</td>
      <td>47.14</td>
      <td>73.24</td>
      <td>14.71</td>
      <td>42.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>6.0</td>
      <td>True</td>
      <td>57e18e617b4fd7ab61bd7da8ee9516513ad76842</td>
      <td>uukuguy/speechless-codellama-dolphin-orca-platypus-34b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/speechlessai/speechless-codellama-34b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_speechlessai__speechless-codellama-34b-v1.0</td>
      <td>51.07</td>
      <td>52.47</td>
      <td>74.13</td>
      <td>53.47</td>
      <td>47.14</td>
      <td>73.24</td>
      <td>14.71</td>
      <td>42.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d64d871cd56da3031e19bc267ef8bd0b85b9936</td>
      <td>speechlessai/speechless-codellama-34b-v1.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/samantha-mistral-instruct-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__samantha-mistral-instruct-7b</td>
      <td>51.02</td>
      <td>53.50</td>
      <td>75.14</td>
      <td>51.72</td>
      <td>58.81</td>
      <td>70.40</td>
      <td>10.84</td>
      <td>36.73</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>14.0</td>
      <td>True</td>
      <td>3a33eea0858d411617c472c3c0ae39f17d2b3f5d</td>
      <td>ehartford/samantha-mistral-instruct-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Weyaxi/ChatAYT-Lora-Assamble-Marcoroni</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Weyaxi__ChatAYT-Lora-Assamble-Marcoroni</td>
      <td>51.00</td>
      <td>62.46</td>
      <td>83.05</td>
      <td>58.72</td>
      <td>56.12</td>
      <td>77.35</td>
      <td>8.87</td>
      <td>10.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>51c9b600023cd26c4eb3754b9a89c60dde959ccc</td>
      <td>Weyaxi/ChatAYT-Lora-Assamble-Marcoroni</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-SLERP-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-SLERP-L2-13B</td>
      <td>50.99</td>
      <td>60.92</td>
      <td>83.56</td>
      <td>55.33</td>
      <td>51.97</td>
      <td>75.22</td>
      <td>9.17</td>
      <td>20.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>8.0</td>
      <td>True</td>
      <td>27baccf242bc1dc34fc39661a40bbf867cbea8b5</td>
      <td>Undi95/ReMM-SLERP-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-v1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-13b-v1.2</td>
      <td>50.99</td>
      <td>60.92</td>
      <td>83.56</td>
      <td>55.33</td>
      <td>51.97</td>
      <td>75.22</td>
      <td>9.17</td>
      <td>20.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>cb3562e7aae05a95fe61610b7b8f4957d3529ce7</td>
      <td>The-Face-Of-Goonery/Huginn-13b-v1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ai-business/Luban-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ai-business__Luban-13B</td>
      <td>50.98</td>
      <td>63.05</td>
      <td>82.80</td>
      <td>58.73</td>
      <td>55.53</td>
      <td>76.56</td>
      <td>9.70</td>
      <td>10.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>13.0</td>
      <td>True</td>
      <td>01b0f2046083dd8d9d8f9e626d78d83eaa1d57dd</td>
      <td>ai-business/Luban-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-13b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-13b-chat</td>
      <td>50.97</td>
      <td>58.62</td>
      <td>80.85</td>
      <td>47.76</td>
      <td>48.73</td>
      <td>76.72</td>
      <td>8.34</td>
      <td>35.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>27002e974774c3599e6a4d731dd44e68b9e41f92</td>
      <td>bofenghuang/vigogne-13b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/adonlee/LLaMA_2_13B_SFT_v0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_adonlee__LLaMA_2_13B_SFT_v0</td>
      <td>50.97</td>
      <td>62.03</td>
      <td>83.80</td>
      <td>58.39</td>
      <td>49.92</td>
      <td>77.27</td>
      <td>12.43</td>
      <td>12.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6790d83337578f38d2bcd51038a779eaa8d0fac</td>
      <td>adonlee/LLaMA_2_13B_SFT_v0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Gryphe/MythoMax-L2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Gryphe__MythoMax-L2-13b</td>
      <td>50.97</td>
      <td>60.92</td>
      <td>83.56</td>
      <td>55.33</td>
      <td>51.97</td>
      <td>75.22</td>
      <td>9.02</td>
      <td>20.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>119.0</td>
      <td>True</td>
      <td>faa4ef8c87dbb00d447904ceb048d49b6a463d07</td>
      <td>Gryphe/MythoMax-L2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-34b-v2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-34b-v2.0</td>
      <td>50.96</td>
      <td>54.35</td>
      <td>75.65</td>
      <td>54.67</td>
      <td>45.21</td>
      <td>73.56</td>
      <td>11.60</td>
      <td>41.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>3.0</td>
      <td>True</td>
      <td>cb81174d72dbe06f8db1c406ef97981532de6f09</td>
      <td>uukuguy/speechless-codellama-34b-v2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-1.4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-1.4</td>
      <td>50.96</td>
      <td>64.42</td>
      <td>85.13</td>
      <td>59.53</td>
      <td>50.47</td>
      <td>77.90</td>
      <td>11.75</td>
      <td>7.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>13.0</td>
      <td>True</td>
      <td>04e1e194247a95cc60ba3cd70d026bc94c1f1764</td>
      <td>jondurbin/airoboros-33b-gpt4-1.4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/CodeEngine</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__CodeEngine</td>
      <td>50.96</td>
      <td>58.36</td>
      <td>82.27</td>
      <td>54.18</td>
      <td>45.18</td>
      <td>74.59</td>
      <td>1.52</td>
      <td>40.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f57879831c39f2dcb656cb2c9e9ce5878e92bb44</td>
      <td>Undi95/CodeEngine</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/huginnv1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__huginnv1.2</td>
      <td>50.95</td>
      <td>62.37</td>
      <td>84.28</td>
      <td>57.02</td>
      <td>47.81</td>
      <td>75.22</td>
      <td>9.17</td>
      <td>20.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>aed4ddc951c657993939fa5b87a4088550569a3b</td>
      <td>The-Face-Of-Goonery/huginnv1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-mistral-7b-v13</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-mistral-7b-v13</td>
      <td>50.94</td>
      <td>52.30</td>
      <td>75.09</td>
      <td>56.34</td>
      <td>50.81</td>
      <td>71.74</td>
      <td>14.71</td>
      <td>35.55</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.13</td>
      <td>3.0</td>
      <td>True</td>
      <td>e6c4cc00e1bb2aa2082c2b8fd93c949aa36ce300</td>
      <td>OpenBuddy/openbuddy-mistral-7b-v13</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/IkariDev/Athena-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_IkariDev__Athena-v4</td>
      <td>50.92</td>
      <td>62.54</td>
      <td>84.19</td>
      <td>57.33</td>
      <td>50.87</td>
      <td>76.48</td>
      <td>11.98</td>
      <td>13.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>11.0</td>
      <td>True</td>
      <td>dde640538a44a08f6f456a2b7634e31a5d7a1245</td>
      <td>IkariDev/Athena-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/Vicuzard-30B-Uncensored-instruct-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__Vicuzard-30B-Uncensored-instruct-PL-lora_unload</td>
      <td>50.86</td>
      <td>62.46</td>
      <td>83.66</td>
      <td>57.82</td>
      <td>50.94</td>
      <td>78.37</td>
      <td>15.31</td>
      <td>7.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>652f03ac67b4293198d98b618e64285fb32a28e9</td>
      <td>Aspik101/Vicuzard-30B-Uncensored-instruct-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HyperbeeAI/Tulpar-7b-v0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HyperbeeAI__Tulpar-7b-v0</td>
      <td>50.84</td>
      <td>56.31</td>
      <td>79.01</td>
      <td>52.55</td>
      <td>51.68</td>
      <td>73.88</td>
      <td>2.73</td>
      <td>39.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>22.0</td>
      <td>True</td>
      <td>d7c2bc52a3ae13571357f51273ae948caf84400e</td>
      <td>HyperbeeAI/Tulpar-7b-v0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-llama2-hermes-orca-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-llama2-hermes-orca-platypus-13b</td>
      <td>50.84</td>
      <td>60.92</td>
      <td>83.50</td>
      <td>59.39</td>
      <td>54.29</td>
      <td>75.22</td>
      <td>9.70</td>
      <td>12.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>f227ad33b16726b099e35e5dc47f4db1f22665a7</td>
      <td>uukuguy/speechless-llama2-hermes-orca-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/caisarl76/Mistral-7B-OpenOrca-Guanaco-accu16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_caisarl76__Mistral-7B-OpenOrca-Guanaco-accu16</td>
      <td>50.84</td>
      <td>59.73</td>
      <td>83.08</td>
      <td>61.29</td>
      <td>50.81</td>
      <td>76.56</td>
      <td>16.00</td>
      <td>8.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>e83b8c1887c45473961a4ff36ae202ada1ca3d42</td>
      <td>caisarl76/Mistral-7B-OpenOrca-Guanaco-accu16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down</td>
      <td>50.82</td>
      <td>59.22</td>
      <td>81.52</td>
      <td>54.94</td>
      <td>42.83</td>
      <td>76.87</td>
      <td>11.60</td>
      <td>28.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a759c4fae8dc5fcd264bf58b89b9fd13d06784ae</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-llama2-luban-orca-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-llama2-luban-orca-platypus-13b</td>
      <td>50.81</td>
      <td>62.54</td>
      <td>82.76</td>
      <td>59.23</td>
      <td>54.66</td>
      <td>77.11</td>
      <td>8.19</td>
      <td>11.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>908cfb670611875b52045c4bab81cff53f0279a7</td>
      <td>uukuguy/speechless-llama2-luban-orca-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down</td>
      <td>50.81</td>
      <td>57.76</td>
      <td>80.78</td>
      <td>54.32</td>
      <td>40.80</td>
      <td>76.72</td>
      <td>7.96</td>
      <td>37.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ebe1b75fa315a9b55f686368070a0bcd0245ee39</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-Inverted</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewd-ReMM-L2-Chat-20B-Inverted</td>
      <td>50.81</td>
      <td>61.69</td>
      <td>85.32</td>
      <td>58.00</td>
      <td>53.77</td>
      <td>75.61</td>
      <td>9.10</td>
      <td>12.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>1.0</td>
      <td>True</td>
      <td>b5b501b4d23ec7ab24b827f79e48b2c67e548ddb</td>
      <td>Undi95/MLewd-ReMM-L2-Chat-20B-Inverted</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/pe-nlp/llama-2-13b-vicuna-wizard</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pe-nlp__llama-2-13b-vicuna-wizard</td>
      <td>50.79</td>
      <td>57.76</td>
      <td>82.16</td>
      <td>54.68</td>
      <td>41.11</td>
      <td>74.98</td>
      <td>0.91</td>
      <td>43.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b51bf8c4e132308751cc8b9d9c1131539f79f07f</td>
      <td>pe-nlp/llama-2-13b-vicuna-wizard</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/camel-ai/CAMEL-33B-Combined-Data</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_camel-ai__CAMEL-33B-Combined-Data</td>
      <td>50.79</td>
      <td>62.97</td>
      <td>83.83</td>
      <td>58.98</td>
      <td>50.21</td>
      <td>78.30</td>
      <td>14.10</td>
      <td>7.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>3.0</td>
      <td>True</td>
      <td>62c74e7531625c1383bbbdc7c8346a996e9d1e21</td>
      <td>camel-ai/CAMEL-33B-Combined-Data</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/caisarl76/Mistral-7B-guanaco1k-ep2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_caisarl76__Mistral-7B-guanaco1k-ep2</td>
      <td>50.76</td>
      <td>60.07</td>
      <td>82.76</td>
      <td>61.50</td>
      <td>54.40</td>
      <td>78.06</td>
      <td>11.98</td>
      <td>6.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.11</td>
      <td>2.0</td>
      <td>True</td>
      <td>9c9f31f213b69da7797c2c0630c17cf8f785fc13</td>
      <td>caisarl76/Mistral-7B-guanaco1k-ep2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/caisarl76/mistral-guanaco1k-ep2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_caisarl76__mistral-guanaco1k-ep2</td>
      <td>50.76</td>
      <td>60.07</td>
      <td>82.76</td>
      <td>61.50</td>
      <td>54.40</td>
      <td>78.06</td>
      <td>11.98</td>
      <td>6.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.11</td>
      <td>2.0</td>
      <td>True</td>
      <td>9c9f31f213b69da7797c2c0630c17cf8f785fc13</td>
      <td>caisarl76/mistral-guanaco1k-ep2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__OpenOrcaxOpenChat-Preview2-13B</td>
      <td>50.76</td>
      <td>62.37</td>
      <td>82.96</td>
      <td>58.68</td>
      <td>51.23</td>
      <td>77.19</td>
      <td>14.10</td>
      <td>8.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>94.0</td>
      <td>True</td>
      <td>26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230</td>
      <td>Open-Orca/OpenOrcaxOpenChat-Preview2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/mistral-7b-platypus1k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__mistral-7b-platypus1k</td>
      <td>50.74</td>
      <td>61.60</td>
      <td>82.93</td>
      <td>63.16</td>
      <td>46.96</td>
      <td>78.14</td>
      <td>16.38</td>
      <td>5.99</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>c34c4a249ecf0cc391beba142a1f9cb23154fcd1</td>
      <td>lgaalves/mistral-7b-platypus1k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/llama2-13b-fintune2-4E</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__llama2-13b-fintune2-4E</td>
      <td>50.73</td>
      <td>55.89</td>
      <td>80.95</td>
      <td>53.73</td>
      <td>42.72</td>
      <td>73.09</td>
      <td>10.92</td>
      <td>37.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>645ede9d6ec60d8fa051bc7ad32ab5f7bfdc066d</td>
      <td>wei123602/llama2-13b-fintune2-4E</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/Chat-AYB-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__Chat-AYB-Nova-13B</td>
      <td>50.73</td>
      <td>62.97</td>
      <td>84.28</td>
      <td>58.58</td>
      <td>51.28</td>
      <td>77.58</td>
      <td>12.36</td>
      <td>8.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>942af4d59533af09cf9ba13d1e369b8e871a0a4b</td>
      <td>PulsarAI/Chat-AYB-Nova-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/Mistral-7B-OpenOrca-lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__Mistral-7B-OpenOrca-lora</td>
      <td>50.72</td>
      <td>61.95</td>
      <td>83.62</td>
      <td>64.16</td>
      <td>42.74</td>
      <td>79.08</td>
      <td>17.29</td>
      <td>6.19</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>605dc043063cb9589c06883d839122920ed1eca5</td>
      <td>uukuguy/Mistral-7B-OpenOrca-lora</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/mncai/Mistral-7B-openplatypus-1k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mncai__Mistral-7B-openplatypus-1k</td>
      <td>50.71</td>
      <td>60.15</td>
      <td>84.25</td>
      <td>59.84</td>
      <td>49.86</td>
      <td>76.87</td>
      <td>17.44</td>
      <td>6.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>7.11</td>
      <td>0.0</td>
      <td>True</td>
      <td>dad401175da3782475a122008720ddc3338e2632</td>
      <td>mncai/Mistral-7B-openplatypus-1k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Unholy-v1-12L-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Unholy-v1-12L-13B</td>
      <td>50.65</td>
      <td>63.57</td>
      <td>83.75</td>
      <td>58.08</td>
      <td>51.09</td>
      <td>77.27</td>
      <td>11.07</td>
      <td>9.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>25.0</td>
      <td>True</td>
      <td>ee25c078f08b0812d82597afa3f5e877c19a5c83</td>
      <td>Undi95/Unholy-v1-12L-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aeala/VicUnlocked-alpaca-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aeala__VicUnlocked-alpaca-30b</td>
      <td>50.65</td>
      <td>61.86</td>
      <td>83.79</td>
      <td>57.64</td>
      <td>51.03</td>
      <td>78.22</td>
      <td>14.63</td>
      <td>7.36</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>7.0</td>
      <td>True</td>
      <td>c63d117d1ec5794766dd6dc5e1469769df8aba1d</td>
      <td>Aeala/VicUnlocked-alpaca-30b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/llama-2-16b-nastychat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__llama-2-16b-nastychat</td>
      <td>50.62</td>
      <td>57.42</td>
      <td>80.59</td>
      <td>55.99</td>
      <td>53.45</td>
      <td>74.66</td>
      <td>8.11</td>
      <td>24.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>16.19</td>
      <td>2.0</td>
      <td>True</td>
      <td>6fb7f82d486b3eee53d750f83cc7eae434349809</td>
      <td>chargoddard/llama-2-16b-nastychat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-v2-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-v2-L2-13B</td>
      <td>50.57</td>
      <td>61.95</td>
      <td>84.00</td>
      <td>56.14</td>
      <td>50.81</td>
      <td>75.85</td>
      <td>13.19</td>
      <td>12.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>bc42c77f88482c37c72c85c66135e99972bbca1b</td>
      <td>Undi95/ReMM-v2-L2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/storytime-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__storytime-13b</td>
      <td>50.55</td>
      <td>62.03</td>
      <td>83.96</td>
      <td>57.48</td>
      <td>52.50</td>
      <td>75.53</td>
      <td>8.34</td>
      <td>14.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>True</td>
      <td>233568319a636b6a7b02a4def2c51d08a3e0fbfc</td>
      <td>chargoddard/storytime-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aeala/Enterredaas-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aeala__Enterredaas-33b</td>
      <td>50.52</td>
      <td>60.92</td>
      <td>84.18</td>
      <td>58.30</td>
      <td>49.02</td>
      <td>78.77</td>
      <td>16.22</td>
      <td>6.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>d72dc1f05eaf1beb6373fd53fd22eb90f293a5c4</td>
      <td>Aeala/Enterredaas-33b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__OpenOrca-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>50.49</td>
      <td>62.37</td>
      <td>82.99</td>
      <td>59.38</td>
      <td>52.20</td>
      <td>75.77</td>
      <td>11.14</td>
      <td>9.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>39ae03b77b4f1d453b02468ce6bb4ddeb6526b77</td>
      <td>TFLai/OpenOrca-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zararp-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zararp-l2-7b</td>
      <td>50.49</td>
      <td>56.31</td>
      <td>79.19</td>
      <td>51.36</td>
      <td>51.26</td>
      <td>74.51</td>
      <td>1.74</td>
      <td>39.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>6032c5106970f98d59925959fbd330ae4b1d1a7e</td>
      <td>zarakiquemparte/zararp-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-dolphin_20w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-dolphin_20w</td>
      <td>50.48</td>
      <td>59.56</td>
      <td>82.55</td>
      <td>55.89</td>
      <td>42.67</td>
      <td>77.27</td>
      <td>12.43</td>
      <td>23.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c75073d7545a4d222f40dc519021c55a81850d75</td>
      <td>CHIH-HUNG/llama-2-13b-dolphin_20w</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/meta-llama/Llama-2-13b-chat-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-13b-chat-hf</td>
      <td>50.48</td>
      <td>59.04</td>
      <td>81.94</td>
      <td>54.64</td>
      <td>44.12</td>
      <td>74.51</td>
      <td>15.24</td>
      <td>23.87</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>617.0</td>
      <td>False</td>
      <td>f848cf15ab9a51ae5735ab28120a9a0773eeb541</td>
      <td>meta-llama/Llama-2-13b-chat-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/deepse/CodeUp-Llama-2-13b-chat-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_deepse__CodeUp-Llama-2-13b-chat-hf</td>
      <td>50.48</td>
      <td>59.04</td>
      <td>81.93</td>
      <td>54.63</td>
      <td>44.12</td>
      <td>74.51</td>
      <td>15.24</td>
      <td>23.87</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail++</td>
      <td>12.85</td>
      <td>25.0</td>
      <td>True</td>
      <td>d4af0b233a5b6a214e96582e103396e99dcf5f95</td>
      <td>deepse/CodeUp-Llama-2-13b-chat-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NewstaR/Morningstar-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NewstaR__Morningstar-13b-hf</td>
      <td>50.48</td>
      <td>59.04</td>
      <td>81.93</td>
      <td>54.63</td>
      <td>44.12</td>
      <td>74.51</td>
      <td>15.24</td>
      <td>23.87</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2605b5b3b0ecba906ac26d39aab40f33c2ec81c9</td>
      <td>NewstaR/Morningstar-13b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__OpenOrca-Platypus2-13B</td>
      <td>50.47</td>
      <td>62.80</td>
      <td>83.15</td>
      <td>59.39</td>
      <td>53.08</td>
      <td>76.24</td>
      <td>9.02</td>
      <td>9.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>204.0</td>
      <td>True</td>
      <td>e7a40134f7eb687c6ab66d445dc7251257f8d391</td>
      <td>Open-Orca/OpenOrca-Platypus2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-1.3</td>
      <td>50.47</td>
      <td>63.91</td>
      <td>85.04</td>
      <td>58.53</td>
      <td>45.36</td>
      <td>78.69</td>
      <td>13.04</td>
      <td>8.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>2.0</td>
      <td>True</td>
      <td>f94e5249d2b998933466d42e08fa9551e3238205</td>
      <td>jondurbin/airoboros-33b-gpt4-1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aeala/GPT4-x-Alpasta-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aeala__GPT4-x-Alpasta-13b</td>
      <td>50.46</td>
      <td>58.53</td>
      <td>79.92</td>
      <td>46.03</td>
      <td>53.06</td>
      <td>73.95</td>
      <td>8.79</td>
      <td>32.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>50af05b015446110a2dc52a1b4b341142c98e62b</td>
      <td>Aeala/GPT4-x-Alpasta-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/llama-2-13b-QLoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__llama-2-13b-QLoRA</td>
      <td>50.46</td>
      <td>58.02</td>
      <td>82.33</td>
      <td>55.80</td>
      <td>46.23</td>
      <td>77.58</td>
      <td>3.26</td>
      <td>29.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d1a41d83c6bcc14378ee4859d65ef77a261d39d7</td>
      <td>yeontaek/llama-2-13b-QLoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-v2.2-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-v2.2-L2-13B</td>
      <td>50.45</td>
      <td>61.26</td>
      <td>84.16</td>
      <td>56.22</td>
      <td>51.35</td>
      <td>75.61</td>
      <td>14.03</td>
      <td>10.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>d55031fbcd41d749bc0c0ffbcd85636718d373b6</td>
      <td>Undi95/ReMM-v2.2-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down</td>
      <td>50.45</td>
      <td>55.80</td>
      <td>81.74</td>
      <td>55.09</td>
      <td>39.12</td>
      <td>76.32</td>
      <td>12.81</td>
      <td>32.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>aefc3a122cb054b070a212d1127600775aded4be</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r4-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-33b-gpt4-1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-33b-gpt4-1.3</td>
      <td>50.45</td>
      <td>63.82</td>
      <td>85.09</td>
      <td>58.94</td>
      <td>45.33</td>
      <td>79.01</td>
      <td>12.74</td>
      <td>8.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>32.32</td>
      <td>2.0</td>
      <td>True</td>
      <td>f94e5249d2b998933466d42e08fa9551e3238205</td>
      <td>jondurbin/airoboros-33b-gpt4-1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down</td>
      <td>50.43</td>
      <td>55.97</td>
      <td>81.53</td>
      <td>54.42</td>
      <td>40.72</td>
      <td>75.06</td>
      <td>9.55</td>
      <td>35.77</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>905fc0b26dcb9e1fc5be99e73596e0884f9b71df</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/Llama-2-13b-FINETUNE4_TEST3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__Llama-2-13b-FINETUNE4_TEST3</td>
      <td>50.41</td>
      <td>59.04</td>
      <td>81.65</td>
      <td>56.37</td>
      <td>39.98</td>
      <td>75.45</td>
      <td>11.22</td>
      <td>29.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e81b5d4550224711929fdea4effdd990cc0c7404</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_TEST3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/ReMM-v2.1-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__ReMM-v2.1-L2-13B</td>
      <td>50.41</td>
      <td>61.43</td>
      <td>83.92</td>
      <td>55.95</td>
      <td>50.30</td>
      <td>75.93</td>
      <td>12.74</td>
      <td>12.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>e6b5ac97f74355cb281a621261debe5720fb4da2</td>
      <td>Undi95/ReMM-v2.1-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-MysteryModel-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-MysteryModel-13b</td>
      <td>50.39</td>
      <td>57.00</td>
      <td>80.35</td>
      <td>52.06</td>
      <td>45.00</td>
      <td>74.82</td>
      <td>0.00</td>
      <td>43.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>c38a9df20162455b53eb35d38a9b67fb824559e8</td>
      <td>PocketDoc/Dans-MysteryModel-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/oh-yeontaek/llama-2-7B-LoRA-assemble</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_oh-yeontaek__llama-2-7B-LoRA-assemble</td>
      <td>50.38</td>
      <td>57.34</td>
      <td>78.81</td>
      <td>50.75</td>
      <td>53.18</td>
      <td>73.48</td>
      <td>0.00</td>
      <td>39.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>4.0</td>
      <td>True</td>
      <td>72e866a96a2e9afc6527c8d757c69088c3a069c8</td>
      <td>oh-yeontaek/llama-2-7B-LoRA-assemble</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Alpacino30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Alpacino30b</td>
      <td>50.38</td>
      <td>62.71</td>
      <td>85.04</td>
      <td>58.48</td>
      <td>44.23</td>
      <td>79.79</td>
      <td>15.77</td>
      <td>6.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>67.0</td>
      <td>True</td>
      <td>300bc5f3dc129a3d17adf059394e381eff7fbd55</td>
      <td>digitous/Alpacino30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-OpenOrca_20w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-OpenOrca_20w</td>
      <td>50.38</td>
      <td>59.90</td>
      <td>82.51</td>
      <td>56.30</td>
      <td>43.14</td>
      <td>77.19</td>
      <td>12.66</td>
      <td>20.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f01882672e89b164f76093cf3bd26cfc6ecf72ed</td>
      <td>CHIH-HUNG/llama-2-13b-OpenOrca_20w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/U-Amethyst-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__U-Amethyst-20B</td>
      <td>50.38</td>
      <td>62.20</td>
      <td>83.11</td>
      <td>55.88</td>
      <td>53.20</td>
      <td>74.19</td>
      <td>5.31</td>
      <td>18.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>19.99</td>
      <td>7.0</td>
      <td>True</td>
      <td>c0cbe0b3c88041bb6beef27dbe85146af8dddec9</td>
      <td>Undi95/U-Amethyst-20B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/2x-LoRA-Assemble-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__2x-LoRA-Assemble-Nova-13B</td>
      <td>50.34</td>
      <td>62.63</td>
      <td>83.24</td>
      <td>58.64</td>
      <td>51.88</td>
      <td>76.95</td>
      <td>10.24</td>
      <td>8.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2a344b91b28ce4d0bd48b9b5a6cc87b71123eab5</td>
      <td>PulsarAI/2x-LoRA-Assemble-Nova-13B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/mistralai/Mistral-7B-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mistralai__Mistral-7B-v0.1</td>
      <td>50.32</td>
      <td>59.98</td>
      <td>83.31</td>
      <td>64.16</td>
      <td>42.15</td>
      <td>78.37</td>
      <td>18.12</td>
      <td>6.14</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>1173.0</td>
      <td>True</td>
      <td>e836d8f71b5812f9fee65618453dc537c66bd82a</td>
      <td>mistralai/Mistral-7B-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/llama2-13b-FINETUNE3_TEST2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__llama2-13b-FINETUNE3_TEST2</td>
      <td>50.32</td>
      <td>54.69</td>
      <td>81.48</td>
      <td>56.80</td>
      <td>39.93</td>
      <td>76.24</td>
      <td>12.59</td>
      <td>30.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>9e6431061bd13852a7435f5fe7a6eb0bbd148e14</td>
      <td>wei123602/llama2-13b-FINETUNE3_TEST2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zararp-1.1-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zararp-1.1-l2-7b</td>
      <td>50.31</td>
      <td>56.48</td>
      <td>78.85</td>
      <td>51.49</td>
      <td>51.99</td>
      <td>73.40</td>
      <td>1.14</td>
      <td>38.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>31fa6527a3285d5fd320219d7c2dadde07b83718</td>
      <td>zarakiquemparte/zararp-1.1-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/totally-not-an-llm/PuddleJumper-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_totally-not-an-llm__PuddleJumper-13b</td>
      <td>50.23</td>
      <td>58.70</td>
      <td>81.18</td>
      <td>58.25</td>
      <td>56.44</td>
      <td>72.77</td>
      <td>3.34</td>
      <td>20.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>f3a8a475ff0c6ae37ac8ae0690980be11cac731a</td>
      <td>totally-not-an-llm/PuddleJumper-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-mistral-7b-v13.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-mistral-7b-v13.1</td>
      <td>50.23</td>
      <td>52.56</td>
      <td>75.73</td>
      <td>56.68</td>
      <td>50.44</td>
      <td>71.59</td>
      <td>8.72</td>
      <td>35.89</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.13</td>
      <td>1.0</td>
      <td>True</td>
      <td>b64386bde3d7850a01df763f5c777c74888d34fc</td>
      <td>OpenBuddy/openbuddy-mistral-7b-v13.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__OpenOrca-Platypus2-13B-GPTQ</td>
      <td>50.22</td>
      <td>62.54</td>
      <td>82.67</td>
      <td>58.56</td>
      <td>51.93</td>
      <td>76.80</td>
      <td>9.40</td>
      <td>9.61</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>cc-by-nc-4.0</td>
      <td>16.24</td>
      <td>49.0</td>
      <td>True</td>
      <td>0fa9a56066656fbc94e3ec088bc900fd1d4d38e8</td>
      <td>TheBloke/OpenOrca-Platypus2-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/Uncensored-Frank-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__Uncensored-Frank-13B</td>
      <td>50.21</td>
      <td>61.60</td>
      <td>82.62</td>
      <td>54.55</td>
      <td>48.34</td>
      <td>74.74</td>
      <td>11.98</td>
      <td>17.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>73a27445e5e5a72857626e551c70542ec607f60c</td>
      <td>ajibawa-2023/Uncensored-Frank-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/Llama-2-13b-FINETUNE4_TEST</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__Llama-2-13b-FINETUNE4_TEST</td>
      <td>50.20</td>
      <td>54.78</td>
      <td>81.52</td>
      <td>56.03</td>
      <td>39.14</td>
      <td>77.03</td>
      <td>13.19</td>
      <td>29.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ed198a814192b06e60715112d2a4b6bfd630806</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_TEST</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zarafusionex-1.1-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zarafusionex-1.1-l2-7b</td>
      <td>50.18</td>
      <td>56.14</td>
      <td>79.34</td>
      <td>52.10</td>
      <td>50.66</td>
      <td>74.43</td>
      <td>7.81</td>
      <td>30.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>7.0</td>
      <td>True</td>
      <td>3268ff5291934a14f3f5e7013bbb408f33adb542</td>
      <td>zarakiquemparte/zarafusionex-1.1-l2-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Azure99/blossom-v2-llama2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Azure99__blossom-v2-llama2-7b</td>
      <td>50.13</td>
      <td>54.10</td>
      <td>78.57</td>
      <td>51.66</td>
      <td>46.84</td>
      <td>74.35</td>
      <td>4.78</td>
      <td>40.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>8c71cdb481ce6bbda3b2042e5526a232ab23825c</td>
      <td>Azure99/blossom-v2-llama2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Mythical-Destroyer-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Mythical-Destroyer-L2-13B</td>
      <td>50.13</td>
      <td>58.70</td>
      <td>82.00</td>
      <td>57.66</td>
      <td>56.35</td>
      <td>74.66</td>
      <td>8.95</td>
      <td>12.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>7c87376b201b1c30c4e12c0b7bc2f28f017ce7bc</td>
      <td>Sao10K/Mythical-Destroyer-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dsvv-cair/alpaca-cleaned-llama-30b-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dsvv-cair__alpaca-cleaned-llama-30b-bf16</td>
      <td>50.12</td>
      <td>61.77</td>
      <td>85.06</td>
      <td>57.52</td>
      <td>51.49</td>
      <td>77.35</td>
      <td>7.73</td>
      <td>9.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>3.0</td>
      <td>True</td>
      <td>2424b6346e9e8fd749b9a6734f5d7125b5926daf</td>
      <td>dsvv-cair/alpaca-cleaned-llama-30b-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/IkariDev/Athena-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_IkariDev__Athena-v3</td>
      <td>50.10</td>
      <td>61.69</td>
      <td>84.34</td>
      <td>57.87</td>
      <td>51.26</td>
      <td>75.77</td>
      <td>11.60</td>
      <td>8.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>8.0</td>
      <td>True</td>
      <td>5e4024b6694bb13f1a81ce4277ac9141f0b226df</td>
      <td>IkariDev/Athena-v3</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/duplicitous-slurpbeast-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__duplicitous-slurpbeast-13b</td>
      <td>50.10</td>
      <td>62.12</td>
      <td>83.92</td>
      <td>57.53</td>
      <td>52.33</td>
      <td>75.06</td>
      <td>8.79</td>
      <td>10.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>88dc61b7afebf2220ca42898e1286c59961ed440</td>
      <td>chargoddard/duplicitous-slurpbeast-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/mcq-vicuna-13b-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__mcq-vicuna-13b-v1.5</td>
      <td>50.10</td>
      <td>56.66</td>
      <td>81.09</td>
      <td>53.30</td>
      <td>43.99</td>
      <td>73.01</td>
      <td>8.04</td>
      <td>34.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f769a92cfeffe8ee07beee8814ce7eca7cd62805</td>
      <td>luffycodes/mcq-vicuna-13b-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/llama-13b-pretrained-sft-do2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__llama-13b-pretrained-sft-do2</td>
      <td>50.10</td>
      <td>58.96</td>
      <td>80.32</td>
      <td>47.25</td>
      <td>47.41</td>
      <td>75.53</td>
      <td>9.25</td>
      <td>31.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>6cb016f5bfcbc24ee08312b52f08ef5e8f860871</td>
      <td>dvruette/llama-13b-pretrained-sft-do2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/vicuna-33b-coder</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__vicuna-33b-coder</td>
      <td>50.08</td>
      <td>60.41</td>
      <td>83.27</td>
      <td>57.17</td>
      <td>51.79</td>
      <td>76.87</td>
      <td>12.89</td>
      <td>8.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>67f6e669d7a15c1104a1478057f3752a503e83c0</td>
      <td>FelixChao/vicuna-33b-coder</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/VicUnlocked-30B-LoRA-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__VicUnlocked-30B-LoRA-HF</td>
      <td>50.06</td>
      <td>59.73</td>
      <td>84.02</td>
      <td>57.81</td>
      <td>48.54</td>
      <td>79.48</td>
      <td>14.40</td>
      <td>6.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>3259cb3c2a10cfb429fb51c4a76fffa049f4c44d</td>
      <td>TheBloke/VicUnlocked-30B-LoRA-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-13b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-13b-instruct</td>
      <td>50.06</td>
      <td>57.94</td>
      <td>81.32</td>
      <td>47.62</td>
      <td>50.23</td>
      <td>77.11</td>
      <td>11.83</td>
      <td>24.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>12.85</td>
      <td>13.0</td>
      <td>True</td>
      <td>a13e08a36c355d64fae59f28162e5fa542a8d235</td>
      <td>bofenghuang/vigogne-13b-instruct</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/heegyu/LIMA2-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__LIMA2-13b-hf</td>
      <td>50.00</td>
      <td>60.24</td>
      <td>83.69</td>
      <td>53.17</td>
      <td>41.81</td>
      <td>73.24</td>
      <td>5.76</td>
      <td>32.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ed3535921eb24e0737f9a6cda70b1a3fd71532cd</td>
      <td>heegyu/LIMA2-13b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Capybara-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Capybara-7B</td>
      <td>50.00</td>
      <td>55.20</td>
      <td>80.76</td>
      <td>48.80</td>
      <td>51.07</td>
      <td>73.40</td>
      <td>6.90</td>
      <td>33.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>[mit]</td>
      <td>6.61</td>
      <td>15.0</td>
      <td>True</td>
      <td>42dfc6f7d735670e2f3e30b0919708a81f9a0df9</td>
      <td>NousResearch/Capybara-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Severian/ANIMA-Phi-Neptune-Mistral-7B-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Severian__ANIMA-Phi-Neptune-Mistral-7B-v4</td>
      <td>49.98</td>
      <td>55.46</td>
      <td>77.63</td>
      <td>53.12</td>
      <td>59.01</td>
      <td>73.48</td>
      <td>14.94</td>
      <td>16.25</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>7.11</td>
      <td>10.0</td>
      <td>True</td>
      <td>a8e18f970f7ca994740177d6c228adee9e17aba9</td>
      <td>Severian/ANIMA-Phi-Neptune-Mistral-7B-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/chickencaesar/llama2-platypus-llama2-chat-13B-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chickencaesar__llama2-platypus-llama2-chat-13B-hf</td>
      <td>49.95</td>
      <td>62.97</td>
      <td>82.75</td>
      <td>56.86</td>
      <td>42.93</td>
      <td>76.32</td>
      <td>2.81</td>
      <td>25.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e35bb473156d74c8b5ad23a5e9df815891e8139a</td>
      <td>chickencaesar/llama2-platypus-llama2-chat-13B-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Nous-Capybara-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Nous-Capybara-7B</td>
      <td>49.94</td>
      <td>55.29</td>
      <td>80.73</td>
      <td>48.72</td>
      <td>51.13</td>
      <td>73.32</td>
      <td>6.97</td>
      <td>33.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>[mit]</td>
      <td>6.61</td>
      <td>15.0</td>
      <td>True</td>
      <td>42dfc6f7d735670e2f3e30b0919708a81f9a0df9</td>
      <td>NousResearch/Nous-Capybara-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HyperbeeAI/Tulpar-7b-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HyperbeeAI__Tulpar-7b-v1</td>
      <td>49.94</td>
      <td>57.00</td>
      <td>79.69</td>
      <td>51.33</td>
      <td>51.83</td>
      <td>72.45</td>
      <td>0.68</td>
      <td>36.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>719d8e1eb4a820f01e0a92ef6220d041964bb472</td>
      <td>HyperbeeAI/Tulpar-7b-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Severian/ANIMA-Phi-Neptune-Mistral-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Severian__ANIMA-Phi-Neptune-Mistral-7B</td>
      <td>49.93</td>
      <td>55.97</td>
      <td>76.22</td>
      <td>52.89</td>
      <td>59.76</td>
      <td>73.48</td>
      <td>14.94</td>
      <td>16.25</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>7.11</td>
      <td>10.0</td>
      <td>False</td>
      <td>e8e9a4804c842b84def9e9aaae38236d4754f277</td>
      <td>Severian/ANIMA-Phi-Neptune-Mistral-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/spicyboros-7b-2.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__spicyboros-7b-2.2</td>
      <td>49.92</td>
      <td>56.57</td>
      <td>80.09</td>
      <td>48.47</td>
      <td>47.22</td>
      <td>74.51</td>
      <td>4.85</td>
      <td>37.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>20.0</td>
      <td>True</td>
      <td>fdf075081555f3ed84c037e8dd3fe85c3b3609d7</td>
      <td>jondurbin/spicyboros-7b-2.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-13B-V1-1-SuperHOT-8K-fp16</td>
      <td>49.92</td>
      <td>58.62</td>
      <td>81.07</td>
      <td>48.32</td>
      <td>54.19</td>
      <td>76.01</td>
      <td>0.76</td>
      <td>30.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>83905656ca3e63877b8d9f3a74118da0c9bc6939</td>
      <td>TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-fp16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/rombodawg/LosslessMegaCoder-llama2-13b-mini</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_rombodawg__LosslessMegaCoder-llama2-13b-mini</td>
      <td>49.92</td>
      <td>60.58</td>
      <td>81.26</td>
      <td>57.92</td>
      <td>48.89</td>
      <td>76.95</td>
      <td>15.92</td>
      <td>7.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>1f5609ffd40bc3af2dcbc5c88e9312d47a73c4b4</td>
      <td>rombodawg/LosslessMegaCoder-llama2-13b-mini</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/andreaskoepf/llama2-13b-megacode2_min100</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_andreaskoepf__llama2-13b-megacode2_min100</td>
      <td>49.92</td>
      <td>60.58</td>
      <td>81.26</td>
      <td>57.92</td>
      <td>48.89</td>
      <td>76.95</td>
      <td>15.92</td>
      <td>7.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b38d1b53c358a0313c69bcceebe97628327ada82</td>
      <td>andreaskoepf/llama2-13b-megacode2_min100</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/mcq-vicuna-13b-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__mcq-vicuna-13b-v1.5</td>
      <td>49.91</td>
      <td>56.23</td>
      <td>81.15</td>
      <td>53.38</td>
      <td>44.08</td>
      <td>72.93</td>
      <td>7.51</td>
      <td>34.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f769a92cfeffe8ee07beee8814ce7eca7cd62805</td>
      <td>luffycodes/mcq-vicuna-13b-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__OpenOrcaxOpenChat-Preview2-13B-GPTQ</td>
      <td>49.91</td>
      <td>61.26</td>
      <td>82.14</td>
      <td>57.85</td>
      <td>50.22</td>
      <td>77.11</td>
      <td>12.43</td>
      <td>8.35</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>16.24</td>
      <td>21.0</td>
      <td>True</td>
      <td>ec9eb4f471b5bb6a7e5e505369628586c0c72252</td>
      <td>TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/llama-13b-pretrained</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__llama-13b-pretrained</td>
      <td>49.90</td>
      <td>56.31</td>
      <td>79.32</td>
      <td>47.03</td>
      <td>48.42</td>
      <td>76.95</td>
      <td>16.07</td>
      <td>25.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c28cc0cf5a1a1bf4de96b23d06b02129dca85eb9</td>
      <td>dvruette/llama-13b-pretrained</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gradientputri/MegaMix-S1-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gradientputri__MegaMix-S1-13B</td>
      <td>49.89</td>
      <td>62.46</td>
      <td>83.65</td>
      <td>57.88</td>
      <td>44.52</td>
      <td>75.85</td>
      <td>18.35</td>
      <td>6.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>afca2c9488cf8738faec4db6721f6a4c755a5d81</td>
      <td>gradientputri/MegaMix-S1-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/Chronorctypus-Limarobormes-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__Chronorctypus-Limarobormes-13b</td>
      <td>49.88</td>
      <td>59.90</td>
      <td>82.75</td>
      <td>58.45</td>
      <td>51.90</td>
      <td>74.43</td>
      <td>3.87</td>
      <td>17.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>10.0</td>
      <td>True</td>
      <td>75c1bf5f4b40cf61873ff6487ccd3efc4f684330</td>
      <td>chargoddard/Chronorctypus-Limarobormes-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/JosephusCheung/Qwen-LLaMAfied-7B-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__Qwen-LLaMAfied-7B-Chat</td>
      <td>49.88</td>
      <td>50.94</td>
      <td>83.47</td>
      <td>53.52</td>
      <td>46.09</td>
      <td>73.16</td>
      <td>4.78</td>
      <td>37.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>4d70cf0047a7a5cd2c864bc2606e81f0830e4405</td>
      <td>JosephusCheung/Qwen-LLaMAfied-7B-Chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ziqingyang/chinese-llama-2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ziqingyang__chinese-llama-2-13b</td>
      <td>49.87</td>
      <td>55.80</td>
      <td>79.53</td>
      <td>53.01</td>
      <td>38.24</td>
      <td>75.69</td>
      <td>3.94</td>
      <td>42.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.97</td>
      <td>22.0</td>
      <td>True</td>
      <td>484c8a18b02f95eb2b6f6302105cf9a329e76ec8</td>
      <td>ziqingyang/chinese-llama-2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r4-gate_up_down</td>
      <td>49.87</td>
      <td>55.38</td>
      <td>81.92</td>
      <td>55.28</td>
      <td>40.76</td>
      <td>76.09</td>
      <td>13.72</td>
      <td>25.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2ca747d779feaa99c475b8015c9b4a50aea41cd2</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r4-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/xxyyy123/10k_v1_lora_qkvo_rank28_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_xxyyy123__10k_v1_lora_qkvo_rank28_v2</td>
      <td>49.86</td>
      <td>55.38</td>
      <td>79.21</td>
      <td>50.50</td>
      <td>52.75</td>
      <td>73.24</td>
      <td>0.61</td>
      <td>37.37</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>70e38a7424544193f0ad6a93ae26a5bfd15e4e90</td>
      <td>xxyyy123/10k_v1_lora_qkvo_rank28_v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1</td>
      <td>49.86</td>
      <td>55.80</td>
      <td>82.27</td>
      <td>55.63</td>
      <td>38.15</td>
      <td>77.43</td>
      <td>12.66</td>
      <td>27.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>48b8ceeb62e5ca897f284bbc0923201689af7c89</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r16-gate_up_down-test1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down</td>
      <td>49.85</td>
      <td>56.40</td>
      <td>81.93</td>
      <td>53.63</td>
      <td>39.23</td>
      <td>76.95</td>
      <td>11.98</td>
      <td>28.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>dd61a482fa2f71efe6f22aae6949355ca4b06ccc</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Medusa-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Medusa-13b</td>
      <td>49.85</td>
      <td>58.19</td>
      <td>81.35</td>
      <td>57.39</td>
      <td>51.24</td>
      <td>73.32</td>
      <td>6.82</td>
      <td>20.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>be755c9eef8233ca59e0178db75de878f5859222</td>
      <td>Sao10K/Medusa-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/OpenOrcaxOpenChat-Preview2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__OpenOrcaxOpenChat-Preview2-13B</td>
      <td>49.85</td>
      <td>62.71</td>
      <td>81.99</td>
      <td>57.51</td>
      <td>47.45</td>
      <td>76.80</td>
      <td>13.72</td>
      <td>8.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>94.0</td>
      <td>True</td>
      <td>26d1bc5c54c1f60a5de0b1ed4d0b16f285aee230</td>
      <td>Open-Orca/OpenOrcaxOpenChat-Preview2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-2.2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-2.2.1</td>
      <td>49.83</td>
      <td>60.92</td>
      <td>83.77</td>
      <td>56.47</td>
      <td>49.42</td>
      <td>76.01</td>
      <td>11.60</td>
      <td>10.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>9b2dbc1f6f17a162228799df6e9449c903ddf04d</td>
      <td>jondurbin/airoboros-l2-13b-2.2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down</td>
      <td>49.81</td>
      <td>57.25</td>
      <td>81.79</td>
      <td>53.96</td>
      <td>39.66</td>
      <td>77.82</td>
      <td>11.75</td>
      <td>26.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>8a75b17d4b60f820159bb0100f26f438727bb199</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r8-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/Llama-2-13b-FINETUNE4_compare8k2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__Llama-2-13b-FINETUNE4_compare8k2</td>
      <td>49.81</td>
      <td>58.28</td>
      <td>81.39</td>
      <td>56.87</td>
      <td>39.86</td>
      <td>76.01</td>
      <td>11.90</td>
      <td>24.36</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>fe1b604097aad9408ce63fa7ffc9c320cdd06e4f</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_compare8k2</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-13B-V1.1-GPTQ</td>
      <td>49.81</td>
      <td>58.53</td>
      <td>80.66</td>
      <td>49.59</td>
      <td>54.35</td>
      <td>74.43</td>
      <td>8.11</td>
      <td>22.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>26.0</td>
      <td>True</td>
      <td>9df807ac64034bc6e7387326689d6e39656ce5e0</td>
      <td>TheBloke/WizardLM-13B-V1.1-GPTQ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/chargoddard/duplicitous-mammal-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__duplicitous-mammal-13b</td>
      <td>49.80</td>
      <td>61.69</td>
      <td>83.79</td>
      <td>57.50</td>
      <td>52.27</td>
      <td>75.06</td>
      <td>9.10</td>
      <td>9.20</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>a05d0562b8da2ac2e76aa65984e8063249bc85c8</td>
      <td>chargoddard/duplicitous-mammal-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down</td>
      <td>49.79</td>
      <td>58.70</td>
      <td>81.89</td>
      <td>56.08</td>
      <td>38.95</td>
      <td>77.35</td>
      <td>12.96</td>
      <td>22.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>4c3a4cb54c0487666bd58589b50f90c22de80969</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r16-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zarafusionix-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zarafusionix-l2-7b</td>
      <td>49.78</td>
      <td>55.55</td>
      <td>79.40</td>
      <td>51.21</td>
      <td>51.05</td>
      <td>74.66</td>
      <td>7.20</td>
      <td>29.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>13d0e2498a4b5f53f6dc2464f20e093b07a4bd4b</td>
      <td>zarakiquemparte/zarafusionix-l2-7b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/huggyllama/llama-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huggyllama__llama-30b</td>
      <td>49.73</td>
      <td>61.43</td>
      <td>84.73</td>
      <td>58.45</td>
      <td>42.27</td>
      <td>80.03</td>
      <td>14.86</td>
      <td>6.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.53</td>
      <td>37.0</td>
      <td>True</td>
      <td>2b1edcdb3c7ced7bce6c1aa75c94545777c3118b</td>
      <td>huggyllama/llama-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/budecosystem/genz-13b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_budecosystem__genz-13b-v2</td>
      <td>49.72</td>
      <td>55.97</td>
      <td>79.98</td>
      <td>54.30</td>
      <td>48.09</td>
      <td>74.59</td>
      <td>12.28</td>
      <td>22.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>98e0e2086df11b9f80e1571110540a657e52c2e8</td>
      <td>budecosystem/genz-13b-v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2xOpenOrca-13B-IA3-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2xOpenOrca-13B-IA3-v3</td>
      <td>49.72</td>
      <td>62.54</td>
      <td>82.10</td>
      <td>58.67</td>
      <td>46.96</td>
      <td>77.82</td>
      <td>12.36</td>
      <td>7.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>17493c1f2e4620a44d7947edad0386d338e805ce</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3-v3</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch</td>
      <td>49.71</td>
      <td>59.81</td>
      <td>82.69</td>
      <td>56.96</td>
      <td>52.92</td>
      <td>74.43</td>
      <td>2.35</td>
      <td>18.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5427ceec420f943a0b011a4d96f3efc292306933</td>
      <td>TFLai/OpenOrcaPlatypus2-Platypus2-13B-QLora-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yhyu13/llama-30B-hf-openassitant</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yhyu13__llama-30B-hf-openassitant</td>
      <td>49.71</td>
      <td>61.26</td>
      <td>84.73</td>
      <td>58.47</td>
      <td>42.27</td>
      <td>80.03</td>
      <td>14.86</td>
      <td>6.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>fba493af11a73cf5a2ee7857dd7aecb98c659dc4</td>
      <td>Yhyu13/llama-30B-hf-openassitant</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llama-30b</td>
      <td>49.71</td>
      <td>61.26</td>
      <td>84.73</td>
      <td>58.47</td>
      <td>42.27</td>
      <td>80.03</td>
      <td>14.86</td>
      <td>6.33</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.53</td>
      <td>0.0</td>
      <td>False</td>
      <td>13c77caa472bfa79d4f3f0ec82cbdc9dd88e5d22</td>
      <td>huggingface/llama-30b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/totally-not-an-llm/PuddleJumper-13b-V2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_totally-not-an-llm__PuddleJumper-13b-V2</td>
      <td>49.69</td>
      <td>57.00</td>
      <td>81.06</td>
      <td>58.30</td>
      <td>52.66</td>
      <td>72.45</td>
      <td>3.64</td>
      <td>22.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>1fe9494e334a32ba73dc2926f58246450850c534</td>
      <td>totally-not-an-llm/PuddleJumper-13b-V2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2xOpenOrca-13B-IA3-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2xOpenOrca-13B-IA3-v4</td>
      <td>49.69</td>
      <td>61.43</td>
      <td>81.84</td>
      <td>59.02</td>
      <td>48.64</td>
      <td>77.19</td>
      <td>10.84</td>
      <td>8.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>3aa9abe9cb2e5c699f80935e04fbb351cdfbf21b</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3-v4</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Orca-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Orca-Nova-13B</td>
      <td>49.69</td>
      <td>62.37</td>
      <td>82.47</td>
      <td>57.44</td>
      <td>45.97</td>
      <td>77.58</td>
      <td>14.48</td>
      <td>7.52</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a6c3686749ecb76971a915403da8c07a98078a6</td>
      <td>TFLai/Orca-Nova-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/duliadotio/dulia-13b-8k-alpha</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_duliadotio__dulia-13b-8k-alpha</td>
      <td>49.67</td>
      <td>60.67</td>
      <td>82.00</td>
      <td>56.87</td>
      <td>42.59</td>
      <td>77.19</td>
      <td>10.69</td>
      <td>17.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>c3bcafd7f6133a7e7c069f8765a99fe84989d926</td>
      <td>duliadotio/dulia-13b-8k-alpha</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/llama2-13b-orca-8k-3319</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__llama2-13b-orca-8k-3319</td>
      <td>49.67</td>
      <td>60.75</td>
      <td>81.91</td>
      <td>57.06</td>
      <td>42.64</td>
      <td>77.19</td>
      <td>10.99</td>
      <td>17.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>115.0</td>
      <td>True</td>
      <td>160f58ec85ef25ad935eb583f14c7e8c7f7e7839</td>
      <td>OpenAssistant/llama2-13b-orca-8k-3319</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-mistral-7b-v13-base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-mistral-7b-v13-base</td>
      <td>49.67</td>
      <td>52.90</td>
      <td>76.12</td>
      <td>57.54</td>
      <td>52.82</td>
      <td>71.35</td>
      <td>1.21</td>
      <td>35.72</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.13</td>
      <td>0.0</td>
      <td>True</td>
      <td>8ff18d61b1c8295ecd73153b8e0b63934187a50e</td>
      <td>OpenBuddy/openbuddy-mistral-7b-v13-base</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/xxyyy123/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_xxyyy123__mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2</td>
      <td>49.66</td>
      <td>57.17</td>
      <td>79.57</td>
      <td>50.24</td>
      <td>52.51</td>
      <td>72.93</td>
      <td>0.38</td>
      <td>34.85</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9c4a7444d6fb12931e50f111053e016531fe60b7</td>
      <td>xxyyy123/mc_data_30k_from_platpus_orca_7b_10k_v1_lora_qkvo_rank14_v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/EnsembleV5-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__EnsembleV5-Nova-13B</td>
      <td>49.65</td>
      <td>62.71</td>
      <td>82.55</td>
      <td>56.79</td>
      <td>49.86</td>
      <td>76.24</td>
      <td>10.77</td>
      <td>8.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>7ba38d309709d35149b4a18f94096875885035ae</td>
      <td>TFLai/EnsembleV5-Nova-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PulsarAI/EnsembleV5-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PulsarAI__EnsembleV5-Nova-13B</td>
      <td>49.65</td>
      <td>62.71</td>
      <td>82.55</td>
      <td>56.79</td>
      <td>49.86</td>
      <td>76.24</td>
      <td>10.77</td>
      <td>8.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>3e25556187ba576082a85c270d2d4b4ea6ea9f6f</td>
      <td>PulsarAI/EnsembleV5-Nova-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-2-7b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-2-7b-chat</td>
      <td>49.65</td>
      <td>55.63</td>
      <td>78.71</td>
      <td>50.98</td>
      <td>47.21</td>
      <td>74.43</td>
      <td>7.73</td>
      <td>32.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>14.0</td>
      <td>True</td>
      <td>7a1b76feabe3e0ed007ea83ee93f7644156d3b23</td>
      <td>bofenghuang/vigogne-2-7b-chat</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Nova-13B</td>
      <td>49.64</td>
      <td>62.71</td>
      <td>82.57</td>
      <td>57.98</td>
      <td>51.34</td>
      <td>77.27</td>
      <td>6.75</td>
      <td>8.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>ae1145f9fa846ab8d39d8b7da888287ef917efb5</td>
      <td>TFLai/Nova-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2xOpenOrca-13B-IA3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2xOpenOrca-13B-IA3</td>
      <td>49.63</td>
      <td>62.12</td>
      <td>82.10</td>
      <td>58.84</td>
      <td>47.88</td>
      <td>77.11</td>
      <td>11.83</td>
      <td>7.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>5ca46029dd22c007d4dc1706f6284a32be4546c2</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/llama-13b-pretrained-sft-epoch-1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__llama-13b-pretrained-sft-epoch-1</td>
      <td>49.63</td>
      <td>57.25</td>
      <td>79.99</td>
      <td>45.52</td>
      <td>44.45</td>
      <td>77.58</td>
      <td>13.87</td>
      <td>28.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>1f839c019153789c15bbc45ecbb512d0f5015881</td>
      <td>dvruette/llama-13b-pretrained-sft-epoch-1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Medusa-1.1-L2-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Medusa-1.1-L2-7B</td>
      <td>49.62</td>
      <td>56.48</td>
      <td>78.57</td>
      <td>51.56</td>
      <td>47.70</td>
      <td>75.06</td>
      <td>1.44</td>
      <td>36.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>df23c3d22bc546dbce0267415e94bdb482446c06</td>
      <td>Sao10K/Medusa-1.1-L2-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-2-7b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-2-7b-instruct</td>
      <td>49.62</td>
      <td>56.23</td>
      <td>79.97</td>
      <td>47.17</td>
      <td>49.51</td>
      <td>75.45</td>
      <td>3.79</td>
      <td>35.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>17.0</td>
      <td>True</td>
      <td>8f4dd9c870f748322989168af5c109e16b01c63d</td>
      <td>bofenghuang/vigogne-2-7b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/llama2-13b-megacode2-oasst</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__llama2-13b-megacode2-oasst</td>
      <td>49.61</td>
      <td>60.67</td>
      <td>81.93</td>
      <td>57.38</td>
      <td>47.85</td>
      <td>76.16</td>
      <td>15.54</td>
      <td>7.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>9.0</td>
      <td>True</td>
      <td>2c45ecf161da2ff2aa984900f2e4d2b7a7311ab8</td>
      <td>OpenAssistant/llama2-13b-megacode2-oasst</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bhenrym14/airophin-13b-pntk-16k-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bhenrym14__airophin-13b-pntk-16k-fp16</td>
      <td>49.59</td>
      <td>61.18</td>
      <td>82.86</td>
      <td>55.19</td>
      <td>43.20</td>
      <td>76.16</td>
      <td>8.04</td>
      <td>20.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>6b5418b69e8270df659eacb192f469e7c3af70b3</td>
      <td>bhenrym14/airophin-13b-pntk-16k-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Chronos-Beluga-v2-13bfp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Chronos-Beluga-v2-13bfp16</td>
      <td>49.58</td>
      <td>60.75</td>
      <td>81.94</td>
      <td>54.08</td>
      <td>53.23</td>
      <td>73.80</td>
      <td>4.62</td>
      <td>18.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>6d50e6681bc26c9bc0c8377c26c438e295ee0c2f</td>
      <td>The-Face-Of-Goonery/Chronos-Beluga-v2-13bfp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Doctor-Shotgun/CalliopeDS-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Doctor-Shotgun__CalliopeDS-L2-13B</td>
      <td>49.57</td>
      <td>60.49</td>
      <td>83.38</td>
      <td>55.80</td>
      <td>51.32</td>
      <td>77.03</td>
      <td>10.01</td>
      <td>8.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>agpl-3.0</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>b373eda586a6527e62382eda5480204652a82499</td>
      <td>Doctor-Shotgun/CalliopeDS-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/minotaur-13b-fixed</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__minotaur-13b-fixed</td>
      <td>49.57</td>
      <td>59.04</td>
      <td>81.66</td>
      <td>50.10</td>
      <td>50.36</td>
      <td>76.87</td>
      <td>13.12</td>
      <td>15.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>5dac6f7559dba1c6fb59fee18c3e713cc3c83db7</td>
      <td>openaccess-ai-collective/minotaur-13b-fixed</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-Inverted-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-Inverted-L2-13B</td>
      <td>49.57</td>
      <td>59.30</td>
      <td>82.90</td>
      <td>56.45</td>
      <td>52.04</td>
      <td>74.74</td>
      <td>13.19</td>
      <td>8.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>efaf592c95ae8e769e0d56d36ba4ed23e3bf4059</td>
      <td>Sao10K/Stheno-Inverted-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lvkaokao/llama2-7b-hf-chat-lora-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lvkaokao__llama2-7b-hf-chat-lora-v2</td>
      <td>49.55</td>
      <td>55.03</td>
      <td>78.81</td>
      <td>51.35</td>
      <td>44.05</td>
      <td>74.90</td>
      <td>10.84</td>
      <td>31.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>0b8e61d3325cddbad207cbf885c2b5db6a83a059</td>
      <td>lvkaokao/llama2-7b-hf-chat-lora-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/Llama-2-13b-FINETUNE4_TEST2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__Llama-2-13b-FINETUNE4_TEST2</td>
      <td>49.55</td>
      <td>58.45</td>
      <td>81.70</td>
      <td>56.61</td>
      <td>40.19</td>
      <td>76.64</td>
      <td>13.19</td>
      <td>20.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e312c4c59cab9d130c33288c92aad7c0cb5331d5</td>
      <td>wei123602/Llama-2-13b-FINETUNE4_TEST2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/minotaur-llama2-13b-qlora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__minotaur-llama2-13b-qlora</td>
      <td>49.54</td>
      <td>60.07</td>
      <td>82.42</td>
      <td>55.87</td>
      <td>45.57</td>
      <td>76.24</td>
      <td>12.05</td>
      <td>14.53</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.00</td>
      <td>2.0</td>
      <td>False</td>
      <td>22c83f7d68e547fb0b59acfa01c60b108c59fe55</td>
      <td>ehartford/minotaur-llama2-13b-qlora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/JosephusCheung/Pwen-VL-Chat-20_30</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__Pwen-VL-Chat-20_30</td>
      <td>49.52</td>
      <td>50.17</td>
      <td>72.21</td>
      <td>56.34</td>
      <td>42.52</td>
      <td>68.35</td>
      <td>19.11</td>
      <td>37.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>64a9b89fb18140fc1af1f11471dc9fe34ebc7446</td>
      <td>JosephusCheung/Pwen-VL-Chat-20_30</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-13B-Uncensored-HF</td>
      <td>49.52</td>
      <td>58.96</td>
      <td>81.95</td>
      <td>47.92</td>
      <td>51.69</td>
      <td>75.69</td>
      <td>8.64</td>
      <td>21.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>197.0</td>
      <td>True</td>
      <td>fff9ac7f0e2e7b340f2301f5f089d989fc03be67</td>
      <td>TheBloke/Wizard-Vicuna-13B-Uncensored-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/Wizard-Vicuna-13B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Wizard-Vicuna-13B-Uncensored</td>
      <td>49.52</td>
      <td>58.96</td>
      <td>81.95</td>
      <td>47.92</td>
      <td>51.69</td>
      <td>75.69</td>
      <td>8.64</td>
      <td>21.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>218.0</td>
      <td>True</td>
      <td>95bfd1640a54e76b3e857c2462fd3a77eca0b275</td>
      <td>ehartford/Wizard-Vicuna-13B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/64bits/LexPodLM-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_64bits__LexPodLM-13B</td>
      <td>49.51</td>
      <td>57.76</td>
      <td>81.04</td>
      <td>48.38</td>
      <td>43.48</td>
      <td>76.16</td>
      <td>0.00</td>
      <td>39.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>3553d84037addc97678f99a3464be4c866a0c268</td>
      <td>64bits/LexPodLM-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Ensemble5-Platypus2-13B-QLora-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Ensemble5-Platypus2-13B-QLora-0.80-epoch</td>
      <td>49.50</td>
      <td>59.73</td>
      <td>82.66</td>
      <td>56.94</td>
      <td>52.92</td>
      <td>74.43</td>
      <td>1.90</td>
      <td>17.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>2af03c3287c60c4ba2fb6afa86c26cf722ab001d</td>
      <td>TFLai/Ensemble5-Platypus2-13B-QLora-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-13b-v1.5-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-13b-v1.5-16k</td>
      <td>49.49</td>
      <td>56.74</td>
      <td>80.37</td>
      <td>55.28</td>
      <td>51.96</td>
      <td>72.38</td>
      <td>13.12</td>
      <td>16.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>167.0</td>
      <td>True</td>
      <td>277697af19d4b267626ebc9f4e078d19a9a0fddf</td>
      <td>lmsys/vicuna-13b-v1.5-16k</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lvkaokao/llama2-7b-hf-instruction-lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lvkaokao__llama2-7b-hf-instruction-lora</td>
      <td>49.49</td>
      <td>55.38</td>
      <td>78.57</td>
      <td>49.39</td>
      <td>41.83</td>
      <td>74.19</td>
      <td>9.86</td>
      <td>37.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>f660a40323b29040e78097acca320517ed242512</td>
      <td>lvkaokao/llama2-7b-hf-instruction-lora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_FINETUNE2_3w</td>
      <td>49.46</td>
      <td>58.62</td>
      <td>82.32</td>
      <td>54.25</td>
      <td>38.17</td>
      <td>76.80</td>
      <td>11.98</td>
      <td>24.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>08bc7112a775dd4223d441355f3d619694013789</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama-13b-v1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama-13b-v1.2</td>
      <td>49.43</td>
      <td>56.74</td>
      <td>80.34</td>
      <td>48.90</td>
      <td>51.00</td>
      <td>75.93</td>
      <td>8.04</td>
      <td>25.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>c0a56d9f5a15bea07493191b5a6295f6797a9b2c</td>
      <td>YeungNLP/firefly-llama-13b-v1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/StableBeluga-13B-instruct-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__StableBeluga-13B-instruct-PL-lora_unload</td>
      <td>49.42</td>
      <td>60.92</td>
      <td>82.13</td>
      <td>56.99</td>
      <td>48.64</td>
      <td>76.56</td>
      <td>12.21</td>
      <td>8.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>6e1a6e1f91f6ac97b643be1bd24be6096e2e7dd3</td>
      <td>Aspik101/StableBeluga-13B-instruct-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elinas/chronos-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elinas__chronos-33b</td>
      <td>49.42</td>
      <td>62.20</td>
      <td>83.48</td>
      <td>55.87</td>
      <td>46.67</td>
      <td>78.30</td>
      <td>13.04</td>
      <td>6.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>23.0</td>
      <td>True</td>
      <td>3c11f81d9180618f13777276b1eb0eb70ab99cf0</td>
      <td>elinas/chronos-33b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/SpeechlessV1-Nova-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__SpeechlessV1-Nova-13B</td>
      <td>49.41</td>
      <td>61.77</td>
      <td>82.68</td>
      <td>57.75</td>
      <td>51.44</td>
      <td>77.43</td>
      <td>5.76</td>
      <td>9.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>fbe6f0e32b5ecf9d75510d0b11a286466f46d79e</td>
      <td>TFLai/SpeechlessV1-Nova-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/mythalion-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__mythalion-13b</td>
      <td>49.41</td>
      <td>61.26</td>
      <td>83.81</td>
      <td>56.53</td>
      <td>46.56</td>
      <td>77.43</td>
      <td>13.27</td>
      <td>7.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>58.0</td>
      <td>True</td>
      <td>24916f62b8243a7e4646ea53eeb45d890cbd308f</td>
      <td>PygmalionAI/mythalion-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE1_17w-r4</td>
      <td>49.41</td>
      <td>56.74</td>
      <td>82.27</td>
      <td>56.18</td>
      <td>39.65</td>
      <td>77.03</td>
      <td>13.19</td>
      <td>20.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>7e0046627fabb0f23ace4b71f279d459ec4a0ff1</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r16-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r16-gate_up_down</td>
      <td>49.39</td>
      <td>55.80</td>
      <td>82.10</td>
      <td>55.33</td>
      <td>39.82</td>
      <td>76.24</td>
      <td>11.37</td>
      <td>25.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>86f255afabc8986c73376cafd98628a068649022</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r16-gate_up_down</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2xOpenOrca-13B-IA3-v2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2xOpenOrca-13B-IA3-v2.1</td>
      <td>49.38</td>
      <td>62.29</td>
      <td>82.09</td>
      <td>57.91</td>
      <td>47.03</td>
      <td>77.43</td>
      <td>10.99</td>
      <td>7.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>31e1e3235515717a151915131bc970be188d964e</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-IA3-v2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zaraxe-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zaraxe-l2-7b</td>
      <td>49.35</td>
      <td>57.17</td>
      <td>79.34</td>
      <td>51.00</td>
      <td>49.11</td>
      <td>73.48</td>
      <td>7.58</td>
      <td>27.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>0875bf202aedeef7a58d7382fd6f55f5bca12968</td>
      <td>zarakiquemparte/zaraxe-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__LLaMA2-13B-Tiefighter</td>
      <td>49.35</td>
      <td>59.90</td>
      <td>84.00</td>
      <td>54.98</td>
      <td>53.02</td>
      <td>74.51</td>
      <td>0.68</td>
      <td>18.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>22.0</td>
      <td>True</td>
      <td>0d193a4562d6836724485cb7df6e58ca846bbfeb</td>
      <td>KoboldAI/LLaMA2-13B-Tiefighter</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Austism/chronos-hermes-13b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Austism__chronos-hermes-13b-v2</td>
      <td>49.34</td>
      <td>60.32</td>
      <td>83.21</td>
      <td>55.05</td>
      <td>50.91</td>
      <td>75.37</td>
      <td>11.75</td>
      <td>8.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>11.0</td>
      <td>True</td>
      <td>2f0e2cb734685a6ce0736a9f3e909a795d7592cc</td>
      <td>Austism/chronos-hermes-13b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gaodrew/gaodrew-gorgonzola-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gaodrew__gaodrew-gorgonzola-13b</td>
      <td>49.33</td>
      <td>53.84</td>
      <td>78.86</td>
      <td>71.54</td>
      <td>42.58</td>
      <td>75.30</td>
      <td>10.01</td>
      <td>13.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a53fbe358d4cb546916847d861ccfaf7c724a103</td>
      <td>gaodrew/gaodrew-gorgonzola-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Open-Orca/OpenOrca-Preview1-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__OpenOrca-Preview1-13B</td>
      <td>49.33</td>
      <td>54.95</td>
      <td>78.19</td>
      <td>50.12</td>
      <td>49.05</td>
      <td>71.03</td>
      <td>4.93</td>
      <td>37.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>143.0</td>
      <td>True</td>
      <td>d120381b03051b60a7c77ec3fb1be6c3c1546466</td>
      <td>Open-Orca/OpenOrca-Preview1-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/Metharme-13b-Merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Metharme-13b-Merged</td>
      <td>49.33</td>
      <td>59.90</td>
      <td>81.12</td>
      <td>47.18</td>
      <td>51.18</td>
      <td>76.80</td>
      <td>8.72</td>
      <td>20.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>90c02cc338afcdd890a948af06432674743363ad</td>
      <td>TehVenom/Metharme-13b-Merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bhenrym14/airophin-v2-13b-PI-8k-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bhenrym14__airophin-v2-13b-PI-8k-fp16</td>
      <td>49.33</td>
      <td>60.58</td>
      <td>82.96</td>
      <td>56.75</td>
      <td>40.14</td>
      <td>76.64</td>
      <td>7.35</td>
      <td>20.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>26b7edfd282af223d86d5e539451357bb114247b</td>
      <td>bhenrym14/airophin-v2-13b-PI-8k-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-1.3-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-1.3-L2-13B</td>
      <td>49.32</td>
      <td>56.83</td>
      <td>81.70</td>
      <td>52.79</td>
      <td>50.23</td>
      <td>71.11</td>
      <td>0.23</td>
      <td>32.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>45ba2f603769aa6b97639962f522b8d7398c2393</td>
      <td>Sao10K/Stheno-1.3-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/WizardLM-1.0-Uncensored-Llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-1.0-Uncensored-Llama2-13b</td>
      <td>49.31</td>
      <td>55.72</td>
      <td>80.34</td>
      <td>55.40</td>
      <td>51.44</td>
      <td>74.66</td>
      <td>13.27</td>
      <td>14.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>36.0</td>
      <td>True</td>
      <td>134cea14627fd875f6f277cad92f988024855478</td>
      <td>ehartford/WizardLM-1.0-Uncensored-Llama2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hpcai-tech__Colossal-LLaMA-2-7b-base</td>
      <td>49.30</td>
      <td>53.50</td>
      <td>70.50</td>
      <td>54.40</td>
      <td>50.19</td>
      <td>70.01</td>
      <td>9.70</td>
      <td>36.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.76</td>
      <td>54.0</td>
      <td>True</td>
      <td>1f30e4f2037e1e30122667639b8ef37138e85057</td>
      <td>hpcai-tech/Colossal-LLaMA-2-7b-base</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Nous-Hermes-13B-SuperHOT-8K-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Nous-Hermes-13B-SuperHOT-8K-fp16</td>
      <td>49.30</td>
      <td>55.29</td>
      <td>81.87</td>
      <td>48.23</td>
      <td>51.19</td>
      <td>75.30</td>
      <td>1.21</td>
      <td>32.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>b407c1ece029ad5693d38e6e0931e9482962ed15</td>
      <td>TheBloke/Nous-Hermes-13B-SuperHOT-8K-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/13B-HyperMantis</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__13B-HyperMantis</td>
      <td>49.29</td>
      <td>58.53</td>
      <td>82.20</td>
      <td>50.61</td>
      <td>47.50</td>
      <td>76.24</td>
      <td>10.39</td>
      <td>19.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>26.0</td>
      <td>True</td>
      <td>aa828ef92c363a5577ffd7d29e678277b9d2eb3c</td>
      <td>digitous/13B-HyperMantis</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Luban-Platypus2-13B-QLora-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Luban-Platypus2-13B-QLora-0.80-epoch</td>
      <td>49.28</td>
      <td>60.24</td>
      <td>82.22</td>
      <td>58.03</td>
      <td>55.26</td>
      <td>75.37</td>
      <td>0.91</td>
      <td>12.95</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>15a99bc147cf9b744cbab7a7c8c5f232cd0c8d10</td>
      <td>TFLai/Luban-Platypus2-13B-QLora-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/llama-13b-pretrained-dropout</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__llama-13b-pretrained-dropout</td>
      <td>49.28</td>
      <td>56.40</td>
      <td>79.34</td>
      <td>46.59</td>
      <td>48.60</td>
      <td>75.22</td>
      <td>11.83</td>
      <td>27.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>045c84727d495bfb4b612a2482ce0d807c067b46</td>
      <td>dvruette/llama-13b-pretrained-dropout</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-c34b-2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-c34b-2.1</td>
      <td>49.27</td>
      <td>54.69</td>
      <td>76.45</td>
      <td>55.08</td>
      <td>46.15</td>
      <td>68.43</td>
      <td>8.34</td>
      <td>35.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>13.0</td>
      <td>True</td>
      <td>2caa8ce3aab012bf34c7c531827f6befc7cc1c98</td>
      <td>jondurbin/airoboros-c34b-2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zarafusionex-1.2-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zarafusionex-1.2-l2-7b</td>
      <td>49.27</td>
      <td>56.66</td>
      <td>79.16</td>
      <td>51.94</td>
      <td>51.29</td>
      <td>74.74</td>
      <td>8.57</td>
      <td>22.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>68ca01427848528ab21263fd06720a081b09d063</td>
      <td>zarakiquemparte/zarafusionex-1.2-l2-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/maywell/Synatra-11B-Testbench</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_maywell__Synatra-11B-Testbench</td>
      <td>49.26</td>
      <td>57.34</td>
      <td>78.66</td>
      <td>55.56</td>
      <td>51.97</td>
      <td>75.77</td>
      <td>17.74</td>
      <td>7.77</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>11.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>9399ea6c2a1d955e31d6b4d68b2b86115aea0e59</td>
      <td>maywell/Synatra-11B-Testbench</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardLM-13B-V1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardLM-13B-V1.2</td>
      <td>49.25</td>
      <td>59.04</td>
      <td>82.21</td>
      <td>54.64</td>
      <td>47.27</td>
      <td>71.90</td>
      <td>13.50</td>
      <td>16.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>157.0</td>
      <td>True</td>
      <td>6760d0c07ffdc2405295ed7a29437cf4dc414bac</td>
      <td>WizardLM/WizardLM-13B-V1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/WizardLM-1.0-Uncensored-Llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-1.0-Uncensored-Llama2-13b</td>
      <td>49.22</td>
      <td>55.80</td>
      <td>80.41</td>
      <td>55.59</td>
      <td>51.42</td>
      <td>74.11</td>
      <td>13.27</td>
      <td>13.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>36.0</td>
      <td>True</td>
      <td>134cea14627fd875f6f277cad92f988024855478</td>
      <td>ehartford/WizardLM-1.0-Uncensored-Llama2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-AdventurousWinds-Mk2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-AdventurousWinds-Mk2-7b</td>
      <td>49.21</td>
      <td>58.19</td>
      <td>83.48</td>
      <td>61.80</td>
      <td>43.56</td>
      <td>76.32</td>
      <td>14.94</td>
      <td>6.19</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>cfcc969a7e97275b2298253f1eabf4575e5a3768</td>
      <td>PocketDoc/Dans-AdventurousWinds-Mk2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-2.1</td>
      <td>49.19</td>
      <td>55.12</td>
      <td>80.24</td>
      <td>50.89</td>
      <td>44.62</td>
      <td>71.90</td>
      <td>2.27</td>
      <td>39.31</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>172e30e56e939f73d7d00a165c2d49cbd284481f</td>
      <td>jondurbin/airoboros-l2-13b-2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/llama2-13b-FINETUNE3_TEST</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__llama2-13b-FINETUNE3_TEST</td>
      <td>49.19</td>
      <td>53.67</td>
      <td>79.66</td>
      <td>54.48</td>
      <td>40.22</td>
      <td>75.93</td>
      <td>14.56</td>
      <td>25.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>22cea7bf138eb0d6c962812df2b2235290acbee2</td>
      <td>wei123602/llama2-13b-FINETUNE3_TEST</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v3.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v3.1</td>
      <td>49.18</td>
      <td>59.81</td>
      <td>82.80</td>
      <td>56.76</td>
      <td>44.45</td>
      <td>76.24</td>
      <td>18.12</td>
      <td>6.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>a95be7130d32da99bcd484f6f436b2dd49341110</td>
      <td>openchat/openchat_v3.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2xOpenOrca-13B-LoRa</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2xOpenOrca-13B-LoRa</td>
      <td>49.16</td>
      <td>60.75</td>
      <td>82.09</td>
      <td>58.77</td>
      <td>45.15</td>
      <td>77.03</td>
      <td>7.13</td>
      <td>13.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>8b2f5d65c03d415b7c43530def622e133e1ef014</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-LoRa</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/Llama-2-13b-FINETUNE4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__Llama-2-13b-FINETUNE4</td>
      <td>49.16</td>
      <td>58.70</td>
      <td>81.93</td>
      <td>57.21</td>
      <td>43.26</td>
      <td>76.95</td>
      <td>12.51</td>
      <td>13.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>939d06081210fa943c60210a47583f43b60901ad</td>
      <td>wei123602/Llama-2-13b-FINETUNE4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LLMs/WizardLM-13B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LLMs__WizardLM-13B-V1.0</td>
      <td>49.13</td>
      <td>57.25</td>
      <td>80.88</td>
      <td>52.92</td>
      <td>50.55</td>
      <td>74.11</td>
      <td>14.10</td>
      <td>14.07</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>f802ea7c01e2da27b0f7091c70d3ecfd8fc042b9</td>
      <td>LLMs/WizardLM-13B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/internlm/internlm-20b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_internlm__internlm-20b-chat</td>
      <td>49.12</td>
      <td>55.38</td>
      <td>78.58</td>
      <td>58.53</td>
      <td>43.22</td>
      <td>78.77</td>
      <td>18.73</td>
      <td>10.65</td>
      <td>InternLMForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.00</td>
      <td>114.0</td>
      <td>True</td>
      <td>79946225fa7a215e0ebcf4440a9cce88e475deaa</td>
      <td>internlm/internlm-20b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama-13b</td>
      <td>49.12</td>
      <td>58.96</td>
      <td>79.71</td>
      <td>49.10</td>
      <td>49.59</td>
      <td>75.61</td>
      <td>8.19</td>
      <td>22.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>dd326f89ce885844d714d9ab33603e0d17f56cc5</td>
      <td>YeungNLP/firefly-llama-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/wizardLM-13B-1.0-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__wizardLM-13B-1.0-fp16</td>
      <td>49.09</td>
      <td>57.25</td>
      <td>80.88</td>
      <td>52.90</td>
      <td>50.55</td>
      <td>74.11</td>
      <td>13.87</td>
      <td>14.07</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>b79733805e98e668ff9a459975c259881b1b8014</td>
      <td>TheBloke/wizardLM-13B-1.0-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE1_17w-r16</td>
      <td>49.09</td>
      <td>57.25</td>
      <td>82.27</td>
      <td>56.16</td>
      <td>39.75</td>
      <td>77.43</td>
      <td>13.34</td>
      <td>17.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>5da5c92f3cf85a62c1be90a0bb2ae8dffce64a7d</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE1_17w-r16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/llama-13b-FINETUNE3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__llama-13b-FINETUNE3</td>
      <td>49.08</td>
      <td>59.30</td>
      <td>81.53</td>
      <td>57.46</td>
      <td>41.63</td>
      <td>76.72</td>
      <td>12.13</td>
      <td>14.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>bacd035db122dafaf86bf52bb9ca8c613070cc58</td>
      <td>wei123602/llama-13b-FINETUNE3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zarablend-1.1-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zarablend-1.1-l2-7b</td>
      <td>49.07</td>
      <td>54.86</td>
      <td>78.58</td>
      <td>47.89</td>
      <td>49.00</td>
      <td>72.61</td>
      <td>4.55</td>
      <td>35.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>e46bfa43829cbea7608192a6d07bcc147387fdb7</td>
      <td>zarakiquemparte/zarablend-1.1-l2-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lu-vae/llama2-13B-sharegpt4-orca-openplatypus-8w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lu-vae__llama2-13B-sharegpt4-orca-openplatypus-8w</td>
      <td>49.05</td>
      <td>62.80</td>
      <td>84.04</td>
      <td>55.13</td>
      <td>45.66</td>
      <td>75.14</td>
      <td>11.75</td>
      <td>8.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ad086aacf0176911133b6cccfb34364afce9de5a</td>
      <td>lu-vae/llama2-13B-sharegpt4-orca-openplatypus-8w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/kuchiki-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__kuchiki-l2-7b</td>
      <td>49.03</td>
      <td>54.35</td>
      <td>78.44</td>
      <td>47.74</td>
      <td>49.88</td>
      <td>73.09</td>
      <td>4.47</td>
      <td>35.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>7.0</td>
      <td>True</td>
      <td>745c34e70aa92056e8cd79c1d16e8fcfe1797645</td>
      <td>zarakiquemparte/kuchiki-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zarablend-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zarablend-l2-7b</td>
      <td>49.03</td>
      <td>54.44</td>
      <td>78.62</td>
      <td>47.61</td>
      <td>49.38</td>
      <td>73.32</td>
      <td>4.40</td>
      <td>35.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>10.0</td>
      <td>True</td>
      <td>8b14e71ae3f52c409a25e1ac98dd05e0bb91eaff</td>
      <td>zarakiquemparte/zarablend-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/haonan-li/bactrian-x-llama-13b-merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_haonan-li__bactrian-x-llama-13b-merged</td>
      <td>49.01</td>
      <td>56.40</td>
      <td>79.33</td>
      <td>48.40</td>
      <td>48.38</td>
      <td>73.95</td>
      <td>5.53</td>
      <td>31.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>cc5ee2231066c147423f89e9df40f7364c3275a5</td>
      <td>haonan-li/bactrian-x-llama-13b-merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/augtoma/qCammel-13</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_augtoma__qCammel-13</td>
      <td>48.98</td>
      <td>60.84</td>
      <td>83.66</td>
      <td>56.73</td>
      <td>47.54</td>
      <td>76.16</td>
      <td>11.37</td>
      <td>6.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>af473e64f6a4fa02a7e24ee7679eea9505eb179d</td>
      <td>augtoma/qCammel-13</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-OpenOrca_5w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-OpenOrca_5w</td>
      <td>48.96</td>
      <td>61.01</td>
      <td>82.82</td>
      <td>56.09</td>
      <td>44.87</td>
      <td>77.74</td>
      <td>12.28</td>
      <td>7.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ddd810c9150492d7318656acac44849651edbf2</td>
      <td>CHIH-HUNG/llama-2-13b-OpenOrca_5w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/airoboros-13B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__airoboros-13B-HF</td>
      <td>48.96</td>
      <td>58.28</td>
      <td>81.05</td>
      <td>50.03</td>
      <td>51.57</td>
      <td>76.24</td>
      <td>7.13</td>
      <td>18.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>12.0</td>
      <td>True</td>
      <td>9219b61a0e8bc880e4cd0f8bebc48a97ee0950c7</td>
      <td>TheBloke/airoboros-13B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b</td>
      <td>48.94</td>
      <td>58.28</td>
      <td>81.05</td>
      <td>50.03</td>
      <td>51.57</td>
      <td>76.24</td>
      <td>6.97</td>
      <td>18.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>93.0</td>
      <td>True</td>
      <td>44830f9e1559f318f5dad875bab40d1d1beddbfc</td>
      <td>jondurbin/airoboros-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Danielbrdz/Barcenas-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Danielbrdz__Barcenas-13b</td>
      <td>48.94</td>
      <td>61.26</td>
      <td>82.13</td>
      <td>56.25</td>
      <td>46.67</td>
      <td>76.32</td>
      <td>12.36</td>
      <td>7.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>fa988ba73f67ad0c8e7fa8f408106ea040070258</td>
      <td>Danielbrdz/Barcenas-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TFLai/Nova-13B-50-step</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Nova-13B-50-step</td>
      <td>48.93</td>
      <td>61.60</td>
      <td>82.31</td>
      <td>57.27</td>
      <td>51.53</td>
      <td>76.56</td>
      <td>4.40</td>
      <td>8.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1a827ccb7f00157b3cc9ce538d61a6ba8d5a65db</td>
      <td>TFLai/Nova-13B-50-step</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama2-13b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama2-13b-chat</td>
      <td>48.91</td>
      <td>57.51</td>
      <td>77.94</td>
      <td>52.56</td>
      <td>48.18</td>
      <td>74.74</td>
      <td>9.86</td>
      <td>21.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.97</td>
      <td>0.0</td>
      <td>True</td>
      <td>9497e3bd12e19e1300bc7b1980fbe232420134b9</td>
      <td>YeungNLP/firefly-llama2-13b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Xwin-LM/Xwin-LM-13B-V0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Xwin-LM__Xwin-LM-13B-V0.1</td>
      <td>48.90</td>
      <td>62.54</td>
      <td>82.80</td>
      <td>56.53</td>
      <td>45.96</td>
      <td>74.27</td>
      <td>9.63</td>
      <td>10.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>57.0</td>
      <td>True</td>
      <td>32938856dc3d713dcba706aded7c82791b6ff647</td>
      <td>Xwin-LM/Xwin-LM-13B-V0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down</td>
      <td>48.87</td>
      <td>54.35</td>
      <td>82.13</td>
      <td>55.33</td>
      <td>39.60</td>
      <td>77.19</td>
      <td>12.89</td>
      <td>20.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>1646a2b77ddeaf0f848c96ed68726556c7539729</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_3.8w-r8-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/Samantha-1.11-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Samantha-1.11-13b</td>
      <td>48.87</td>
      <td>60.84</td>
      <td>82.99</td>
      <td>55.96</td>
      <td>47.72</td>
      <td>76.01</td>
      <td>12.28</td>
      <td>6.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>e355ead3a939f471fe2586201156fb972fad0f4b</td>
      <td>ehartford/Samantha-1.11-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/kuchiki-1.1-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__kuchiki-1.1-l2-7b</td>
      <td>48.82</td>
      <td>54.18</td>
      <td>78.00</td>
      <td>48.14</td>
      <td>49.96</td>
      <td>73.16</td>
      <td>4.70</td>
      <td>33.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>10fe70fec0df5c4dcbdfd2e9ec74830c41b3cfd2</td>
      <td>zarakiquemparte/kuchiki-1.1-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-Inverted-1.2-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-Inverted-1.2-L2-13B</td>
      <td>48.80</td>
      <td>59.39</td>
      <td>83.01</td>
      <td>55.77</td>
      <td>51.22</td>
      <td>74.66</td>
      <td>8.95</td>
      <td>8.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>8d2e9087093eef1c9173e167beb40b9d034a4655</td>
      <td>Sao10K/Stheno-Inverted-1.2-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Mythical-Destroyer-V2-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Mythical-Destroyer-V2-L2-13B</td>
      <td>48.79</td>
      <td>59.30</td>
      <td>82.66</td>
      <td>57.39</td>
      <td>57.09</td>
      <td>74.74</td>
      <td>0.00</td>
      <td>10.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>10.0</td>
      <td>True</td>
      <td>cbc8b2e4a3beafc311b9e61f8fa9f7526a77c360</td>
      <td>Sao10K/Mythical-Destroyer-V2-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-13B-V1-1-SuperHOT-8K-GPTQ</td>
      <td>48.79</td>
      <td>57.00</td>
      <td>80.32</td>
      <td>47.08</td>
      <td>53.46</td>
      <td>74.35</td>
      <td>0.68</td>
      <td>28.62</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>other</td>
      <td>16.22</td>
      <td>44.0</td>
      <td>True</td>
      <td>085eb5cd394f30d72bf5efcf83a580e87264b3e8</td>
      <td>TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/kingbri/chronolima-airo-grad-l2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_kingbri__chronolima-airo-grad-l2-13B</td>
      <td>48.78</td>
      <td>59.56</td>
      <td>83.47</td>
      <td>55.80</td>
      <td>44.58</td>
      <td>75.61</td>
      <td>13.95</td>
      <td>8.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>agpl-3.0</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>9195bd6ea775daf347a275e190665e10bf1fb54b</td>
      <td>kingbri/chronolima-airo-grad-l2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Athena-Platypus2-13B-QLora-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Athena-Platypus2-13B-QLora-0.80-epoch</td>
      <td>48.78</td>
      <td>56.66</td>
      <td>80.56</td>
      <td>55.43</td>
      <td>53.62</td>
      <td>72.61</td>
      <td>0.08</td>
      <td>22.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7b6c11b4df16079dfdd1e8dd8c489a8835c7cc4</td>
      <td>TFLai/Athena-Platypus2-13B-QLora-0.80-epoch</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/teknium/OpenHermes-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_teknium__OpenHermes-7B</td>
      <td>48.76</td>
      <td>56.14</td>
      <td>78.32</td>
      <td>48.62</td>
      <td>45.00</td>
      <td>74.51</td>
      <td>5.00</td>
      <td>33.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>mit</td>
      <td>6.61</td>
      <td>7.0</td>
      <td>True</td>
      <td>74edb1ad58d3d517ef46c4e2a31081084ecbc473</td>
      <td>teknium/OpenHermes-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/llama2-13b-math1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__llama2-13b-math1.2</td>
      <td>48.72</td>
      <td>56.91</td>
      <td>80.71</td>
      <td>53.21</td>
      <td>48.25</td>
      <td>74.74</td>
      <td>11.30</td>
      <td>15.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b05b4c22893e950e8e33acb67087a9acc8f0ab97</td>
      <td>FelixChao/llama2-13b-math1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/kingbri/airolima-chronos-grad-l2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_kingbri__airolima-chronos-grad-l2-13B</td>
      <td>48.69</td>
      <td>59.56</td>
      <td>83.50</td>
      <td>55.78</td>
      <td>44.67</td>
      <td>75.85</td>
      <td>13.65</td>
      <td>7.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>d2ad57b2b50361485b2b04e59a989161599cb08b</td>
      <td>kingbri/airolima-chronos-grad-l2-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lu-vae/llama2-13b-sharegpt4-test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lu-vae__llama2-13b-sharegpt4-test</td>
      <td>48.68</td>
      <td>58.02</td>
      <td>82.65</td>
      <td>55.99</td>
      <td>48.27</td>
      <td>76.09</td>
      <td>13.12</td>
      <td>6.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2be36a2dab4ed0f97727a1508367f53d59950818</td>
      <td>lu-vae/llama2-13b-sharegpt4-test</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/llama2-13b-math1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__llama2-13b-math1.2</td>
      <td>48.65</td>
      <td>57.08</td>
      <td>80.61</td>
      <td>53.05</td>
      <td>48.30</td>
      <td>74.27</td>
      <td>10.99</td>
      <td>16.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b05b4c22893e950e8e33acb67087a9acc8f0ab97</td>
      <td>FelixChao/llama2-13b-math1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v3.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v3.1</td>
      <td>48.65</td>
      <td>60.15</td>
      <td>82.84</td>
      <td>56.84</td>
      <td>44.38</td>
      <td>76.24</td>
      <td>13.80</td>
      <td>6.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>cc708183e430234b8718c08d9f90474569eabeac</td>
      <td>openchat/openchat_v3.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FlagAlpha__Llama2-Chinese-7b-Chat</td>
      <td>48.62</td>
      <td>52.39</td>
      <td>77.52</td>
      <td>47.72</td>
      <td>46.87</td>
      <td>74.27</td>
      <td>8.04</td>
      <td>33.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>104.0</td>
      <td>True</td>
      <td>4c3bc725f71898c6a1acd4ea98a2f8d74d1b1b6b</td>
      <td>FlagAlpha/Llama2-Chinese-7b-Chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v3.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v3.2</td>
      <td>48.62</td>
      <td>59.64</td>
      <td>82.68</td>
      <td>56.68</td>
      <td>44.49</td>
      <td>76.95</td>
      <td>13.65</td>
      <td>6.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>36.0</td>
      <td>True</td>
      <td>65320bf6dbe0cb4682d45a9e55dbc876502f8b66</td>
      <td>openchat/openchat_v3.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hyunseoki/ko-en-llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hyunseoki__ko-en-llama2-13b</td>
      <td>48.61</td>
      <td>58.19</td>
      <td>81.89</td>
      <td>52.02</td>
      <td>39.96</td>
      <td>74.82</td>
      <td>0.76</td>
      <td>32.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>13.0</td>
      <td>True</td>
      <td>2768cf6f955b65868ccbb20658e2cc444b2f3be9</td>
      <td>hyunseoki/ko-en-llama2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/dolphin-llama-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__dolphin-llama-13b</td>
      <td>48.60</td>
      <td>55.55</td>
      <td>77.11</td>
      <td>52.16</td>
      <td>52.23</td>
      <td>69.93</td>
      <td>14.40</td>
      <td>18.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>56.0</td>
      <td>True</td>
      <td>b6d16c3e1cffef5e914863f41fd96152dafddd6f</td>
      <td>ehartford/dolphin-llama-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-dolphin_5w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-dolphin_5w</td>
      <td>48.59</td>
      <td>60.67</td>
      <td>82.69</td>
      <td>56.23</td>
      <td>44.41</td>
      <td>77.35</td>
      <td>11.83</td>
      <td>6.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>0ec406128968b41a9b7a5f18c358f7638d696b56</td>
      <td>CHIH-HUNG/llama-2-13b-dolphin_5w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Expert68/llama2_13b_instructed_version2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Expert68__llama2_13b_instructed_version2</td>
      <td>48.57</td>
      <td>60.07</td>
      <td>84.05</td>
      <td>55.61</td>
      <td>46.12</td>
      <td>75.61</td>
      <td>10.99</td>
      <td>7.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea321257d81e0f41c985f5155297b7fbd6ac375a</td>
      <td>Expert68/llama2_13b_instructed_version2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj</td>
      <td>48.57</td>
      <td>57.17</td>
      <td>82.26</td>
      <td>55.89</td>
      <td>39.93</td>
      <td>76.56</td>
      <td>12.89</td>
      <td>15.27</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c1a5ad1b5e490ed860eeb1b449a02e14da10717f</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_Fintune_1_17w-gate_up_down_proj</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-13B</td>
      <td>48.56</td>
      <td>59.98</td>
      <td>81.86</td>
      <td>56.11</td>
      <td>47.41</td>
      <td>76.09</td>
      <td>10.99</td>
      <td>7.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>fbb23bc41438b016f1df1e9180c6c350a03557ea</td>
      <td>migtissera/Synthia-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/vonjack/Qwen-LLaMAfied-HFTok-7B-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vonjack__Qwen-LLaMAfied-HFTok-7B-Chat</td>
      <td>48.51</td>
      <td>50.51</td>
      <td>83.65</td>
      <td>51.53</td>
      <td>44.23</td>
      <td>71.43</td>
      <td>2.50</td>
      <td>35.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.10</td>
      <td>19.0</td>
      <td>True</td>
      <td>b8d5c09c83b1ef23668cb9209dbc43c0df2de8ae</td>
      <td>vonjack/Qwen-LLaMAfied-HFTok-7B-Chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down</td>
      <td>48.45</td>
      <td>58.53</td>
      <td>82.27</td>
      <td>55.90</td>
      <td>40.26</td>
      <td>76.95</td>
      <td>15.39</td>
      <td>9.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>fdc145fe1b47cdda483535c018e35a5ab249a552</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_addto15k_4.5w-r16-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v3.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v3.2</td>
      <td>48.44</td>
      <td>59.47</td>
      <td>82.60</td>
      <td>56.82</td>
      <td>44.51</td>
      <td>76.09</td>
      <td>13.42</td>
      <td>6.15</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>36.0</td>
      <td>True</td>
      <td>bc771c901529dedbf04864d0b81452f62301f882</td>
      <td>openchat/openchat_v3.2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-c34b-2.2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-c34b-2.2.1</td>
      <td>48.42</td>
      <td>54.69</td>
      <td>76.84</td>
      <td>55.43</td>
      <td>51.36</td>
      <td>72.53</td>
      <td>20.02</td>
      <td>8.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>8.0</td>
      <td>True</td>
      <td>79d9761af231fecbfaf6066d6d405a0f8c04f4ba</td>
      <td>jondurbin/airoboros-c34b-2.2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jphme/em_german_leo_mistral</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jphme__em_german_leo_mistral</td>
      <td>48.40</td>
      <td>52.82</td>
      <td>78.03</td>
      <td>50.03</td>
      <td>50.19</td>
      <td>73.48</td>
      <td>5.61</td>
      <td>28.65</td>
      <td>MistralForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>aa63a32154923034fb89b1408d3d7ffa994d3327</td>
      <td>jphme/em_german_leo_mistral</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/manticore-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__manticore-13b</td>
      <td>48.39</td>
      <td>58.70</td>
      <td>81.63</td>
      <td>50.84</td>
      <td>49.17</td>
      <td>76.64</td>
      <td>12.21</td>
      <td>9.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>108.0</td>
      <td>True</td>
      <td>aed786b0200251c9962ac200c50f7e367f264b46</td>
      <td>openaccess-ai-collective/manticore-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-v4.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-13b-v4.5</td>
      <td>48.39</td>
      <td>60.67</td>
      <td>82.34</td>
      <td>52.32</td>
      <td>50.62</td>
      <td>73.64</td>
      <td>4.62</td>
      <td>14.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>f3be56d8bf71a8d3905974b1e5fcba7336b02159</td>
      <td>The-Face-Of-Goonery/Huginn-13b-v4.5</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-v3-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-v3-13b</td>
      <td>48.39</td>
      <td>60.67</td>
      <td>82.34</td>
      <td>52.32</td>
      <td>50.62</td>
      <td>73.64</td>
      <td>4.62</td>
      <td>14.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>11.0</td>
      <td>True</td>
      <td>6c2faf828c5380d28c51fcb4d3d0f1a420fb9a9a</td>
      <td>The-Face-Of-Goonery/Huginn-v3-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-V4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-13b-V4</td>
      <td>48.39</td>
      <td>60.67</td>
      <td>82.34</td>
      <td>52.32</td>
      <td>50.62</td>
      <td>73.64</td>
      <td>4.62</td>
      <td>14.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>6186feee849e0c2b7e62d4cbdc4cdc48260ac684</td>
      <td>The-Face-Of-Goonery/Huginn-13b-V4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/vicuna-13b-v1.5-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__vicuna-13b-v1.5-PL-lora_unload</td>
      <td>48.38</td>
      <td>56.91</td>
      <td>81.22</td>
      <td>56.06</td>
      <td>49.76</td>
      <td>75.22</td>
      <td>12.28</td>
      <td>7.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>5c8aeb722e11d1c7258abd45f9f2840f57976c28</td>
      <td>Aspik101/vicuna-13b-v1.5-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/llama2-13b-math1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__llama2-13b-math1.1</td>
      <td>48.36</td>
      <td>56.83</td>
      <td>80.69</td>
      <td>53.43</td>
      <td>48.48</td>
      <td>74.74</td>
      <td>10.69</td>
      <td>13.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>3c4d83d3525e54a493ff510443fdcca44bf63b59</td>
      <td>FelixChao/llama2-13b-math1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/SthenoWriter-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__SthenoWriter-L2-13B</td>
      <td>48.35</td>
      <td>62.29</td>
      <td>83.28</td>
      <td>56.14</td>
      <td>44.72</td>
      <td>74.35</td>
      <td>11.22</td>
      <td>6.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6d9e26ab765eb170cc0aa428ee5e25b08524657</td>
      <td>Sao10K/SthenoWriter-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-30b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-30b-chat</td>
      <td>48.34</td>
      <td>58.70</td>
      <td>82.54</td>
      <td>51.16</td>
      <td>52.42</td>
      <td>75.30</td>
      <td>12.13</td>
      <td>6.16</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>29.96</td>
      <td>183.0</td>
      <td>True</td>
      <td>54f33278a04aa4e612bca482b82f801ab658e890</td>
      <td>mosaicml/mpt-30b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/llama2-13b-math1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__llama2-13b-math1.1</td>
      <td>48.34</td>
      <td>57.25</td>
      <td>80.74</td>
      <td>53.56</td>
      <td>48.43</td>
      <td>74.43</td>
      <td>10.69</td>
      <td>13.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>3c4d83d3525e54a493ff510443fdcca44bf63b59</td>
      <td>FelixChao/llama2-13b-math1.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/llama-2-13b-Guanaco-QLoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__llama-2-13b-Guanaco-QLoRA</td>
      <td>48.33</td>
      <td>61.09</td>
      <td>82.99</td>
      <td>55.47</td>
      <td>44.12</td>
      <td>77.19</td>
      <td>10.99</td>
      <td>6.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>67e68284234538d3851d5c0c334383daffec57a2</td>
      <td>yeontaek/llama-2-13b-Guanaco-QLoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elinas/chronos-13b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elinas__chronos-13b-v2</td>
      <td>48.32</td>
      <td>58.70</td>
      <td>82.52</td>
      <td>53.39</td>
      <td>50.55</td>
      <td>75.06</td>
      <td>11.30</td>
      <td>6.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>17.0</td>
      <td>True</td>
      <td>e5d411138e72370c5613dfea0f66ded99f6e62f9</td>
      <td>elinas/chronos-13b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/mcq-hal-vicuna-13b-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__mcq-hal-vicuna-13b-v1.5</td>
      <td>48.31</td>
      <td>55.97</td>
      <td>80.72</td>
      <td>52.85</td>
      <td>45.03</td>
      <td>72.77</td>
      <td>8.87</td>
      <td>21.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>bb3029bce8347b09c2fd6908475b195bcabe53e3</td>
      <td>luffycodes/mcq-hal-vicuna-13b-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/teknium/OpenHermes-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_teknium__OpenHermes-13B</td>
      <td>48.30</td>
      <td>59.81</td>
      <td>82.24</td>
      <td>56.35</td>
      <td>46.01</td>
      <td>75.45</td>
      <td>11.60</td>
      <td>6.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>28.0</td>
      <td>True</td>
      <td>f09d0fe655ad57cce9179b7b40ea6f81e07db18c</td>
      <td>teknium/OpenHermes-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE2_TEST_2.2w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE2_TEST_2.2w</td>
      <td>48.28</td>
      <td>56.23</td>
      <td>82.70</td>
      <td>55.35</td>
      <td>39.55</td>
      <td>76.72</td>
      <td>8.64</td>
      <td>18.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>3be177b35f1b44d147751ab38ca6d8a008eb6b7f</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE2_TEST_2.2w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/Samantha-1.11-CodeLlama-34b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Samantha-1.11-CodeLlama-34b</td>
      <td>48.26</td>
      <td>56.57</td>
      <td>75.47</td>
      <td>53.51</td>
      <td>50.46</td>
      <td>73.48</td>
      <td>19.33</td>
      <td>8.97</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>41.0</td>
      <td>True</td>
      <td>3fd110de9282e52f56f999bf1da1a76425f00e29</td>
      <td>ehartford/Samantha-1.11-CodeLlama-34b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/shareAI/llama2-13b-Chinese-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shareAI__llama2-13b-Chinese-chat</td>
      <td>48.23</td>
      <td>60.58</td>
      <td>82.19</td>
      <td>55.45</td>
      <td>45.11</td>
      <td>76.64</td>
      <td>11.37</td>
      <td>6.24</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.00</td>
      <td>37.0</td>
      <td>False</td>
      <td>31103acf93479d5c3865fb9b51dcb38e10d8b801</td>
      <td>shareAI/llama2-13b-Chinese-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ausboss/llama-13b-supercot</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ausboss__llama-13b-supercot</td>
      <td>48.22</td>
      <td>56.06</td>
      <td>81.71</td>
      <td>45.36</td>
      <td>48.55</td>
      <td>75.77</td>
      <td>7.20</td>
      <td>22.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>8.0</td>
      <td>True</td>
      <td>f6953fa162b487a3d4c6bdc7b7951e09576c2ae5</td>
      <td>ausboss/llama-13b-supercot</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gaodrew/OpenOrca-Platypus2-13B-thera-1250</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gaodrew__OpenOrca-Platypus2-13B-thera-1250</td>
      <td>48.22</td>
      <td>59.22</td>
      <td>81.02</td>
      <td>57.04</td>
      <td>48.43</td>
      <td>73.09</td>
      <td>8.57</td>
      <td>10.17</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1c2ebcda387211732e87911e39edca503502a33</td>
      <td>gaodrew/OpenOrca-Platypus2-13B-thera-1250</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/beaugogh/Llama2-13b-sharegpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beaugogh__Llama2-13b-sharegpt4</td>
      <td>48.19</td>
      <td>61.77</td>
      <td>84.53</td>
      <td>55.21</td>
      <td>45.94</td>
      <td>75.22</td>
      <td>8.79</td>
      <td>5.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>294c40349bf0c5377f71d92e7539bf5de3176a74</td>
      <td>beaugogh/Llama2-13b-sharegpt4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b-gpt4-1.2</td>
      <td>48.19</td>
      <td>58.36</td>
      <td>81.61</td>
      <td>48.84</td>
      <td>47.54</td>
      <td>73.64</td>
      <td>3.87</td>
      <td>23.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>482bd38b65e73fde13f5d03fed2bee7acda8fadd</td>
      <td>jondurbin/airoboros-13b-gpt4-1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/circulus/Llama-2-7b-orca-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_circulus__Llama-2-7b-orca-v1</td>
      <td>48.17</td>
      <td>56.31</td>
      <td>79.14</td>
      <td>52.71</td>
      <td>50.19</td>
      <td>75.22</td>
      <td>7.81</td>
      <td>15.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>6.61</td>
      <td>5.0</td>
      <td>True</td>
      <td>e501f231277671710384ba0397da2c4486865958</td>
      <td>circulus/Llama-2-7b-orca-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/stabilityai/StableBeluga-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__StableBeluga-7B</td>
      <td>48.17</td>
      <td>56.31</td>
      <td>79.14</td>
      <td>52.71</td>
      <td>50.19</td>
      <td>75.22</td>
      <td>7.81</td>
      <td>15.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>112.0</td>
      <td>True</td>
      <td>329adcfc39f48dce183eb0b155b732dbe03c6304</td>
      <td>stabilityai/StableBeluga-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj</td>
      <td>48.14</td>
      <td>57.42</td>
      <td>82.42</td>
      <td>55.57</td>
      <td>39.19</td>
      <td>77.03</td>
      <td>12.05</td>
      <td>13.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>469c6674ad2190b639d6f5ce6bfecc1463825dfb</td>
      <td>CHIH-HUNG/llama-2-13b-huangyt_FINETUNE2_3w-gate_up_down_proj</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CalderaAI/13B-Legerdemain-L2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CalderaAI__13B-Legerdemain-L2</td>
      <td>48.14</td>
      <td>61.26</td>
      <td>83.26</td>
      <td>56.00</td>
      <td>41.99</td>
      <td>75.22</td>
      <td>13.04</td>
      <td>6.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>d6624ce1bcc6b50c86b86e879a8c9822218b84d2</td>
      <td>CalderaAI/13B-Legerdemain-L2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jphme/Llama-2-13b-chat-german</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jphme__Llama-2-13b-chat-german</td>
      <td>48.14</td>
      <td>57.85</td>
      <td>81.66</td>
      <td>54.45</td>
      <td>46.32</td>
      <td>76.48</td>
      <td>13.65</td>
      <td>6.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>46.0</td>
      <td>True</td>
      <td>d72667bd92fd6f76835466d302563d213e0b1ee1</td>
      <td>jphme/Llama-2-13b-chat-german</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/pygmalion-2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__pygmalion-2-13b</td>
      <td>48.13</td>
      <td>60.32</td>
      <td>82.37</td>
      <td>56.02</td>
      <td>42.22</td>
      <td>78.06</td>
      <td>11.75</td>
      <td>6.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>29.0</td>
      <td>True</td>
      <td>3cdc103995ccd5fc7fd2cb5f51f71b510466f5fc</td>
      <td>PygmalionAI/pygmalion-2-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16</td>
      <td>48.08</td>
      <td>60.41</td>
      <td>82.58</td>
      <td>55.86</td>
      <td>43.61</td>
      <td>76.72</td>
      <td>8.49</td>
      <td>8.92</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>891be2d8f205baa04c8a92f6ab1225f0d0c3e5bd</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wei123602/FINETUNE3_TEST4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wei123602__FINETUNE3_TEST4</td>
      <td>48.08</td>
      <td>55.63</td>
      <td>81.31</td>
      <td>52.13</td>
      <td>41.14</td>
      <td>76.72</td>
      <td>11.22</td>
      <td>18.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>5195e87bb34317c5aaf201faa476aae78ecc9f1b</td>
      <td>wei123602/FINETUNE3_TEST4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DopeorNope/LaOT</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DopeorNope__LaOT</td>
      <td>48.07</td>
      <td>55.63</td>
      <td>78.96</td>
      <td>50.30</td>
      <td>44.72</td>
      <td>74.11</td>
      <td>0.00</td>
      <td>32.75</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>df3a2c77a63a370405c7711b323e7ffa550cdd9e</td>
      <td>DopeorNope/LaOT</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r8-gate_up_down</td>
      <td>48.07</td>
      <td>57.17</td>
      <td>82.15</td>
      <td>54.88</td>
      <td>40.23</td>
      <td>76.32</td>
      <td>13.34</td>
      <td>12.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>86adab5c098c9338e098a8e5b0188b0aa39b2478</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Voicelab/trurl-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Voicelab__trurl-2-7b</td>
      <td>48.05</td>
      <td>53.41</td>
      <td>75.29</td>
      <td>50.00</td>
      <td>45.42</td>
      <td>72.22</td>
      <td>7.13</td>
      <td>32.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>e26ca5f157c60fc527170cc04db7fc0ea04ad26f</td>
      <td>Voicelab/trurl-2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/UltraLM-13B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__UltraLM-13B-fp16</td>
      <td>48.05</td>
      <td>57.59</td>
      <td>80.20</td>
      <td>51.85</td>
      <td>51.56</td>
      <td>75.85</td>
      <td>10.69</td>
      <td>8.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>734f5641f6c548474517d1536c46024517f120e0</td>
      <td>TheBloke/UltraLM-13B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Platypus2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Platypus2-13B</td>
      <td>48.04</td>
      <td>61.26</td>
      <td>82.56</td>
      <td>56.70</td>
      <td>44.86</td>
      <td>76.87</td>
      <td>7.05</td>
      <td>6.95</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>12.85</td>
      <td>15.0</td>
      <td>True</td>
      <td>b5e926e3d6c03e83c7983e87eb71098b5e80a62e</td>
      <td>garage-bAInd/Platypus2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lajonbot/tableBeluga-7B-instruct-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lajonbot__tableBeluga-7B-instruct-pl-lora_unload</td>
      <td>48.02</td>
      <td>56.23</td>
      <td>79.12</td>
      <td>52.70</td>
      <td>50.19</td>
      <td>75.22</td>
      <td>7.81</td>
      <td>14.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>eeb22ca9481a5ed7e131a329324494f234300a45</td>
      <td>Lajonbot/tableBeluga-7B-instruct-pl-lora_unload</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2-13B-LoRa</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2-13B-LoRa</td>
      <td>48.01</td>
      <td>60.67</td>
      <td>82.50</td>
      <td>56.34</td>
      <td>43.91</td>
      <td>75.93</td>
      <td>7.51</td>
      <td>9.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>1450c541cf9e378e81862fabeb234b8e0a2bdf5a</td>
      <td>yeontaek/Platypus2-13B-LoRa</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/pankajmathur/orca_mini_v3_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pankajmathur__orca_mini_v3_7b</td>
      <td>47.98</td>
      <td>56.91</td>
      <td>79.64</td>
      <td>52.37</td>
      <td>50.51</td>
      <td>74.27</td>
      <td>7.13</td>
      <td>15.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>37.0</td>
      <td>True</td>
      <td>f9849ea6bf0f6ebb78dca1cea1c7a3ef8f7d715c</td>
      <td>pankajmathur/orca_mini_v3_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_v3_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_v3_7b</td>
      <td>47.98</td>
      <td>56.91</td>
      <td>79.64</td>
      <td>52.37</td>
      <td>50.51</td>
      <td>74.27</td>
      <td>7.13</td>
      <td>15.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>37.0</td>
      <td>True</td>
      <td>a1583d2f02041fb37df28eeae4da644d8dff33eb</td>
      <td>psmathur/orca_mini_v3_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Kimiko-v2-13B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Kimiko-v2-13B-fp16</td>
      <td>47.98</td>
      <td>61.01</td>
      <td>83.32</td>
      <td>55.17</td>
      <td>40.65</td>
      <td>76.80</td>
      <td>12.51</td>
      <td>6.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>0fed305667508e50330e71a2d43e9cee5ea73783</td>
      <td>TheBloke/Kimiko-v2-13B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v3.2_super</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v3.2_super</td>
      <td>47.97</td>
      <td>59.81</td>
      <td>82.50</td>
      <td>55.90</td>
      <td>42.30</td>
      <td>75.93</td>
      <td>13.50</td>
      <td>5.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>24.0</td>
      <td>True</td>
      <td>aab7ce4d48b31a295a0116b61569d8e87a09bb7a</td>
      <td>openchat/openchat_v3.2_super</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/gywy/llama2-13b-chinese-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gywy__llama2-13b-chinese-v2</td>
      <td>47.97</td>
      <td>53.92</td>
      <td>74.64</td>
      <td>49.74</td>
      <td>45.43</td>
      <td>71.59</td>
      <td>2.20</td>
      <td>38.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.94</td>
      <td>2.0</td>
      <td>True</td>
      <td>8f6b11ca4344ac230d6b55defa4e04e60a39f9b5</td>
      <td>gywy/llama2-13b-chinese-v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Aspik101/Redmond-Puffin-13B-instruct-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__Redmond-Puffin-13B-instruct-PL-lora_unload</td>
      <td>47.96</td>
      <td>60.92</td>
      <td>82.43</td>
      <td>55.61</td>
      <td>44.26</td>
      <td>75.69</td>
      <td>11.07</td>
      <td>5.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b933009635299bca32c694336aa2007d756a2dda</td>
      <td>Aspik101/Redmond-Puffin-13B-instruct-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikael110/llama-2-13b-guanaco-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikael110__llama-2-13b-guanaco-fp16</td>
      <td>47.96</td>
      <td>60.92</td>
      <td>83.18</td>
      <td>54.58</td>
      <td>44.00</td>
      <td>74.90</td>
      <td>11.60</td>
      <td>6.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>11.0</td>
      <td>True</td>
      <td>feb7ef47ceca6aec9548264a39622b63fdcb853c</td>
      <td>Mikael110/llama-2-13b-guanaco-fp16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch</td>
      <td>47.95</td>
      <td>57.34</td>
      <td>81.24</td>
      <td>55.64</td>
      <td>55.98</td>
      <td>73.88</td>
      <td>0.00</td>
      <td>11.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>ada55b32fe8ed55b7691d997ad2e86f232c91aad</td>
      <td>TFLai/MythicalDestroyerV2-Platypus2-13B-QLora-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/LewdEngine</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__LewdEngine</td>
      <td>47.92</td>
      <td>60.49</td>
      <td>83.08</td>
      <td>54.84</td>
      <td>43.63</td>
      <td>74.90</td>
      <td>12.36</td>
      <td>6.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>6e918ff9f563552af4ad66f4308f6d040e24af4b</td>
      <td>Undi95/LewdEngine</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hiyouga/Baichuan2-7B-Chat-LLaMAfied</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hiyouga__Baichuan2-7B-Chat-LLaMAfied</td>
      <td>47.92</td>
      <td>52.47</td>
      <td>74.04</td>
      <td>53.88</td>
      <td>48.04</td>
      <td>69.14</td>
      <td>10.92</td>
      <td>26.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.99</td>
      <td>3.0</td>
      <td>True</td>
      <td>da2cd76e2d61bf0247bd67a4f2835319c54a7d62</td>
      <td>hiyouga/Baichuan2-7B-Chat-LLaMAfied</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/zarakiquemparte/zaraxls-l2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_zarakiquemparte__zaraxls-l2-7b</td>
      <td>47.89</td>
      <td>54.44</td>
      <td>78.94</td>
      <td>50.39</td>
      <td>46.51</td>
      <td>73.16</td>
      <td>0.23</td>
      <td>31.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>cc1dad50689b3ebcc1c9c67f275da6b4bb63e2ce</td>
      <td>zarakiquemparte/zaraxls-l2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/tulu-7B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__tulu-7B-fp16</td>
      <td>47.89</td>
      <td>50.17</td>
      <td>77.04</td>
      <td>47.63</td>
      <td>41.61</td>
      <td>73.80</td>
      <td>11.22</td>
      <td>33.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>8a026683f79119643f4007da4e9155c7849792cc</td>
      <td>TheBloke/tulu-7B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Redmond-Puffin-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Redmond-Puffin-13B</td>
      <td>47.88</td>
      <td>60.41</td>
      <td>83.20</td>
      <td>55.36</td>
      <td>42.12</td>
      <td>76.64</td>
      <td>11.45</td>
      <td>5.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>[mit]</td>
      <td>12.85</td>
      <td>99.0</td>
      <td>True</td>
      <td>12af25fa7ea02c4fc636952ea8b9dc9cf48e35be</td>
      <td>NousResearch/Redmond-Puffin-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bhenrym14__airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
      <td>47.87</td>
      <td>60.58</td>
      <td>82.97</td>
      <td>52.10</td>
      <td>46.10</td>
      <td>73.64</td>
      <td>8.11</td>
      <td>11.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.53</td>
      <td>4.0</td>
      <td>True</td>
      <td>24ebae726954e4c1f24a8b2cbe0ca863012a7338</td>
      <td>bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b-gpt4-1.4-fp16</td>
      <td>47.86</td>
      <td>59.64</td>
      <td>83.22</td>
      <td>47.56</td>
      <td>48.82</td>
      <td>76.24</td>
      <td>7.73</td>
      <td>11.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>037e369be06a8a0eef87f2cddfd3469670483f29</td>
      <td>jondurbin/airoboros-13b-gpt4-1.4-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b-gpt4-1.4</td>
      <td>47.86</td>
      <td>59.64</td>
      <td>83.22</td>
      <td>47.56</td>
      <td>48.82</td>
      <td>76.24</td>
      <td>7.73</td>
      <td>11.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>18.0</td>
      <td>True</td>
      <td>d0d2687ed2b4a63a644ed6c5b3f6401844718659</td>
      <td>jondurbin/airoboros-13b-gpt4-1.4</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>47.85</td>
      <td>54.52</td>
      <td>79.36</td>
      <td>55.15</td>
      <td>54.32</td>
      <td>71.11</td>
      <td>0.00</td>
      <td>20.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>4b5aabc51907e4cba49f373c6dc09a2634f2fb8a</td>
      <td>TFLai/PuddleJumper-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/gaodrew/gaodrew-gorgonzola-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_gaodrew__gaodrew-gorgonzola-13b</td>
      <td>47.84</td>
      <td>50.94</td>
      <td>77.65</td>
      <td>68.93</td>
      <td>40.63</td>
      <td>75.45</td>
      <td>8.57</td>
      <td>12.74</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a53fbe358d4cb546916847d861ccfaf7c724a103</td>
      <td>gaodrew/gaodrew-gorgonzola-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Secbone/llama-2-13B-instructed</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Secbone__llama-2-13B-instructed</td>
      <td>47.84</td>
      <td>59.39</td>
      <td>83.88</td>
      <td>55.57</td>
      <td>46.89</td>
      <td>74.03</td>
      <td>8.04</td>
      <td>7.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e676fbd9015beacfba5d71426beace7605200477</td>
      <td>Secbone/llama-2-13B-instructed</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/llama-anon/instruct-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llama-anon__instruct-13b</td>
      <td>47.83</td>
      <td>56.14</td>
      <td>80.27</td>
      <td>47.89</td>
      <td>36.97</td>
      <td>73.56</td>
      <td>2.27</td>
      <td>37.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>agpl-3.0</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>142e198df473fd0cd4370b0d50be5f57e1da399b</td>
      <td>llama-anon/instruct-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-7b-2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-7b-2.1</td>
      <td>47.82</td>
      <td>54.44</td>
      <td>78.68</td>
      <td>44.45</td>
      <td>43.95</td>
      <td>74.11</td>
      <td>2.20</td>
      <td>36.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>5.0</td>
      <td>True</td>
      <td>699491e2e73cc2936205db143f59c1a686b88f14</td>
      <td>jondurbin/airoboros-l2-7b-2.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</td>
      <td>47.80</td>
      <td>59.13</td>
      <td>82.13</td>
      <td>54.98</td>
      <td>44.23</td>
      <td>76.40</td>
      <td>8.11</td>
      <td>9.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>aad13bce3b243721e52e9cda479f1102dda99f12</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airocoder-34b-2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airocoder-34b-2.1</td>
      <td>47.78</td>
      <td>54.18</td>
      <td>73.84</td>
      <td>50.67</td>
      <td>40.70</td>
      <td>69.93</td>
      <td>8.34</td>
      <td>36.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>4.0</td>
      <td>True</td>
      <td>f66e783ac783837b3f59f274ecf55f18a9221cd0</td>
      <td>jondurbin/airocoder-34b-2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Redmond-Puffin-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Redmond-Puffin-13B</td>
      <td>47.78</td>
      <td>60.49</td>
      <td>83.21</td>
      <td>54.95</td>
      <td>42.08</td>
      <td>76.48</td>
      <td>11.22</td>
      <td>6.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>[mit]</td>
      <td>12.85</td>
      <td>99.0</td>
      <td>True</td>
      <td>12af25fa7ea02c4fc636952ea8b9dc9cf48e35be</td>
      <td>NousResearch/Redmond-Puffin-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down</td>
      <td>47.76</td>
      <td>55.72</td>
      <td>81.55</td>
      <td>53.90</td>
      <td>41.89</td>
      <td>77.19</td>
      <td>11.90</td>
      <td>12.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>eb934db4644738a74143b381445213979c8858ed</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE5_4w-r8-q_k_v_o_gate_up_down</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Limarp-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Limarp-Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>47.74</td>
      <td>60.49</td>
      <td>82.76</td>
      <td>56.52</td>
      <td>44.14</td>
      <td>76.80</td>
      <td>6.07</td>
      <td>7.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>0a8560232ff73ca3c3f8e217b4517fa6c4f55558</td>
      <td>TFLai/Limarp-Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Tap-M/Luna-AI-Llama2-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Tap-M__Luna-AI-Llama2-Uncensored</td>
      <td>47.73</td>
      <td>54.35</td>
      <td>78.60</td>
      <td>46.70</td>
      <td>45.50</td>
      <td>72.77</td>
      <td>9.86</td>
      <td>26.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>6.61</td>
      <td>101.0</td>
      <td>True</td>
      <td>6b5e1067e412cc5750aec7415a065671df3618be</td>
      <td>Tap-M/Luna-AI-Llama2-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/anhnv125/llama-op-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_anhnv125__llama-op-v4</td>
      <td>47.71</td>
      <td>61.52</td>
      <td>79.21</td>
      <td>57.01</td>
      <td>42.72</td>
      <td>75.93</td>
      <td>9.63</td>
      <td>7.93</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>False</td>
      <td>6cd644049de2b944beaefcc6aa34965c00e08529</td>
      <td>anhnv125/llama-op-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__manticore-13b-chat-pyg</td>
      <td>47.71</td>
      <td>58.53</td>
      <td>81.96</td>
      <td>48.76</td>
      <td>48.76</td>
      <td>77.19</td>
      <td>9.55</td>
      <td>9.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>24.0</td>
      <td>True</td>
      <td>f9ef65a3cf50e3c09ccb443f99225148e08517aa</td>
      <td>openaccess-ai-collective/manticore-13b-chat-pyg</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-13b-v1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-13b-v1.3</td>
      <td>47.69</td>
      <td>54.61</td>
      <td>80.41</td>
      <td>52.88</td>
      <td>52.14</td>
      <td>74.82</td>
      <td>10.77</td>
      <td>8.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>160.0</td>
      <td>True</td>
      <td>7900eeb715a49affee9e6390f824e62eea3f3fb1</td>
      <td>lmsys/vicuna-13b-v1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/chansung/gpt4-alpaca-lora-13b-decapoda-1024</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chansung__gpt4-alpaca-lora-13b-decapoda-1024</td>
      <td>47.68</td>
      <td>59.39</td>
      <td>81.87</td>
      <td>47.75</td>
      <td>52.59</td>
      <td>77.35</td>
      <td>8.11</td>
      <td>6.74</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>13.00</td>
      <td>3.0</td>
      <td>False</td>
      <td>7aedafea409de07a997d70a84e30242c7b86877c</td>
      <td>chansung/gpt4-alpaca-lora-13b-decapoda-1024</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KoboldAI/LLaMA2-13B-Holomax</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__LLaMA2-13B-Holomax</td>
      <td>47.60</td>
      <td>60.49</td>
      <td>82.86</td>
      <td>54.67</td>
      <td>42.97</td>
      <td>74.66</td>
      <td>11.45</td>
      <td>6.07</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>13.02</td>
      <td>12.0</td>
      <td>True</td>
      <td>2c4fddeb097636d6462b7628a8e053ad3ff4678c</td>
      <td>KoboldAI/LLaMA2-13B-Holomax</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Envoid/Yousei-22B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Envoid__Yousei-22B</td>
      <td>47.59</td>
      <td>55.89</td>
      <td>78.55</td>
      <td>52.31</td>
      <td>50.68</td>
      <td>71.51</td>
      <td>0.45</td>
      <td>23.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>21.83</td>
      <td>1.0</td>
      <td>True</td>
      <td>ae8f93963266d31000433f1a52d43435e1473e2b</td>
      <td>Envoid/Yousei-22B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-ziya-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-ziya-13b</td>
      <td>47.58</td>
      <td>55.38</td>
      <td>78.47</td>
      <td>45.18</td>
      <td>49.29</td>
      <td>74.82</td>
      <td>6.90</td>
      <td>23.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.89</td>
      <td>7.0</td>
      <td>True</td>
      <td>9a21051ae490d2f8ab8b1181c1b45e0412d71a90</td>
      <td>YeungNLP/firefly-ziya-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__Nous-Hermes-llama-2-7b</td>
      <td>47.52</td>
      <td>55.12</td>
      <td>78.94</td>
      <td>48.34</td>
      <td>49.01</td>
      <td>74.03</td>
      <td>5.76</td>
      <td>21.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>[mit]</td>
      <td>6.74</td>
      <td>48.0</td>
      <td>True</td>
      <td>60e58acecdc1552e1b1752a38d1d91d942d1c3f0</td>
      <td>NousResearch/Nous-Hermes-llama-2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NekoPunchBBB/Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NekoPunchBBB__Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu</td>
      <td>47.51</td>
      <td>57.51</td>
      <td>82.49</td>
      <td>54.83</td>
      <td>43.81</td>
      <td>77.27</td>
      <td>10.46</td>
      <td>6.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f65029ea8f030731ace568e40bab33a7097a13de</td>
      <td>NekoPunchBBB/Llama-2-13b-hf_Open-Platypus-QLoRA-multigpu</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-Open-Platypus_2.5w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-Open-Platypus_2.5w</td>
      <td>47.51</td>
      <td>59.56</td>
      <td>82.46</td>
      <td>56.06</td>
      <td>42.45</td>
      <td>76.80</td>
      <td>8.57</td>
      <td>6.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>bc55678af8226e1323305f743a4882da31994e0c</td>
      <td>CHIH-HUNG/llama-2-13b-Open-Platypus_2.5w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-7B-v1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-7B-v1.2</td>
      <td>47.50</td>
      <td>54.35</td>
      <td>79.29</td>
      <td>49.33</td>
      <td>48.92</td>
      <td>73.56</td>
      <td>10.84</td>
      <td>16.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>10.0</td>
      <td>True</td>
      <td>85ea4f4818478084eedd01e958ac5cc7cf64b3bb</td>
      <td>migtissera/Synthia-7B-v1.2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TFLai/Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__Platypus2-13B-QLoRA-0.80-epoch</td>
      <td>47.48</td>
      <td>57.76</td>
      <td>81.63</td>
      <td>55.63</td>
      <td>39.70</td>
      <td>75.93</td>
      <td>2.96</td>
      <td>18.76</td>
      <td>?</td>
      <td>4bit</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>114eb8efd2de1c9eae85d92de490b95c854dfae9</td>
      <td>TFLai/Platypus2-13B-QLoRA-0.80-epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/itsliupeng/llama2_7b_zh</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_itsliupeng__llama2_7b_zh</td>
      <td>47.48</td>
      <td>52.05</td>
      <td>74.88</td>
      <td>60.69</td>
      <td>42.86</td>
      <td>71.74</td>
      <td>6.44</td>
      <td>23.67</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>mit</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>410711781d2e24226c0d62959e4990d1de851c3c</td>
      <td>itsliupeng/llama2_7b_zh</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jjaaaww/posi_13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jjaaaww__posi_13b</td>
      <td>47.47</td>
      <td>59.64</td>
      <td>82.52</td>
      <td>56.56</td>
      <td>42.14</td>
      <td>76.24</td>
      <td>1.59</td>
      <td>13.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ff4eeb0f876c41553c302020041a0e78a15f9aa7</td>
      <td>jjaaaww/posi_13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/StudentLLM/Alpagasus-2-13b-QLoRA-merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_StudentLLM__Alpagasus-2-13b-QLoRA-merged</td>
      <td>47.45</td>
      <td>61.09</td>
      <td>82.46</td>
      <td>55.27</td>
      <td>38.53</td>
      <td>77.35</td>
      <td>11.14</td>
      <td>6.27</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>other</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>dacbafa40716a2d87e593240cc5c1dc883b5066a</td>
      <td>StudentLLM/Alpagasus-2-13b-QLoRA-merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/oasst-llama-13b-2-epochs</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-llama-13b-2-epochs</td>
      <td>47.44</td>
      <td>57.94</td>
      <td>82.40</td>
      <td>48.56</td>
      <td>47.27</td>
      <td>76.87</td>
      <td>8.26</td>
      <td>10.81</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>8.0</td>
      <td>True</td>
      <td>0e3796192f7edf43968541b9454ea35da4a2b1c5</td>
      <td>dvruette/oasst-llama-13b-2-epochs</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/formulae/Dorflan</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_formulae__Dorflan</td>
      <td>47.44</td>
      <td>54.44</td>
      <td>75.78</td>
      <td>51.36</td>
      <td>51.17</td>
      <td>72.61</td>
      <td>0.38</td>
      <td>26.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>5d8e7e5764ace89e6ccd1deece33b0e8a4b4587b</td>
      <td>formulae/Dorflan</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openaccess-ai-collective/wizard-mega-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openaccess-ai-collective__wizard-mega-13b</td>
      <td>47.44</td>
      <td>57.34</td>
      <td>81.09</td>
      <td>50.59</td>
      <td>50.22</td>
      <td>76.32</td>
      <td>10.08</td>
      <td>6.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>103.0</td>
      <td>True</td>
      <td>76e90314541be6cfa2b55208831c99f1351c1a33</td>
      <td>openaccess-ai-collective/wizard-mega-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b-gpt4-1.1</td>
      <td>47.44</td>
      <td>59.04</td>
      <td>83.05</td>
      <td>49.41</td>
      <td>46.62</td>
      <td>75.77</td>
      <td>8.19</td>
      <td>9.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>19c7060adcb34d42e742fe51dd36b8657ac069b7</td>
      <td>jondurbin/airoboros-13b-gpt4-1.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TaylorAI/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TaylorAI__FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model</td>
      <td>47.43</td>
      <td>52.47</td>
      <td>79.08</td>
      <td>47.58</td>
      <td>37.14</td>
      <td>74.74</td>
      <td>6.82</td>
      <td>34.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>819f3f384e37f8906a62a8048556c9e58e495c02</td>
      <td>TaylorAI/FLAN-Llama-7B-2_Llama2-7B-Flash_868_full_model</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lajonbot/Llama-2-13b-hf-instruct-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lajonbot__Llama-2-13b-hf-instruct-pl-lora_unload</td>
      <td>47.41</td>
      <td>59.47</td>
      <td>82.16</td>
      <td>54.83</td>
      <td>41.45</td>
      <td>76.24</td>
      <td>11.90</td>
      <td>5.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>4ef2c736641c2983996c4662bf481782a9de5055</td>
      <td>Lajonbot/Llama-2-13b-hf-instruct-pl-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/layoric/llama-2-13b-code-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_layoric__llama-2-13b-code-alpaca</td>
      <td>47.41</td>
      <td>60.84</td>
      <td>82.14</td>
      <td>55.93</td>
      <td>38.27</td>
      <td>76.40</td>
      <td>11.90</td>
      <td>6.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>aa1d543fe3391fe9f0e6143ef785fffe9c871225</td>
      <td>layoric/llama-2-13b-code-alpaca</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16</td>
      <td>47.39</td>
      <td>59.98</td>
      <td>82.43</td>
      <td>55.41</td>
      <td>39.90</td>
      <td>76.56</td>
      <td>10.54</td>
      <td>6.89</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6a0a2b6672c7b36c714a66c4a836e0b50c6cb5e6</td>
      <td>dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/Nous-Hermes-13b-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__Nous-Hermes-13b-pl-lora_unload</td>
      <td>47.36</td>
      <td>57.08</td>
      <td>81.49</td>
      <td>49.17</td>
      <td>48.30</td>
      <td>76.40</td>
      <td>9.25</td>
      <td>9.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d0ef3991a11c4dc2ea2f832d4082c89c3c5e810c</td>
      <td>Aspik101/Nous-Hermes-13b-pl-lora_unload</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/StudentLLM/Alpagasus-2-13b-QLoRA-merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_StudentLLM__Alpagasus-2-13b-QLoRA-merged</td>
      <td>47.35</td>
      <td>60.84</td>
      <td>82.43</td>
      <td>55.55</td>
      <td>38.65</td>
      <td>76.87</td>
      <td>10.84</td>
      <td>6.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>e324e828c8d68aa8510f50dfab133388a44fd821</td>
      <td>StudentLLM/Alpagasus-2-13b-QLoRA-merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FPHam/Free_Sydney_13b_HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FPHam__Free_Sydney_13b_HF</td>
      <td>47.35</td>
      <td>59.39</td>
      <td>81.40</td>
      <td>53.73</td>
      <td>45.63</td>
      <td>76.01</td>
      <td>9.17</td>
      <td>6.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>5474ecbccd1f2a2cda9f77a157993f55c97377ed</td>
      <td>FPHam/Free_Sydney_13b_HF</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/llama-2-13b-hf-platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__llama-2-13b-hf-platypus</td>
      <td>47.33</td>
      <td>58.87</td>
      <td>82.14</td>
      <td>54.98</td>
      <td>42.84</td>
      <td>77.11</td>
      <td>9.40</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>39e07f6213a64d79cf31e9c0773dea6224f7f021</td>
      <td>lgaalves/llama-2-13b-hf-platypus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NekoPunchBBB/Llama-2-13b-hf_Open-Platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NekoPunchBBB__Llama-2-13b-hf_Open-Platypus</td>
      <td>47.33</td>
      <td>58.87</td>
      <td>82.14</td>
      <td>54.98</td>
      <td>42.84</td>
      <td>77.11</td>
      <td>9.40</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c318a24121bd69509f395e17a9636093213ece21</td>
      <td>NekoPunchBBB/Llama-2-13b-hf_Open-Platypus</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2-13B-IA3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2-13B-IA3</td>
      <td>47.31</td>
      <td>61.09</td>
      <td>82.65</td>
      <td>56.32</td>
      <td>38.35</td>
      <td>75.69</td>
      <td>11.30</td>
      <td>5.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b738c64d536df02f5c137a94bc7a32a4c486012b</td>
      <td>yeontaek/Platypus2-13B-IA3</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardMath-13B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardMath-13B-V1.0</td>
      <td>47.27</td>
      <td>60.07</td>
      <td>82.01</td>
      <td>54.80</td>
      <td>42.70</td>
      <td>71.90</td>
      <td>12.36</td>
      <td>7.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>16.0</td>
      <td>True</td>
      <td>209316bea6eab73d8b18fca2a730b1dff3dcf999</td>
      <td>WizardLM/WizardMath-13B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Gryphe/MythoBoros-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Gryphe__MythoBoros-13b</td>
      <td>47.27</td>
      <td>58.19</td>
      <td>81.75</td>
      <td>50.13</td>
      <td>48.93</td>
      <td>75.77</td>
      <td>8.64</td>
      <td>7.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>12.0</td>
      <td>True</td>
      <td>67695d15e6610bc8055fbcde82f298e48ad2d374</td>
      <td>Gryphe/MythoBoros-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/Guanaco-13B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__Guanaco-13B-Uncensored</td>
      <td>47.23</td>
      <td>59.56</td>
      <td>82.70</td>
      <td>53.65</td>
      <td>43.26</td>
      <td>76.32</td>
      <td>9.10</td>
      <td>6.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>9.0</td>
      <td>True</td>
      <td>cf315234979f5924ad73399bcdcdf51b05a1fc98</td>
      <td>Fredithefish/Guanaco-13B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_v2_13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_v2_13b</td>
      <td>47.23</td>
      <td>55.12</td>
      <td>79.69</td>
      <td>50.07</td>
      <td>52.56</td>
      <td>72.69</td>
      <td>6.37</td>
      <td>14.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>12.85</td>
      <td>30.0</td>
      <td>True</td>
      <td>1058709314f7ca090937d0a2b7b37b0b3a8f12a3</td>
      <td>psmathur/orca_mini_v2_13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Gryphe/MythoLogic-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Gryphe__MythoLogic-13b</td>
      <td>47.23</td>
      <td>58.45</td>
      <td>81.56</td>
      <td>49.36</td>
      <td>49.47</td>
      <td>75.61</td>
      <td>8.64</td>
      <td>7.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>17.0</td>
      <td>True</td>
      <td>d89d925ad1eeaee465c4de3e5c74240a5a40b585</td>
      <td>Gryphe/MythoLogic-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down</td>
      <td>47.23</td>
      <td>58.36</td>
      <td>82.33</td>
      <td>56.14</td>
      <td>39.51</td>
      <td>76.40</td>
      <td>10.92</td>
      <td>6.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d824054153586d58139b7c3527ba211f33a81382</td>
      <td>CHIH-HUNG/llama-2-13b-FINETUNE4_compare15k_4.5w-r16-gate_up_down</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch</td>
      <td>47.19</td>
      <td>58.62</td>
      <td>82.56</td>
      <td>55.84</td>
      <td>42.09</td>
      <td>76.64</td>
      <td>7.05</td>
      <td>7.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>001a5f96daea57b5f256c2df270b35653b439f6f</td>
      <td>CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w-3_epoch</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/NobodyExistsOnTheInternet/GiftedConvo13bLoraNoEconsE4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NobodyExistsOnTheInternet__GiftedConvo13bLoraNoEconsE4</td>
      <td>47.18</td>
      <td>59.90</td>
      <td>84.11</td>
      <td>54.67</td>
      <td>41.94</td>
      <td>74.03</td>
      <td>7.81</td>
      <td>7.77</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f3d421aadb29830345bf392f793ce3c33e7d68c5</td>
      <td>NobodyExistsOnTheInternet/GiftedConvo13bLoraNoEconsE4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__gpt4-alpaca-lora-13B-HF</td>
      <td>47.17</td>
      <td>59.56</td>
      <td>82.09</td>
      <td>47.48</td>
      <td>48.96</td>
      <td>76.72</td>
      <td>9.10</td>
      <td>6.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>49678a2dd15fb4e1f1b99616ccc1ffd269912833</td>
      <td>TheBloke/gpt4-alpaca-lora-13B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v2</td>
      <td>47.16</td>
      <td>57.17</td>
      <td>81.14</td>
      <td>50.58</td>
      <td>49.54</td>
      <td>76.24</td>
      <td>9.10</td>
      <td>6.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>12.0</td>
      <td>True</td>
      <td>bd2a0968964c0f2dfae8f5a8950b43e35142f830</td>
      <td>openchat/openchat_v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_v2_w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_v2_w</td>
      <td>47.16</td>
      <td>57.34</td>
      <td>81.23</td>
      <td>50.17</td>
      <td>50.70</td>
      <td>75.93</td>
      <td>8.42</td>
      <td>6.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>29.0</td>
      <td>True</td>
      <td>0eb53946b8fac30606dc72541f2fc073cb6a0e12</td>
      <td>openchat/openchat_v2_w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/oasst-llama-13b-1000-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-llama-13b-1000-steps</td>
      <td>47.15</td>
      <td>58.11</td>
      <td>81.52</td>
      <td>48.65</td>
      <td>35.99</td>
      <td>77.51</td>
      <td>11.30</td>
      <td>16.97</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d2cd599cc40db3370009f45d6caa7e486cb6d31f</td>
      <td>dvruette/oasst-llama-13b-1000-steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NobodyExistsOnTheInternet/PuffedLIMA13bQLORA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NobodyExistsOnTheInternet__PuffedLIMA13bQLORA</td>
      <td>47.14</td>
      <td>59.90</td>
      <td>84.39</td>
      <td>53.68</td>
      <td>39.90</td>
      <td>75.22</td>
      <td>8.72</td>
      <td>8.21</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>7da6d235d625e16c850ccd0b947dee40071b1f89</td>
      <td>NobodyExistsOnTheInternet/PuffedLIMA13bQLORA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-gpt4-m2.0</td>
      <td>47.14</td>
      <td>59.22</td>
      <td>81.02</td>
      <td>53.73</td>
      <td>39.70</td>
      <td>73.64</td>
      <td>8.64</td>
      <td>14.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>26.0</td>
      <td>True</td>
      <td>a852b77f7d0777092c76898bc83f8e657ca2af3e</td>
      <td>jondurbin/airoboros-l2-13b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NobodyExistsOnTheInternet/PuffedConvo13bLoraE4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NobodyExistsOnTheInternet__PuffedConvo13bLoraE4</td>
      <td>47.14</td>
      <td>59.81</td>
      <td>84.39</td>
      <td>53.62</td>
      <td>39.87</td>
      <td>75.22</td>
      <td>8.79</td>
      <td>8.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>40e4fce0c25bd23f6011b424748ee2b5374b98d5</td>
      <td>NobodyExistsOnTheInternet/PuffedConvo13bLoraE4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lajonbot/WizardLM-13B-V1.2-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lajonbot__WizardLM-13B-V1.2-PL-lora_unload</td>
      <td>47.12</td>
      <td>58.53</td>
      <td>81.10</td>
      <td>55.15</td>
      <td>46.18</td>
      <td>71.03</td>
      <td>11.14</td>
      <td>6.67</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>5f14e6f5ea67fd2840791c46b3e00846cbdb32cf</td>
      <td>Lajonbot/WizardLM-13B-V1.2-PL-lora_unload</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/llama-2-13b-chat-platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__llama-2-13b-chat-platypus</td>
      <td>47.12</td>
      <td>53.84</td>
      <td>80.67</td>
      <td>54.44</td>
      <td>46.23</td>
      <td>76.01</td>
      <td>12.36</td>
      <td>6.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>828aa1020fc7d394fe8ee2c596e3211df7656eac</td>
      <td>lgaalves/llama-2-13b-chat-platypus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ziqingyang/chinese-alpaca-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ziqingyang__chinese-alpaca-2-7b</td>
      <td>47.11</td>
      <td>49.57</td>
      <td>72.62</td>
      <td>46.50</td>
      <td>48.63</td>
      <td>70.01</td>
      <td>5.76</td>
      <td>36.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.70</td>
      <td>93.0</td>
      <td>True</td>
      <td>ab2476bffedeed752daedd77e71900578e136e7c</td>
      <td>ziqingyang/chinese-alpaca-2-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/heegyu/WizardVicuna2-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__WizardVicuna2-13b-hf</td>
      <td>47.05</td>
      <td>55.38</td>
      <td>79.14</td>
      <td>48.46</td>
      <td>42.43</td>
      <td>73.48</td>
      <td>7.43</td>
      <td>23.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>6cfd95e2dcdb6996afa9eb5c63273a1a3524c6c6</td>
      <td>heegyu/WizardVicuna2-13b-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-19b-prototype</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-19b-prototype</td>
      <td>47.04</td>
      <td>59.22</td>
      <td>81.03</td>
      <td>55.73</td>
      <td>41.15</td>
      <td>76.40</td>
      <td>4.40</td>
      <td>11.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>19.36</td>
      <td>1.0</td>
      <td>True</td>
      <td>d2c8cc15c57da217ff29ebaaae4bc4f57d6b21b0</td>
      <td>The-Face-Of-Goonery/Huginn-19b-prototype</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b-gpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b-gpt4</td>
      <td>47.04</td>
      <td>59.39</td>
      <td>83.29</td>
      <td>47.89</td>
      <td>47.65</td>
      <td>75.77</td>
      <td>7.88</td>
      <td>7.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>17.0</td>
      <td>True</td>
      <td>c0eef6e6f63d4b11953539308717cea0079b44f9</td>
      <td>jondurbin/airoboros-13b-gpt4</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2-13B-QLoRa</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2-13B-QLoRa</td>
      <td>47.03</td>
      <td>57.51</td>
      <td>82.55</td>
      <td>57.34</td>
      <td>43.38</td>
      <td>76.64</td>
      <td>5.00</td>
      <td>6.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e62a8fafce0d64ac03d465a4e915bc1f50776a08</td>
      <td>yeontaek/Platypus2-13B-QLoRa</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/vicuna-13B-1.1-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__vicuna-13B-1.1-HF</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>96.0</td>
      <td>True</td>
      <td>8c71dbe9221e83d2ec72e4dc08beccfc78b563c0</td>
      <td>TheBloke/vicuna-13B-1.1-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/kevinpro/Vicuna-13B-CoT</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_kevinpro__Vicuna-13B-CoT</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>346e3c46959cf9f1e03feffa761afe020c0fb6a8</td>
      <td>kevinpro/Vicuna-13B-CoT</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/eachadea/vicuna-13b-1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_eachadea__vicuna-13b-1.1</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>134.0</td>
      <td>True</td>
      <td>bfcc6ca66694310be6c85ba0638597f4256c4143</td>
      <td>eachadea/vicuna-13b-1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/pillowtalks-ai/delta13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pillowtalks-ai__delta13b</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.13</td>
      <td>51.94</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>83fa0860990df1db35550f973ba4306449e35412</td>
      <td>pillowtalks-ai/delta13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-13b-v1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-13b-v1.1</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.14</td>
      <td>51.90</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>96.0</td>
      <td>True</td>
      <td>8c71dbe9221e83d2ec72e4dc08beccfc78b563c0</td>
      <td>lmsys/vicuna-13b-v1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-13b-delta-v1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-13b-delta-v1.1</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.14</td>
      <td>51.90</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>402.0</td>
      <td>True</td>
      <td>ffed4c7cf1b9814812078efbe29ec3f610ea39e7</td>
      <td>lmsys/vicuna-13b-delta-v1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Vicuna-13B-CoT-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Vicuna-13B-CoT-fp16</td>
      <td>47.00</td>
      <td>52.73</td>
      <td>80.14</td>
      <td>51.90</td>
      <td>52.08</td>
      <td>74.19</td>
      <td>8.64</td>
      <td>9.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>fe74a0ece9089828b301bd0f067ae5f257516179</td>
      <td>TheBloke/Vicuna-13B-CoT-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Kimiko-13B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Kimiko-13B-fp16</td>
      <td>46.98</td>
      <td>59.22</td>
      <td>82.35</td>
      <td>55.85</td>
      <td>39.55</td>
      <td>76.72</td>
      <td>8.79</td>
      <td>6.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>27868769e2d6b1af46337f0997c71b0577952a3d</td>
      <td>TheBloke/Kimiko-13B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NekoPunchBBB/Llama-2-13b-hf_Open-Platypus-8bit-att</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NekoPunchBBB__Llama-2-13b-hf_Open-Platypus-8bit-att</td>
      <td>46.97</td>
      <td>57.51</td>
      <td>82.14</td>
      <td>54.56</td>
      <td>42.21</td>
      <td>76.56</td>
      <td>9.55</td>
      <td>6.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>83a8e51d0a72dcfbe5de13dc7ee10dc20e91602e</td>
      <td>NekoPunchBBB/Llama-2-13b-hf_Open-Platypus-8bit-att</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lajonbot/vicuna-13b-v1.3-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lajonbot__vicuna-13b-v1.3-PL-lora_unload</td>
      <td>46.97</td>
      <td>54.86</td>
      <td>80.41</td>
      <td>52.20</td>
      <td>49.62</td>
      <td>76.09</td>
      <td>9.02</td>
      <td>6.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>5582369752583b02df3cba4bd2a733d12265cddb</td>
      <td>Lajonbot/vicuna-13b-v1.3-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wahaha1987/llama_13b_sharegpt94k_fastchat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wahaha1987__llama_13b_sharegpt94k_fastchat</td>
      <td>46.96</td>
      <td>53.75</td>
      <td>79.47</td>
      <td>51.50</td>
      <td>49.54</td>
      <td>72.61</td>
      <td>8.42</td>
      <td>13.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>388bc2f82a1ee8b963c7f94f9c7b6743f7214306</td>
      <td>wahaha1987/llama_13b_sharegpt94k_fastchat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jb723/llama2-ko-7B-model</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jb723__llama2-ko-7B-model</td>
      <td>46.95</td>
      <td>56.31</td>
      <td>79.51</td>
      <td>45.71</td>
      <td>40.98</td>
      <td>72.06</td>
      <td>2.58</td>
      <td>31.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>03d23910fa0f9b0542ce7634cbcd36983321f55a</td>
      <td>jb723/llama2-ko-7B-model</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</td>
      <td>46.93</td>
      <td>58.96</td>
      <td>81.94</td>
      <td>55.00</td>
      <td>40.26</td>
      <td>76.56</td>
      <td>8.72</td>
      <td>7.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>30edbe648df2661dd779cd19ef613e6914dcc8e0</td>
      <td>dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/BramVanroy/Llama-2-13b-chat-dutch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BramVanroy__Llama-2-13b-chat-dutch</td>
      <td>46.91</td>
      <td>59.30</td>
      <td>81.45</td>
      <td>55.82</td>
      <td>38.23</td>
      <td>76.64</td>
      <td>10.69</td>
      <td>6.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>13.02</td>
      <td>10.0</td>
      <td>True</td>
      <td>428508a0cf288c0f5b7891c9b2f758ddf4d62c26</td>
      <td>BramVanroy/Llama-2-13b-chat-dutch</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/meta-llama/Llama-2-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-13b-hf</td>
      <td>46.89</td>
      <td>59.39</td>
      <td>82.13</td>
      <td>55.77</td>
      <td>37.38</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>6.08</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>389.0</td>
      <td>False</td>
      <td>7da18fb10421c3ae2a1eb92815bad75e84816e35</td>
      <td>meta-llama/Llama-2-13b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TaylorAI/Flash-Llama-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TaylorAI__Flash-Llama-13B</td>
      <td>46.87</td>
      <td>59.30</td>
      <td>82.15</td>
      <td>55.67</td>
      <td>37.39</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>6.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>81b40096471a8980e3e1a8998f358bd363033783</td>
      <td>TaylorAI/Flash-Llama-13B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TheBloke/Llama-2-13B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Llama-2-13B-fp16</td>
      <td>46.87</td>
      <td>59.30</td>
      <td>82.15</td>
      <td>55.67</td>
      <td>37.39</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>6.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>51.0</td>
      <td>True</td>
      <td>b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb</td>
      <td>TheBloke/Llama-2-13B-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NewstaR/Starlight-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NewstaR__Starlight-13B</td>
      <td>46.87</td>
      <td>59.30</td>
      <td>82.15</td>
      <td>55.67</td>
      <td>37.39</td>
      <td>76.64</td>
      <td>10.84</td>
      <td>6.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>cb9fced568b1abd881133c642c427aaa488f00cc</td>
      <td>NewstaR/Starlight-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/chargoddard/llama2-22b-blocktriangular</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__llama2-22b-blocktriangular</td>
      <td>46.86</td>
      <td>58.28</td>
      <td>82.69</td>
      <td>54.53</td>
      <td>39.23</td>
      <td>75.93</td>
      <td>11.22</td>
      <td>6.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>21.62</td>
      <td>4.0</td>
      <td>True</td>
      <td>7adbaa5b8e122bb93bf510d8655ec4132d7b4a8a</td>
      <td>chargoddard/llama2-22b-blocktriangular</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/llm-agents/tora-13b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llm-agents__tora-13b-v1.0</td>
      <td>46.85</td>
      <td>58.96</td>
      <td>82.31</td>
      <td>54.73</td>
      <td>40.25</td>
      <td>75.61</td>
      <td>9.86</td>
      <td>6.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>0636c1f582c979a5a292cc5f3dc293800b1494e2</td>
      <td>llm-agents/tora-13b-v1.0</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/chargoddard/llama2-22b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__llama2-22b</td>
      <td>46.85</td>
      <td>58.53</td>
      <td>82.55</td>
      <td>54.68</td>
      <td>39.84</td>
      <td>76.32</td>
      <td>9.93</td>
      <td>6.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>21.62</td>
      <td>35.0</td>
      <td>True</td>
      <td>2bece0787009b4b584f49d0e0d1b49ecf4a52da9</td>
      <td>chargoddard/llama2-22b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/shareAI/bimoGPT-llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shareAI__bimoGPT-llama2-13b</td>
      <td>46.84</td>
      <td>58.79</td>
      <td>82.08</td>
      <td>55.60</td>
      <td>37.82</td>
      <td>76.48</td>
      <td>11.30</td>
      <td>5.84</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>13.00</td>
      <td>6.0</td>
      <td>False</td>
      <td>c29b67965ea55da3e2ac678eef7ffdf36f8ef5ab</td>
      <td>shareAI/bimoGPT-llama2-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/NobodyExistsOnTheInternet/GiftedConvo13bLoraNoEcons</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NobodyExistsOnTheInternet__GiftedConvo13bLoraNoEcons</td>
      <td>46.84</td>
      <td>59.39</td>
      <td>83.19</td>
      <td>55.15</td>
      <td>40.56</td>
      <td>74.03</td>
      <td>7.81</td>
      <td>7.77</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>9d7031e7d956dd2d25c61d85f594d115ce65b172</td>
      <td>NobodyExistsOnTheInternet/GiftedConvo13bLoraNoEcons</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/MLewd-L2-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__MLewd-L2-13B</td>
      <td>46.84</td>
      <td>58.28</td>
      <td>82.32</td>
      <td>54.67</td>
      <td>48.66</td>
      <td>73.48</td>
      <td>1.29</td>
      <td>9.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>feb1fa71e0b24261d3ca428b4aed881dd31f166e</td>
      <td>Undi95/MLewd-L2-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/nkpz/llama2-22b-chat-wizard-uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nkpz__llama2-22b-chat-wizard-uncensored</td>
      <td>46.83</td>
      <td>56.23</td>
      <td>80.39</td>
      <td>53.62</td>
      <td>45.76</td>
      <td>70.24</td>
      <td>11.14</td>
      <td>10.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>21.83</td>
      <td>3.0</td>
      <td>True</td>
      <td>90cffebc8f530161505b84740ff6c8f646299d6c</td>
      <td>nkpz/llama2-22b-chat-wizard-uncensored</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardCoder-Python-34B-V1.0</td>
      <td>46.83</td>
      <td>52.13</td>
      <td>74.78</td>
      <td>49.15</td>
      <td>48.85</td>
      <td>68.35</td>
      <td>9.48</td>
      <td>25.06</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>616.0</td>
      <td>True</td>
      <td>5cdc34e4a81d202f1d4a3b5d60e028aab895dfeb</td>
      <td>WizardLM/WizardCoder-Python-34B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BramVanroy/llama2-13b-ft-mc4_nl_cleaned_tiny</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BramVanroy__llama2-13b-ft-mc4_nl_cleaned_tiny</td>
      <td>46.81</td>
      <td>59.30</td>
      <td>82.04</td>
      <td>54.67</td>
      <td>38.03</td>
      <td>77.27</td>
      <td>10.31</td>
      <td>6.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>b23fe7d174653b87dc08507d9b83504a8dddbc45</td>
      <td>BramVanroy/llama2-13b-ft-mc4_nl_cleaned_tiny</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/guanaco-13B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__guanaco-13B-HF</td>
      <td>46.80</td>
      <td>57.85</td>
      <td>83.84</td>
      <td>48.28</td>
      <td>46.73</td>
      <td>75.85</td>
      <td>8.72</td>
      <td>6.36</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>7.0</td>
      <td>True</td>
      <td>bd59c700815124df616a17f5b49a0bc51590b231</td>
      <td>TheBloke/guanaco-13B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/xzuyn/Alpacino-SuperCOT-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_xzuyn__Alpacino-SuperCOT-13B</td>
      <td>46.80</td>
      <td>58.36</td>
      <td>81.69</td>
      <td>47.89</td>
      <td>45.42</td>
      <td>76.95</td>
      <td>7.51</td>
      <td>9.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>3a82b04684fe99d59556421c3f96a187049a3cec</td>
      <td>xzuyn/Alpacino-SuperCOT-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-7b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-7b-chat</td>
      <td>46.79</td>
      <td>52.47</td>
      <td>78.35</td>
      <td>39.51</td>
      <td>44.52</td>
      <td>73.16</td>
      <td>7.58</td>
      <td>31.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>9af636df9c8693ea857b62442bd1c6c73d657dc6</td>
      <td>bofenghuang/vigogne-7b-chat</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/deacon-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__deacon-13b</td>
      <td>46.78</td>
      <td>57.85</td>
      <td>82.63</td>
      <td>55.25</td>
      <td>39.33</td>
      <td>76.32</td>
      <td>10.39</td>
      <td>5.67</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.02</td>
      <td>1.0</td>
      <td>True</td>
      <td>6c3a002f6c9e8a481a7375d91856d603bf6dd040</td>
      <td>KnutJaegersberg/deacon-13b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/chavinlo/gpt4-x-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chavinlo__gpt4-x-alpaca</td>
      <td>46.78</td>
      <td>52.82</td>
      <td>79.59</td>
      <td>48.19</td>
      <td>48.88</td>
      <td>70.17</td>
      <td>2.81</td>
      <td>24.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>456.0</td>
      <td>True</td>
      <td>6a571f458cab9a23d14324ec63e0abd1744c8353</td>
      <td>chavinlo/gpt4-x-alpaca</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CHIH-HUNG__llama-2-13b-Open_Platypus_and_ccp_2.6w</td>
      <td>46.76</td>
      <td>58.96</td>
      <td>82.51</td>
      <td>56.12</td>
      <td>40.07</td>
      <td>76.64</td>
      <td>6.82</td>
      <td>6.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2929bfa1049db46df94f5710755178d18a981665</td>
      <td>CHIH-HUNG/llama-2-13b-Open_Platypus_and_ccp_2.6w</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/chargoddard/llama2-22b-blocktriangular</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__llama2-22b-blocktriangular</td>
      <td>46.76</td>
      <td>58.53</td>
      <td>82.59</td>
      <td>54.64</td>
      <td>39.30</td>
      <td>76.32</td>
      <td>9.78</td>
      <td>6.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>21.62</td>
      <td>4.0</td>
      <td>True</td>
      <td>40a51343ae776b5cb39f2b4343ae8f9b676ffd58</td>
      <td>chargoddard/llama2-22b-blocktriangular</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/IGeniusDev/llama13B-quant8-testv1-openorca-customdataset</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_IGeniusDev__llama13B-quant8-testv1-openorca-customdataset</td>
      <td>46.74</td>
      <td>60.49</td>
      <td>82.97</td>
      <td>54.44</td>
      <td>37.34</td>
      <td>75.69</td>
      <td>10.08</td>
      <td>6.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>f364d000bedac80e72aa103c08b77aee1b61b7da</td>
      <td>IGeniusDev/llama13B-quant8-testv1-openorca-customdataset</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/quantumaikr/QuantumLM</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__QuantumLM</td>
      <td>46.73</td>
      <td>55.80</td>
      <td>79.74</td>
      <td>54.17</td>
      <td>46.71</td>
      <td>74.19</td>
      <td>9.86</td>
      <td>6.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>9058130b416355b37f5f78777748aa56d98a4da0</td>
      <td>quantumaikr/QuantumLM</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/ehartford/WizardLM-1.0-Uncensored-CodeLlama-34b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-1.0-Uncensored-CodeLlama-34b</td>
      <td>46.72</td>
      <td>56.40</td>
      <td>75.45</td>
      <td>54.51</td>
      <td>43.06</td>
      <td>72.45</td>
      <td>19.64</td>
      <td>5.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>20.0</td>
      <td>True</td>
      <td>3e8df2cf4a4ee1c0b2d079cb7be70024d425ea8c</td>
      <td>ehartford/WizardLM-1.0-Uncensored-CodeLlama-34b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/vicuna-13b-v1.3.0-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__vicuna-13b-v1.3.0-GPTQ</td>
      <td>46.72</td>
      <td>54.35</td>
      <td>79.47</td>
      <td>51.97</td>
      <td>50.88</td>
      <td>74.66</td>
      <td>8.42</td>
      <td>7.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>19.0</td>
      <td>True</td>
      <td>6ef1f8d8638ea2d6681a8e3da73be57c501d847b</td>
      <td>TheBloke/vicuna-13b-v1.3.0-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elliotthwang/Elliott-Chinese-LLaMa-GPTQ-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elliotthwang__Elliott-Chinese-LLaMa-GPTQ-V1.0</td>
      <td>46.72</td>
      <td>50.68</td>
      <td>75.36</td>
      <td>49.33</td>
      <td>44.70</td>
      <td>72.38</td>
      <td>17.36</td>
      <td>17.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>53.90</td>
      <td>0.0</td>
      <td>True</td>
      <td>01305dc473ba231519fe71e7f4b2d1e3f6aa9bc8</td>
      <td>elliotthwang/Elliott-Chinese-LLaMa-GPTQ-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/wizard-vicuna-13B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__wizard-vicuna-13B-HF</td>
      <td>46.66</td>
      <td>54.69</td>
      <td>79.18</td>
      <td>48.88</td>
      <td>49.62</td>
      <td>74.82</td>
      <td>9.33</td>
      <td>10.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>48.0</td>
      <td>True</td>
      <td>12dc8aacb474522ae2a83c18cb0fdf0907987f8f</td>
      <td>TheBloke/wizard-vicuna-13B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/junelee/wizard-vicuna-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_junelee__wizard-vicuna-13b</td>
      <td>46.64</td>
      <td>54.69</td>
      <td>79.18</td>
      <td>48.88</td>
      <td>49.62</td>
      <td>74.82</td>
      <td>9.17</td>
      <td>10.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>70.0</td>
      <td>True</td>
      <td>419dc5acc391de54a60d0b041e94e767d1ef2032</td>
      <td>junelee/wizard-vicuna-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shibing624__chinese-alpaca-plus-13b-hf</td>
      <td>46.63</td>
      <td>53.16</td>
      <td>73.51</td>
      <td>48.81</td>
      <td>45.32</td>
      <td>75.06</td>
      <td>2.12</td>
      <td>28.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.94</td>
      <td>32.0</td>
      <td>True</td>
      <td>a118d2c35573b9a70c6f5b56fba4b657f74ce00c</td>
      <td>shibing624/chinese-alpaca-plus-13b-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Open-Orca/LlongOrca-7B-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Open-Orca__LlongOrca-7B-16k</td>
      <td>46.60</td>
      <td>57.51</td>
      <td>79.44</td>
      <td>49.35</td>
      <td>49.84</td>
      <td>74.51</td>
      <td>7.51</td>
      <td>8.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>39.0</td>
      <td>True</td>
      <td>1370c7c595e6c8394e6332bc535ae25e21def85b</td>
      <td>Open-Orca/LlongOrca-7B-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-7b-gpt4-2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-7b-gpt4-2.0</td>
      <td>46.57</td>
      <td>52.90</td>
      <td>78.53</td>
      <td>45.09</td>
      <td>39.45</td>
      <td>71.11</td>
      <td>3.18</td>
      <td>35.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>8432fe95c426ca7709cf2d31a64eee612c4dea42</td>
      <td>jondurbin/airoboros-l2-7b-gpt4-2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Undi95/Llama2-13B-no_robots-alpaca-lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Undi95__Llama2-13B-no_robots-alpaca-lora</td>
      <td>46.55</td>
      <td>58.87</td>
      <td>82.43</td>
      <td>53.11</td>
      <td>40.46</td>
      <td>75.30</td>
      <td>6.44</td>
      <td>9.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>13.00</td>
      <td>1.0</td>
      <td>True</td>
      <td>581aba329e607533c299746bb9eb4154a7aab139</td>
      <td>Undi95/Llama2-13B-no_robots-alpaca-lora</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/The-Face-Of-Goonery/Huginn-22b-Prototype</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_The-Face-Of-Goonery__Huginn-22b-Prototype</td>
      <td>46.52</td>
      <td>57.68</td>
      <td>80.69</td>
      <td>49.81</td>
      <td>52.11</td>
      <td>71.59</td>
      <td>2.27</td>
      <td>11.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>21.83</td>
      <td>2.0</td>
      <td>True</td>
      <td>29222b05794abb862ad0aaaf3020696c9f599810</td>
      <td>The-Face-Of-Goonery/Huginn-22b-Prototype</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Llama-2-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Llama-2-13B-GPTQ</td>
      <td>46.51</td>
      <td>59.13</td>
      <td>81.48</td>
      <td>54.45</td>
      <td>37.07</td>
      <td>76.16</td>
      <td>11.30</td>
      <td>6.01</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>16.23</td>
      <td>99.0</td>
      <td>True</td>
      <td>b7db471d1789802a3a8e3b93cdd66a9f046f17c3</td>
      <td>TheBloke/Llama-2-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/migtissera/Synthia-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_migtissera__Synthia-7B</td>
      <td>46.50</td>
      <td>56.14</td>
      <td>78.60</td>
      <td>50.35</td>
      <td>45.03</td>
      <td>74.27</td>
      <td>6.60</td>
      <td>14.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>4f9e95665d95b4c692910190ff77257216e476f1</td>
      <td>migtissera/Synthia-7B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Yhyu13/chimera-inst-chat-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yhyu13__chimera-inst-chat-13b-hf</td>
      <td>46.49</td>
      <td>55.38</td>
      <td>78.93</td>
      <td>50.60</td>
      <td>50.12</td>
      <td>73.95</td>
      <td>8.19</td>
      <td>8.30</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>a6943d2d30d0af904b3321559157d589e60f9e0f</td>
      <td>Yhyu13/chimera-inst-chat-13b-hf</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/manticore-13b-chat-pyg-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__manticore-13b-chat-pyg-GPTQ</td>
      <td>46.49</td>
      <td>57.85</td>
      <td>81.07</td>
      <td>47.56</td>
      <td>47.77</td>
      <td>75.93</td>
      <td>8.49</td>
      <td>6.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>33.0</td>
      <td>True</td>
      <td>923f27245d13058c9c1b3ab0eab6c6c93ffc162e</td>
      <td>TheBloke/manticore-13b-chat-pyg-GPTQ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Yehoon/yehoon_llama2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yehoon__yehoon_llama2</td>
      <td>46.49</td>
      <td>54.78</td>
      <td>78.98</td>
      <td>51.29</td>
      <td>49.17</td>
      <td>74.74</td>
      <td>7.28</td>
      <td>9.16</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>443cb81ce988ea6c0b1e20132c170463d559367e</td>
      <td>Yehoon/yehoon_llama2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16</td>
      <td>46.47</td>
      <td>59.04</td>
      <td>82.33</td>
      <td>55.36</td>
      <td>35.75</td>
      <td>76.32</td>
      <td>10.01</td>
      <td>6.48</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a3ed7416156963f49bf4dc056188e006c0c214d2</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-atom-13b-v9-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-atom-13b-v9-bf16</td>
      <td>46.47</td>
      <td>51.19</td>
      <td>75.99</td>
      <td>49.33</td>
      <td>48.66</td>
      <td>73.32</td>
      <td>15.39</td>
      <td>11.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>12.94</td>
      <td>5.0</td>
      <td>True</td>
      <td>35bb2c73953f6ea40be6f0c8c6b2dfa7ecbaa0df</td>
      <td>OpenBuddy/openbuddy-atom-13b-v9-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-13b-gpt4-1.4.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-13b-gpt4-1.4.1</td>
      <td>46.46</td>
      <td>59.13</td>
      <td>82.78</td>
      <td>55.62</td>
      <td>40.27</td>
      <td>73.32</td>
      <td>6.97</td>
      <td>7.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>12.0</td>
      <td>True</td>
      <td>35ff51ebe5668269dfd33a9ed94412d88f1f4b65</td>
      <td>jondurbin/airoboros-l2-13b-gpt4-1.4.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/trurl-2-7b-pl-instruct_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__trurl-2-7b-pl-instruct_unload</td>
      <td>46.46</td>
      <td>53.16</td>
      <td>74.64</td>
      <td>49.89</td>
      <td>45.74</td>
      <td>72.30</td>
      <td>7.43</td>
      <td>22.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>768d800e4dbe3fc95334f30ca7cd02113d3e3fd3</td>
      <td>Aspik101/trurl-2-7b-pl-instruct_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/stabilityai/StableBeluga1-Delta</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__StableBeluga1-Delta</td>
      <td>46.36</td>
      <td>68.17</td>
      <td>85.88</td>
      <td>64.83</td>
      <td>55.81</td>
      <td>49.80</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>65.29</td>
      <td>54.0</td>
      <td>False</td>
      <td>40a78d91d43ad9aef6663ff15ddc15be9922bce5</td>
      <td>stabilityai/StableBeluga1-Delta</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LinkSoul/Chinese-Llama-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LinkSoul__Chinese-Llama-2-7b</td>
      <td>46.34</td>
      <td>52.99</td>
      <td>75.64</td>
      <td>50.74</td>
      <td>48.94</td>
      <td>72.77</td>
      <td>14.48</td>
      <td>8.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>6.61</td>
      <td>261.0</td>
      <td>True</td>
      <td>72efd71d7f89d9c46008b7a574faf90300ed9ba8</td>
      <td>LinkSoul/Chinese-Llama-2-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</td>
      <td>46.33</td>
      <td>58.45</td>
      <td>81.97</td>
      <td>55.02</td>
      <td>35.85</td>
      <td>75.69</td>
      <td>10.69</td>
      <td>6.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>5a89844b1aea3f0573e696143ec66727df4b5d79</td>
      <td>dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/longchat-13b-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__longchat-13b-16k</td>
      <td>46.32</td>
      <td>53.58</td>
      <td>77.67</td>
      <td>45.24</td>
      <td>47.07</td>
      <td>70.09</td>
      <td>4.17</td>
      <td>26.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>123.0</td>
      <td>True</td>
      <td>70e2e38b82f1e25d8b90b50fbfc2361123bef45f</td>
      <td>lmsys/longchat-13b-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/itsliupeng/llama2_7b_mmlu</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_itsliupeng__llama2_7b_mmlu</td>
      <td>46.31</td>
      <td>56.14</td>
      <td>79.13</td>
      <td>60.04</td>
      <td>40.95</td>
      <td>74.43</td>
      <td>7.88</td>
      <td>5.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>553178f8d5d69eb1dfa5b9503d2ce0c1e481e5b1</td>
      <td>itsliupeng/llama2_7b_mmlu</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/project-baize/baize-v2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_project-baize__baize-v2-13b</td>
      <td>46.30</td>
      <td>56.91</td>
      <td>79.29</td>
      <td>49.72</td>
      <td>47.88</td>
      <td>74.90</td>
      <td>8.95</td>
      <td>6.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>22.0</td>
      <td>True</td>
      <td>a3c4bbccca8b650700a49a225582c17bb49b446b</td>
      <td>project-baize/baize-v2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elliotthwang/Elliott-Chinese-LLaMa-GPTQ-V2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elliotthwang__Elliott-Chinese-LLaMa-GPTQ-V2.0</td>
      <td>46.30</td>
      <td>50.77</td>
      <td>75.36</td>
      <td>49.41</td>
      <td>44.70</td>
      <td>72.61</td>
      <td>16.00</td>
      <td>15.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>52.86</td>
      <td>0.0</td>
      <td>True</td>
      <td>ebffe57ba6cc70b60ff5295889abc62d91eeb4dd</td>
      <td>elliotthwang/Elliott-Chinese-LLaMa-GPTQ-V2.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/clibrain/Llama-2-13b-ft-instruct-es</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_clibrain__Llama-2-13b-ft-instruct-es</td>
      <td>46.27</td>
      <td>59.39</td>
      <td>81.51</td>
      <td>54.31</td>
      <td>37.81</td>
      <td>75.77</td>
      <td>8.57</td>
      <td>6.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>10.0</td>
      <td>True</td>
      <td>772b53f64f484fa0d651d453bcefc35a0f52f251</td>
      <td>clibrain/Llama-2-13b-ft-instruct-es</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/teknium/Mistral-Trismegistus-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_teknium__Mistral-Trismegistus-7B</td>
      <td>46.17</td>
      <td>54.10</td>
      <td>77.91</td>
      <td>54.49</td>
      <td>49.36</td>
      <td>70.17</td>
      <td>9.93</td>
      <td>7.24</td>
      <td>MistralForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>7.11</td>
      <td>59.0</td>
      <td>True</td>
      <td>0a5752d096ebab21759dbe203f6b7c7f6092faf2</td>
      <td>teknium/Mistral-Trismegistus-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/totally-not-an-llm/EverythingLM-13b-V2-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_totally-not-an-llm__EverythingLM-13b-V2-16k</td>
      <td>46.08</td>
      <td>58.70</td>
      <td>80.88</td>
      <td>49.69</td>
      <td>47.37</td>
      <td>73.01</td>
      <td>6.82</td>
      <td>6.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>31.0</td>
      <td>True</td>
      <td>943f932ae1ae462389e6d2db5273158530749fff</td>
      <td>totally-not-an-llm/EverythingLM-13b-V2-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/camel-ai/CAMEL-13B-Combined-Data</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_camel-ai__CAMEL-13B-Combined-Data</td>
      <td>46.07</td>
      <td>55.63</td>
      <td>79.25</td>
      <td>49.74</td>
      <td>47.42</td>
      <td>75.45</td>
      <td>7.13</td>
      <td>7.86</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>11.0</td>
      <td>True</td>
      <td>6d98f2801f13d89de7978ee9f348a52ea46a24ec</td>
      <td>camel-ai/CAMEL-13B-Combined-Data</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openchat/openchat_8192</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__openchat_8192</td>
      <td>46.07</td>
      <td>59.56</td>
      <td>81.44</td>
      <td>46.26</td>
      <td>46.70</td>
      <td>74.98</td>
      <td>7.35</td>
      <td>6.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>220.0</td>
      <td>True</td>
      <td>f661da5af278fbda8a43b19ff0250e4efc103e3e</td>
      <td>openchat/openchat_8192</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/PocketDoc/Dans-PersonalityEngine-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-PersonalityEngine-13b</td>
      <td>46.00</td>
      <td>58.45</td>
      <td>82.30</td>
      <td>47.58</td>
      <td>41.12</td>
      <td>77.51</td>
      <td>9.33</td>
      <td>5.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>3b37c31e04419adcc91eddb57f24fd6f9ac91938</td>
      <td>PocketDoc/Dans-PersonalityEngine-13b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/frank098/Wizard-Vicuna-13B-juniper</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_frank098__Wizard-Vicuna-13B-juniper</td>
      <td>45.98</td>
      <td>55.89</td>
      <td>79.75</td>
      <td>44.99</td>
      <td>54.72</td>
      <td>72.69</td>
      <td>7.28</td>
      <td>6.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>24f58beb9ed4cf635fc962853ed71d0f4b1909ba</td>
      <td>frank098/Wizard-Vicuna-13B-juniper</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LeoLM/leo-hessianai-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LeoLM__leo-hessianai-13b</td>
      <td>45.97</td>
      <td>57.25</td>
      <td>81.94</td>
      <td>53.65</td>
      <td>38.03</td>
      <td>76.09</td>
      <td>8.95</td>
      <td>5.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>16.0</td>
      <td>True</td>
      <td>a947965cb07ca12a38ff981fe65b618d7dea28d3</td>
      <td>LeoLM/leo-hessianai-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/abhishek/llama2guanacotest</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_abhishek__llama2guanacotest</td>
      <td>45.95</td>
      <td>51.62</td>
      <td>77.55</td>
      <td>48.49</td>
      <td>43.88</td>
      <td>73.16</td>
      <td>11.75</td>
      <td>15.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>679d17809939a0bf9b79bbb027898cbea64045b2</td>
      <td>abhishek/llama2guanacotest</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/meta-math/MetaMath-13B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-math__MetaMath-13B-V1.0</td>
      <td>45.95</td>
      <td>49.49</td>
      <td>76.48</td>
      <td>47.74</td>
      <td>41.58</td>
      <td>72.45</td>
      <td>28.51</td>
      <td>5.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>0b448f6f64808f8bca94dc871e96a3eae7e95621</td>
      <td>meta-math/MetaMath-13B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lizhuang144/llama_mirror_13b_v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lizhuang144__llama_mirror_13b_v1.0</td>
      <td>45.94</td>
      <td>57.59</td>
      <td>80.53</td>
      <td>48.00</td>
      <td>44.54</td>
      <td>76.64</td>
      <td>7.43</td>
      <td>6.87</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>379cb8f080110f3418155029f534f67a79e25db4</td>
      <td>lizhuang144/llama_mirror_13b_v1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Xwin-LM__Xwin-LM-7B-V0.1</td>
      <td>45.94</td>
      <td>56.57</td>
      <td>79.40</td>
      <td>49.98</td>
      <td>47.89</td>
      <td>73.32</td>
      <td>5.31</td>
      <td>9.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>73.0</td>
      <td>True</td>
      <td>470e680120a7249d6e8a875345015ddba1711100</td>
      <td>Xwin-LM/Xwin-LM-7B-V0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/heegyu/LIMA-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__LIMA-13b-hf</td>
      <td>45.92</td>
      <td>57.42</td>
      <td>81.68</td>
      <td>48.72</td>
      <td>41.76</td>
      <td>77.19</td>
      <td>8.87</td>
      <td>5.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>98faa74a9b41cbd9033904cd58420705936849eb</td>
      <td>heegyu/LIMA-13b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-7b-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-7b-v1.5</td>
      <td>45.90</td>
      <td>53.24</td>
      <td>77.39</td>
      <td>51.04</td>
      <td>50.34</td>
      <td>72.14</td>
      <td>8.19</td>
      <td>8.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>70.0</td>
      <td>True</td>
      <td>de56c35b1763eaae20f4d60efd64af0a9091ebe5</td>
      <td>lmsys/vicuna-7b-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/WizardLM-13B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-13B-Uncensored</td>
      <td>45.90</td>
      <td>50.94</td>
      <td>76.64</td>
      <td>43.96</td>
      <td>46.73</td>
      <td>70.56</td>
      <td>2.05</td>
      <td>30.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>433.0</td>
      <td>True</td>
      <td>9025c5f96fef9525da9238369ad082961b0e9494</td>
      <td>ehartford/WizardLM-13B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/beaugogh/Llama2-7b-openorca-mc-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beaugogh__Llama2-7b-openorca-mc-v2</td>
      <td>45.88</td>
      <td>55.55</td>
      <td>81.26</td>
      <td>48.30</td>
      <td>51.49</td>
      <td>72.85</td>
      <td>5.38</td>
      <td>6.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e74a9cca843cdeb8591d4e4f4320dc1870adf1b</td>
      <td>beaugogh/Llama2-7b-openorca-mc-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-7b-v1.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-7b-v1.5</td>
      <td>45.88</td>
      <td>53.24</td>
      <td>77.39</td>
      <td>50.82</td>
      <td>50.33</td>
      <td>72.06</td>
      <td>8.11</td>
      <td>9.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>70.0</td>
      <td>True</td>
      <td>de56c35b1763eaae20f4d60efd64af0a9091ebe5</td>
      <td>lmsys/vicuna-7b-v1.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mncai/Llama2-7B-guanaco-dolphin-500</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mncai__Llama2-7B-guanaco-dolphin-500</td>
      <td>45.80</td>
      <td>56.74</td>
      <td>81.63</td>
      <td>48.69</td>
      <td>46.94</td>
      <td>74.27</td>
      <td>5.99</td>
      <td>6.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>afe00170f084f773e401ba7d738d692533cca6b4</td>
      <td>mncai/Llama2-7B-guanaco-dolphin-500</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Alpacino13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Alpacino13b</td>
      <td>45.78</td>
      <td>58.53</td>
      <td>81.31</td>
      <td>47.92</td>
      <td>41.66</td>
      <td>76.95</td>
      <td>7.96</td>
      <td>6.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>29.0</td>
      <td>True</td>
      <td>7092a5c8dec649694dd66ff8cfe5452ce52e6a40</td>
      <td>digitous/Alpacino13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-13b-longlora-32k-ft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-13b-longlora-32k-ft</td>
      <td>45.77</td>
      <td>59.47</td>
      <td>82.61</td>
      <td>52.13</td>
      <td>37.44</td>
      <td>75.53</td>
      <td>7.73</td>
      <td>5.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>6d17c854025b0bd54ce572ac803f1bb052875dbf</td>
      <td>Yukang/Llama-2-13b-longlora-32k-ft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/xzuyn/MedicWizard-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_xzuyn__MedicWizard-7B</td>
      <td>45.76</td>
      <td>53.50</td>
      <td>78.39</td>
      <td>44.61</td>
      <td>41.32</td>
      <td>70.56</td>
      <td>4.93</td>
      <td>27.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>13.0</td>
      <td>True</td>
      <td>0b3ef975fb5e8ac1eae775160ab54c98221889df</td>
      <td>xzuyn/MedicWizard-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-PileOfSets-Mk1-llama-13b-merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-PileOfSets-Mk1-llama-13b-merged</td>
      <td>45.76</td>
      <td>58.79</td>
      <td>81.79</td>
      <td>48.12</td>
      <td>41.24</td>
      <td>76.16</td>
      <td>8.49</td>
      <td>5.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>a7e5484df8aceae7800ae9301a3954cf74b527e9</td>
      <td>PocketDoc/Dans-PileOfSets-Mk1-llama-13b-merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Sao10K/Stheno-Mix-L2-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Sao10K__Stheno-Mix-L2-20B</td>
      <td>45.76</td>
      <td>57.76</td>
      <td>79.63</td>
      <td>52.51</td>
      <td>51.80</td>
      <td>68.98</td>
      <td>0.08</td>
      <td>9.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>20.63</td>
      <td>0.0</td>
      <td>True</td>
      <td>6f9dcdaae6ef9071effe63d2107abe8b9712345b</td>
      <td>Sao10K/Stheno-Mix-L2-20B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lvkaokao/llama2-7b-hf-chat-lora-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lvkaokao__llama2-7b-hf-chat-lora-v3</td>
      <td>45.74</td>
      <td>57.25</td>
      <td>78.62</td>
      <td>50.57</td>
      <td>50.62</td>
      <td>76.32</td>
      <td>1.52</td>
      <td>5.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>79047f667253c878ad3143b016e3dcb3df707572</td>
      <td>lvkaokao/llama2-7b-hf-chat-lora-v3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/vicuna-shishya-7b-ep3-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__vicuna-shishya-7b-ep3-v1</td>
      <td>45.74</td>
      <td>45.90</td>
      <td>76.36</td>
      <td>50.04</td>
      <td>40.32</td>
      <td>71.74</td>
      <td>0.00</td>
      <td>35.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>082cf758aa3f6d8f956056003b5b3b6cde447d88</td>
      <td>luffycodes/vicuna-shishya-7b-ep3-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/totally-not-an-llm/EverythingLM-13b-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_totally-not-an-llm__EverythingLM-13b-16k</td>
      <td>45.70</td>
      <td>56.57</td>
      <td>80.58</td>
      <td>50.18</td>
      <td>47.46</td>
      <td>72.77</td>
      <td>6.44</td>
      <td>5.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>29.0</td>
      <td>True</td>
      <td>8456a856a8b115b05e76a7d0d945853b10ac71e2</td>
      <td>totally-not-an-llm/EverythingLM-13b-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/eachadea/vicuna-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_eachadea__vicuna-13b</td>
      <td>45.70</td>
      <td>51.71</td>
      <td>79.94</td>
      <td>50.84</td>
      <td>52.68</td>
      <td>71.03</td>
      <td>7.58</td>
      <td>6.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>95.0</td>
      <td>True</td>
      <td>ac4218770a58baaaaf25201076fe082abb6ffd13</td>
      <td>eachadea/vicuna-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/garage-bAInd/Platypus2-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_garage-bAInd__Platypus2-7B</td>
      <td>45.69</td>
      <td>55.20</td>
      <td>78.84</td>
      <td>49.83</td>
      <td>40.64</td>
      <td>73.48</td>
      <td>1.82</td>
      <td>20.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>f784afa7887b0738d92ea470797582756f02e630</td>
      <td>garage-bAInd/Platypus2-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LTC-AI-Labs/L2-7b-Beluga-WVG-Test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Beluga-WVG-Test</td>
      <td>45.68</td>
      <td>53.75</td>
      <td>78.38</td>
      <td>51.57</td>
      <td>45.76</td>
      <td>74.90</td>
      <td>7.88</td>
      <td>7.52</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>b90c207e248c0ad541274c2eb5ef76da1181802f</td>
      <td>LTC-AI-Labs/L2-7b-Beluga-WVG-Test</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/beaugogh/Llama2-7b-openorca-mc-v2-dpo</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beaugogh__Llama2-7b-openorca-mc-v2-dpo</td>
      <td>45.65</td>
      <td>54.78</td>
      <td>81.48</td>
      <td>47.20</td>
      <td>53.13</td>
      <td>72.85</td>
      <td>4.47</td>
      <td>5.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>734a6f0c69e1e53b988c107926bc17cb0536f851</td>
      <td>beaugogh/Llama2-7b-openorca-mc-v2-dpo</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mncai/Llama2-7B-guanaco-1k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mncai__Llama2-7B-guanaco-1k</td>
      <td>45.65</td>
      <td>55.12</td>
      <td>80.53</td>
      <td>47.93</td>
      <td>47.69</td>
      <td>74.82</td>
      <td>7.58</td>
      <td>5.87</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>5f3194b779897bbc4c4218a9dddc44a9b5faea15</td>
      <td>mncai/Llama2-7B-guanaco-1k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-7b-delta-v1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-7b-delta-v1.1</td>
      <td>45.60</td>
      <td>53.67</td>
      <td>77.50</td>
      <td>45.61</td>
      <td>48.95</td>
      <td>70.96</td>
      <td>5.53</td>
      <td>16.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>197.0</td>
      <td>True</td>
      <td>24fb8e1e9cc78e0aa7ef154b026c4a83296e3fc4</td>
      <td>lmsys/vicuna-7b-delta-v1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/eachadea/vicuna-7b-1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_eachadea__vicuna-7b-1.1</td>
      <td>45.60</td>
      <td>53.67</td>
      <td>77.46</td>
      <td>45.63</td>
      <td>48.94</td>
      <td>70.96</td>
      <td>5.53</td>
      <td>16.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>109.0</td>
      <td>True</td>
      <td>9d8eea215e00b388a22e8f050768ea8911d41f1d</td>
      <td>eachadea/vicuna-7b-1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Ejafa/vicuna_7B_vanilla_1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Ejafa__vicuna_7B_vanilla_1.1</td>
      <td>45.60</td>
      <td>53.67</td>
      <td>77.46</td>
      <td>45.63</td>
      <td>48.94</td>
      <td>70.96</td>
      <td>5.53</td>
      <td>16.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>d971d788db19648ad16bf77ec3f1de35ebf9a8e0</td>
      <td>Ejafa/vicuna_7B_vanilla_1.1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/Platypus2xOpenOrca-13B-LoRa-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__Platypus2xOpenOrca-13B-LoRa-v2</td>
      <td>45.59</td>
      <td>58.62</td>
      <td>81.17</td>
      <td>50.23</td>
      <td>43.43</td>
      <td>76.16</td>
      <td>0.08</td>
      <td>9.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>568ac6a5f1a9f5eb6bc09efb2188740d771ed0e9</td>
      <td>yeontaek/Platypus2xOpenOrca-13B-LoRa-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/l3utterfly/llama2-7b-layla</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_l3utterfly__llama2-7b-layla</td>
      <td>45.56</td>
      <td>54.18</td>
      <td>79.34</td>
      <td>49.70</td>
      <td>46.50</td>
      <td>74.11</td>
      <td>8.49</td>
      <td>6.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>733016abcd2abee63eb45ed63d2bba14b91da217</td>
      <td>l3utterfly/llama2-7b-layla</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/beaugogh/Llama2-7b-openorca-mc-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beaugogh__Llama2-7b-openorca-mc-v1</td>
      <td>45.55</td>
      <td>55.63</td>
      <td>80.17</td>
      <td>48.44</td>
      <td>51.62</td>
      <td>73.48</td>
      <td>4.09</td>
      <td>5.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>2c4096fa2129665fb127f1c2a1302f30565a5265</td>
      <td>beaugogh/Llama2-7b-openorca-mc-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/StudentLLM/Alpagasus-2-13B-QLoRA-pipeline</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_StudentLLM__Alpagasus-2-13B-QLoRA-pipeline</td>
      <td>45.55</td>
      <td>58.28</td>
      <td>80.98</td>
      <td>54.14</td>
      <td>34.21</td>
      <td>75.93</td>
      <td>9.25</td>
      <td>6.07</td>
      <td>?</td>
      <td>4bit</td>
      <td>other</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>False</td>
      <td>86329885e029c1f4fb6ff6b6f3409007158499e7</td>
      <td>StudentLLM/Alpagasus-2-13B-QLoRA-pipeline</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ</td>
      <td>45.54</td>
      <td>52.82</td>
      <td>79.63</td>
      <td>39.83</td>
      <td>52.55</td>
      <td>71.82</td>
      <td>0.15</td>
      <td>22.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>15.0</td>
      <td>True</td>
      <td>bd3c66e626c81de4977f197e1534bd3dfa2f569d</td>
      <td>TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lvkaokao/llama2-7b-hf-chat-lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lvkaokao__llama2-7b-hf-chat-lora</td>
      <td>45.53</td>
      <td>55.72</td>
      <td>78.75</td>
      <td>47.99</td>
      <td>43.11</td>
      <td>75.85</td>
      <td>10.77</td>
      <td>6.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e92a1439ac8d2edb5e311b8a42e13ed7c5e70db5</td>
      <td>lvkaokao/llama2-7b-hf-chat-lora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-13b-gpt4-1.3</td>
      <td>45.52</td>
      <td>58.53</td>
      <td>81.60</td>
      <td>46.96</td>
      <td>45.29</td>
      <td>75.85</td>
      <td>2.35</td>
      <td>8.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>32a474742c2a235ca12c96afaea57dcb6b46ef56</td>
      <td>jondurbin/airoboros-13b-gpt4-1.3</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Harshvir/Llama-2-7B-physics</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Harshvir__Llama-2-7B-physics</td>
      <td>45.44</td>
      <td>52.90</td>
      <td>77.71</td>
      <td>48.83</td>
      <td>48.93</td>
      <td>71.90</td>
      <td>7.05</td>
      <td>10.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>5e66b59c145586266b2351a63f0cf1b4f62f5454</td>
      <td>Harshvir/Llama-2-7B-physics</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardMath-7B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardMath-7B-V1.0</td>
      <td>45.44</td>
      <td>54.10</td>
      <td>79.55</td>
      <td>45.97</td>
      <td>43.65</td>
      <td>72.69</td>
      <td>2.73</td>
      <td>19.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>32.0</td>
      <td>True</td>
      <td>06dbd3e0da08255c575e585cb82e0554c1d2707a</td>
      <td>WizardLM/WizardMath-7B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lazycuber/L2-7b-Orca-WVG-Test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lazycuber__L2-7b-Orca-WVG-Test</td>
      <td>45.39</td>
      <td>54.86</td>
      <td>78.25</td>
      <td>51.13</td>
      <td>43.68</td>
      <td>74.35</td>
      <td>8.04</td>
      <td>7.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>6073a87872eb36149404bfb7d60e0108074ee1c3</td>
      <td>Lazycuber/L2-7b-Orca-WVG-Test</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/TheBloke/stable-vicuna-13B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__stable-vicuna-13B-HF</td>
      <td>45.36</td>
      <td>53.33</td>
      <td>78.50</td>
      <td>50.29</td>
      <td>48.38</td>
      <td>75.22</td>
      <td>4.09</td>
      <td>7.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>12.85</td>
      <td>95.0</td>
      <td>True</td>
      <td>2b099b2be0dafb2606ae9808c0f6183fe4bff7bc</td>
      <td>TheBloke/stable-vicuna-13B-HF</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-7b-chat-hf</td>
      <td>45.36</td>
      <td>52.90</td>
      <td>78.55</td>
      <td>48.32</td>
      <td>45.57</td>
      <td>71.74</td>
      <td>7.35</td>
      <td>13.09</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>1415.0</td>
      <td>False</td>
      <td>b7701a9e825e79a5ab18b5801be113c2160cc627</td>
      <td>meta-llama/Llama-2-7b-chat-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/synapsoft/Llama-2-7b-chat-hf-flan2022-1.2M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_synapsoft__Llama-2-7b-chat-hf-flan2022-1.2M</td>
      <td>45.35</td>
      <td>49.57</td>
      <td>76.25</td>
      <td>45.99</td>
      <td>42.17</td>
      <td>71.82</td>
      <td>1.52</td>
      <td>30.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>825506858e4603745a479215b8dea1524bfab6a0</td>
      <td>synapsoft/Llama-2-7b-chat-hf-flan2022-1.2M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LTC-AI-Labs/L2-7b-Base-test-WVG</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Base-test-WVG</td>
      <td>45.35</td>
      <td>54.27</td>
      <td>77.81</td>
      <td>51.07</td>
      <td>46.28</td>
      <td>73.56</td>
      <td>6.97</td>
      <td>7.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>2491546f1219c3e9bb1a8cf37fbecf0b299c2177</td>
      <td>LTC-AI-Labs/L2-7b-Base-test-WVG</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/davzoku/cria-llama2-7b-v1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_davzoku__cria-llama2-7b-v1.3</td>
      <td>45.34</td>
      <td>52.73</td>
      <td>78.58</td>
      <td>48.30</td>
      <td>45.58</td>
      <td>71.90</td>
      <td>8.49</td>
      <td>11.77</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>163a5bec7b6f5aaa4667aa6a95746deff50ceab1</td>
      <td>davzoku/cria-llama2-7b-v1.3</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/rombodawg/LosslessMegaCoder-llama2-7b-mini</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_rombodawg__LosslessMegaCoder-llama2-7b-mini</td>
      <td>45.33</td>
      <td>53.50</td>
      <td>77.38</td>
      <td>49.72</td>
      <td>45.77</td>
      <td>74.03</td>
      <td>9.55</td>
      <td>7.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>10.0</td>
      <td>True</td>
      <td>186b105d61054611d0b921a55c220d41c6aefe43</td>
      <td>rombodawg/LosslessMegaCoder-llama2-7b-mini</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/nkpz/llama2-22b-daydreamer-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nkpz__llama2-22b-daydreamer-v3</td>
      <td>45.31</td>
      <td>56.06</td>
      <td>80.07</td>
      <td>52.49</td>
      <td>42.43</td>
      <td>73.48</td>
      <td>3.79</td>
      <td>8.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>21.62</td>
      <td>11.0</td>
      <td>True</td>
      <td>e6c74222958328e50712aa00294dc818c24075b2</td>
      <td>nkpz/llama2-22b-daydreamer-v3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ashercn97/manatee-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ashercn97__manatee-7b</td>
      <td>45.29</td>
      <td>54.52</td>
      <td>78.95</td>
      <td>49.26</td>
      <td>46.77</td>
      <td>74.51</td>
      <td>7.05</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>e66094c43ffe6c5b3f4164cd4ba048d3bc422fd0</td>
      <td>ashercn97/manatee-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-mctaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mctaco</td>
      <td>45.28</td>
      <td>45.65</td>
      <td>75.65</td>
      <td>49.27</td>
      <td>43.12</td>
      <td>69.93</td>
      <td>4.47</td>
      <td>28.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>883b0fa4158de8207d0a94f4b8cb188e6250aa9d</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.1</td>
      <td>45.26</td>
      <td>54.61</td>
      <td>80.15</td>
      <td>39.25</td>
      <td>41.22</td>
      <td>73.09</td>
      <td>3.11</td>
      <td>25.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>5.0</td>
      <td>True</td>
      <td>5a45a16bac51ed9529a6dc2eab7355cc61eefb5b</td>
      <td>jondurbin/airoboros-7b-gpt4-1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-7b-v1.5-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-7b-v1.5-16k</td>
      <td>45.24</td>
      <td>54.69</td>
      <td>77.32</td>
      <td>49.51</td>
      <td>50.41</td>
      <td>71.11</td>
      <td>6.44</td>
      <td>7.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>55.0</td>
      <td>True</td>
      <td>9a93d7d11fac7f3f9074510b80092b53bc1a5bec</td>
      <td>lmsys/vicuna-7b-v1.5-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b-gpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4</td>
      <td>45.23</td>
      <td>53.07</td>
      <td>78.69</td>
      <td>38.90</td>
      <td>40.72</td>
      <td>73.09</td>
      <td>1.74</td>
      <td>30.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>4.0</td>
      <td>True</td>
      <td>d9bcb0ad365bfacdf95128bc1272b4106aff7be9</td>
      <td>jondurbin/airoboros-7b-gpt4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/airoboros-7b-gpt4-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__airoboros-7b-gpt4-fp16</td>
      <td>45.22</td>
      <td>53.07</td>
      <td>78.67</td>
      <td>38.88</td>
      <td>40.73</td>
      <td>73.09</td>
      <td>1.74</td>
      <td>30.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>14aa50fba9f6418c0d5e2d24087eb802931040ef</td>
      <td>TheBloke/airoboros-7b-gpt4-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-7b-v1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-7b-v1.3</td>
      <td>45.22</td>
      <td>50.43</td>
      <td>76.92</td>
      <td>48.14</td>
      <td>47.01</td>
      <td>70.48</td>
      <td>5.69</td>
      <td>17.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>112.0</td>
      <td>True</td>
      <td>ac066c83424c4a7221aa10c0ebe074b24d3bcdb6</td>
      <td>lmsys/vicuna-7b-v1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elliotthwang/Elliott-Chinese-LLaMa-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elliotthwang__Elliott-Chinese-LLaMa-GPTQ</td>
      <td>45.21</td>
      <td>51.02</td>
      <td>75.23</td>
      <td>49.58</td>
      <td>45.09</td>
      <td>72.61</td>
      <td>17.21</td>
      <td>5.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>53.90</td>
      <td>0.0</td>
      <td>True</td>
      <td>bbbca62bb340b4ae0a19ba93dae38fc9f9787c16</td>
      <td>elliotthwang/Elliott-Chinese-LLaMa-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/frank098/WizardLM_13B_juniper</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_frank098__WizardLM_13B_juniper</td>
      <td>45.16</td>
      <td>55.38</td>
      <td>77.20</td>
      <td>45.46</td>
      <td>51.50</td>
      <td>71.11</td>
      <td>8.04</td>
      <td>7.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>2204970fc0d96b071e2b1b003fbc5c87cfc46840</td>
      <td>frank098/WizardLM_13B_juniper</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/koala-13B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__koala-13B-HF</td>
      <td>45.16</td>
      <td>52.99</td>
      <td>77.59</td>
      <td>45.32</td>
      <td>50.23</td>
      <td>74.03</td>
      <td>6.82</td>
      <td>9.11</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>40.0</td>
      <td>True</td>
      <td>b20f96a0171ce4c0fa27d6048215ebe710521587</td>
      <td>TheBloke/koala-13B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/vicuna-7b-v1.5-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__vicuna-7b-v1.5-16k</td>
      <td>45.11</td>
      <td>54.18</td>
      <td>77.31</td>
      <td>49.30</td>
      <td>50.35</td>
      <td>71.03</td>
      <td>6.37</td>
      <td>7.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>55.0</td>
      <td>True</td>
      <td>9a93d7d11fac7f3f9074510b80092b53bc1a5bec</td>
      <td>lmsys/vicuna-7b-v1.5-16k</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-7b-2.2.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-7b-2.2.1</td>
      <td>45.10</td>
      <td>55.03</td>
      <td>80.06</td>
      <td>47.64</td>
      <td>44.65</td>
      <td>73.80</td>
      <td>6.14</td>
      <td>8.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>eafbba6fec094a17ca7bce6d9605cac97b90a483</td>
      <td>jondurbin/airoboros-l2-7b-2.2.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/haoranxu/ALMA-13B-Pretrain</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_haoranxu__ALMA-13B-Pretrain</td>
      <td>45.10</td>
      <td>56.91</td>
      <td>80.15</td>
      <td>50.31</td>
      <td>37.44</td>
      <td>76.40</td>
      <td>8.87</td>
      <td>5.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>69e9e12d8bab66dffdcb15fa534fc3f0dc34acec</td>
      <td>haoranxu/ALMA-13B-Pretrain</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/golaxy/gowizardlm</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gowizardlm</td>
      <td>45.10</td>
      <td>49.74</td>
      <td>71.90</td>
      <td>42.96</td>
      <td>47.66</td>
      <td>69.61</td>
      <td>3.94</td>
      <td>29.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.76</td>
      <td>0.0</td>
      <td>True</td>
      <td>385f2d164e7fe780e053276d95d36240f2368c21</td>
      <td>golaxy/gowizardlm</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/facebook/opt-iml-max-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-iml-max-30b</td>
      <td>45.06</td>
      <td>43.86</td>
      <td>72.39</td>
      <td>41.09</td>
      <td>38.16</td>
      <td>73.72</td>
      <td>2.50</td>
      <td>43.70</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>29.98</td>
      <td>34.0</td>
      <td>True</td>
      <td>291753b04817a31a742631053ee361874d6db8a4</td>
      <td>facebook/opt-iml-max-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-dpo</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-dpo</td>
      <td>45.06</td>
      <td>53.67</td>
      <td>78.79</td>
      <td>46.78</td>
      <td>43.97</td>
      <td>71.74</td>
      <td>7.35</td>
      <td>13.09</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>ec98429034fc84a4555dd4e3db4d6af534a03832</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-dpo</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-openllama-13b-v7-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-openllama-13b-v7-fp16</td>
      <td>45.05</td>
      <td>47.61</td>
      <td>72.24</td>
      <td>47.74</td>
      <td>48.73</td>
      <td>69.69</td>
      <td>9.86</td>
      <td>19.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.89</td>
      <td>3.0</td>
      <td>True</td>
      <td>8690c065bccd3e897ccbf3d8aa24b0216a6f5dba</td>
      <td>OpenBuddy/openbuddy-openllama-13b-v7-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lajonbot/vicuna-7b-v1.5-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lajonbot__vicuna-7b-v1.5-PL-lora_unload</td>
      <td>45.05</td>
      <td>53.50</td>
      <td>76.74</td>
      <td>49.69</td>
      <td>49.68</td>
      <td>71.98</td>
      <td>7.20</td>
      <td>6.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>92bf763ce7ae0bfe155bfd60190eed64582e5080</td>
      <td>Lajonbot/vicuna-7b-v1.5-PL-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/camel-ai/CAMEL-13B-Role-Playing-Data</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_camel-ai__CAMEL-13B-Role-Playing-Data</td>
      <td>45.03</td>
      <td>54.95</td>
      <td>79.25</td>
      <td>46.61</td>
      <td>46.35</td>
      <td>74.03</td>
      <td>7.35</td>
      <td>6.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>12.0</td>
      <td>True</td>
      <td>762ecb0d85572c8f8bcbca06d27f7f64a4d74615</td>
      <td>camel-ai/CAMEL-13B-Role-Playing-Data</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/medalpaca/medalpaca-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_medalpaca__medalpaca-7b</td>
      <td>44.98</td>
      <td>54.10</td>
      <td>80.42</td>
      <td>41.47</td>
      <td>40.46</td>
      <td>71.19</td>
      <td>3.03</td>
      <td>24.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc</td>
      <td>6.61</td>
      <td>30.0</td>
      <td>True</td>
      <td>b57b9f5ff34059e485b769973d023021fc66a8f7</td>
      <td>medalpaca/medalpaca-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NewstaR/Koss-7B-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NewstaR__Koss-7B-chat</td>
      <td>44.98</td>
      <td>53.67</td>
      <td>78.79</td>
      <td>46.72</td>
      <td>43.97</td>
      <td>71.74</td>
      <td>7.35</td>
      <td>12.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-4.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1ab836d9ebf7029fafa07949b51d3838501d537</td>
      <td>NewstaR/Koss-7B-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LTC-AI-Labs/L2-7b-Hermes-WVG-Test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Hermes-WVG-Test</td>
      <td>44.97</td>
      <td>54.95</td>
      <td>78.48</td>
      <td>48.36</td>
      <td>45.72</td>
      <td>74.74</td>
      <td>5.84</td>
      <td>6.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>eb5b1d65fdf916ca71f89a46eb91175c1c630a57</td>
      <td>LTC-AI-Labs/L2-7b-Hermes-WVG-Test</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LTC-AI-Labs/L2-7b-Synthia-WVG-Test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Synthia-WVG-Test</td>
      <td>44.95</td>
      <td>55.97</td>
      <td>77.89</td>
      <td>49.48</td>
      <td>44.11</td>
      <td>74.11</td>
      <td>5.91</td>
      <td>7.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>23ae02efba01c37abe3cff0fedc7d2d9644fe98e</td>
      <td>LTC-AI-Labs/L2-7b-Synthia-WVG-Test</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/psyche/kollama2-7b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psyche__kollama2-7b-v2</td>
      <td>44.91</td>
      <td>53.33</td>
      <td>78.50</td>
      <td>43.61</td>
      <td>46.37</td>
      <td>75.61</td>
      <td>6.52</td>
      <td>10.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>d5b6e9d5b882d4f6ba322396e027925ed915f848</td>
      <td>psyche/kollama2-7b-v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/guardrail/llama-2-7b-guanaco-instruct-sharded</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_guardrail__llama-2-7b-guanaco-instruct-sharded</td>
      <td>44.87</td>
      <td>53.75</td>
      <td>78.69</td>
      <td>46.65</td>
      <td>43.93</td>
      <td>72.61</td>
      <td>7.81</td>
      <td>10.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.74</td>
      <td>4.0</td>
      <td>True</td>
      <td>fc7a3abbc3b9a9b3e163ef3c4844307ac270fca7</td>
      <td>guardrail/llama-2-7b-guanaco-instruct-sharded</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llama-13b</td>
      <td>44.83</td>
      <td>56.23</td>
      <td>80.93</td>
      <td>47.67</td>
      <td>39.48</td>
      <td>76.24</td>
      <td>7.58</td>
      <td>5.66</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>False</td>
      <td>4022c52fcc7473ce7364bb5ac166195903ea1efb</td>
      <td>huggingface/llama-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/totally-not-an-llm/EverythingLM-13b-V3-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_totally-not-an-llm__EverythingLM-13b-V3-16k</td>
      <td>44.82</td>
      <td>58.19</td>
      <td>80.12</td>
      <td>50.48</td>
      <td>45.18</td>
      <td>70.72</td>
      <td>1.97</td>
      <td>7.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>1de9244bfadb947f80872727f76790cbc76e7142</td>
      <td>totally-not-an-llm/EverythingLM-13b-V3-16k</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/huggyllama/llama-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huggyllama__llama-13b</td>
      <td>44.81</td>
      <td>56.14</td>
      <td>80.92</td>
      <td>47.61</td>
      <td>39.48</td>
      <td>76.24</td>
      <td>7.58</td>
      <td>5.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>13.02</td>
      <td>119.0</td>
      <td>True</td>
      <td>bf57045473f207bb1de1ed035ace226f4d9f9bba</td>
      <td>huggyllama/llama-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/AlekseyKorshuk/vic15-exp-syn-fight-cp3838</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AlekseyKorshuk__vic15-exp-syn-fight-cp3838</td>
      <td>44.80</td>
      <td>51.79</td>
      <td>75.79</td>
      <td>50.23</td>
      <td>49.61</td>
      <td>71.82</td>
      <td>6.60</td>
      <td>7.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>91ce25dbdb67793ad1fcfdfd59f7603c2be65aea</td>
      <td>AlekseyKorshuk/vic15-exp-syn-fight-cp3838</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jordiclive/gpt4all-alpaca-oa-codealpaca-lora-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jordiclive__gpt4all-alpaca-oa-codealpaca-lora-13b</td>
      <td>44.80</td>
      <td>56.14</td>
      <td>80.93</td>
      <td>47.66</td>
      <td>39.48</td>
      <td>76.16</td>
      <td>7.58</td>
      <td>5.64</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>13.00</td>
      <td>11.0</td>
      <td>False</td>
      <td>13443d633eaa5b7e1a90ac9cdb4a4d51b1c8d0d1</td>
      <td>jordiclive/gpt4all-alpaca-oa-codealpaca-lora-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/Pygmalion-13b-Merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Pygmalion-13b-Merged</td>
      <td>44.80</td>
      <td>56.48</td>
      <td>80.02</td>
      <td>42.93</td>
      <td>35.86</td>
      <td>75.53</td>
      <td>0.08</td>
      <td>22.67</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>26.0</td>
      <td>True</td>
      <td>f96308083033c84db47b6c093da3817c085c87c7</td>
      <td>TehVenom/Pygmalion-13b-Merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Norquinal/llama-2-7b-claude-chat-rp</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Norquinal__llama-2-7b-claude-chat-rp</td>
      <td>44.78</td>
      <td>54.95</td>
      <td>80.05</td>
      <td>47.03</td>
      <td>43.47</td>
      <td>74.74</td>
      <td>7.28</td>
      <td>5.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>4309eedebe8ba5709e0cc7cf186cb783f3bc8060</td>
      <td>Norquinal/llama-2-7b-claude-chat-rp</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-7B-Uncensored-HF</td>
      <td>44.77</td>
      <td>53.41</td>
      <td>78.85</td>
      <td>37.09</td>
      <td>43.48</td>
      <td>72.22</td>
      <td>4.55</td>
      <td>23.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>19.0</td>
      <td>True</td>
      <td>b802f1b4401d0b2242137160c20cc11b9ffd3a4c</td>
      <td>TheBloke/Wizard-Vicuna-7B-Uncensored-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Wizard-Vicuna-7B-Uncensored</td>
      <td>44.77</td>
      <td>53.41</td>
      <td>78.85</td>
      <td>37.09</td>
      <td>43.48</td>
      <td>72.22</td>
      <td>4.55</td>
      <td>23.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>63.0</td>
      <td>True</td>
      <td>1097285acd9c48a1d09bc0a9844d365384732111</td>
      <td>ehartford/Wizard-Vicuna-7B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Danielbrdz/Barcenas-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Danielbrdz__Barcenas-7b</td>
      <td>44.77</td>
      <td>55.12</td>
      <td>77.40</td>
      <td>49.27</td>
      <td>43.64</td>
      <td>73.64</td>
      <td>6.14</td>
      <td>8.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>770fa73981a599e935c21a95b1817a553c726694</td>
      <td>Danielbrdz/Barcenas-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/maximuslee07/llama-2-7b-rockwell-final</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_maximuslee07__llama-2-7b-rockwell-final</td>
      <td>44.77</td>
      <td>52.73</td>
      <td>79.10</td>
      <td>47.88</td>
      <td>47.21</td>
      <td>68.43</td>
      <td>7.96</td>
      <td>10.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>de4cfe99e9e3db62733b40f48b2b11faf9abe4bf</td>
      <td>maximuslee07/llama-2-7b-rockwell-final</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/davzoku/cria-llama2-7b-v1.3_peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_davzoku__cria-llama2-7b-v1.3_peft</td>
      <td>44.74</td>
      <td>51.45</td>
      <td>77.35</td>
      <td>46.47</td>
      <td>45.52</td>
      <td>70.80</td>
      <td>6.75</td>
      <td>14.82</td>
      <td>?</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6864fa8ee43fa4d6b4f3ae055bbf464a5dcca570</td>
      <td>davzoku/cria-llama2-7b-v1.3_peft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bofenghuang/vigogne-7b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bofenghuang__vigogne-7b-instruct</td>
      <td>44.70</td>
      <td>51.96</td>
      <td>78.11</td>
      <td>38.43</td>
      <td>42.47</td>
      <td>72.85</td>
      <td>2.73</td>
      <td>26.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>6.61</td>
      <td>21.0</td>
      <td>True</td>
      <td>c6e2f515a0b289478118b5b75ff74107002ad962</td>
      <td>bofenghuang/vigogne-7b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/speechlessai/speechless-codellama-dolphin-orca-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_speechlessai__speechless-codellama-dolphin-orca-platypus-13b</td>
      <td>44.67</td>
      <td>45.82</td>
      <td>67.71</td>
      <td>45.88</td>
      <td>44.67</td>
      <td>65.35</td>
      <td>8.49</td>
      <td>34.77</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>25e1c346c2a01588a728307d5c35fbeecd58b51b</td>
      <td>speechlessai/speechless-codellama-dolphin-orca-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/pygmalion-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__pygmalion-2-7b</td>
      <td>44.66</td>
      <td>54.01</td>
      <td>78.23</td>
      <td>49.11</td>
      <td>43.78</td>
      <td>75.14</td>
      <td>6.37</td>
      <td>5.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>25.0</td>
      <td>True</td>
      <td>983f8ad5c156f4a0e4d2b7b5f1146981ad2e8a8b</td>
      <td>PygmalionAI/pygmalion-2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CalderaAI/13B-Ouroboros</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CalderaAI__13B-Ouroboros</td>
      <td>44.66</td>
      <td>57.42</td>
      <td>82.11</td>
      <td>51.43</td>
      <td>47.99</td>
      <td>57.85</td>
      <td>0.45</td>
      <td>15.36</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>97981254d4b0ac0d1472376f602c004670070fdd</td>
      <td>CalderaAI/13B-Ouroboros</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/keyfan/vicuna-chinese-replication-v1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_keyfan__vicuna-chinese-replication-v1.1</td>
      <td>44.65</td>
      <td>42.83</td>
      <td>71.47</td>
      <td>47.47</td>
      <td>47.24</td>
      <td>67.40</td>
      <td>9.48</td>
      <td>26.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.94</td>
      <td>4.0</td>
      <td>True</td>
      <td>259ab0967975012a546f2362d6cd03ab10768157</td>
      <td>keyfan/vicuna-chinese-replication-v1.1</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/ehartford/Samantha-1.11-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__Samantha-1.11-7b</td>
      <td>44.65</td>
      <td>55.03</td>
      <td>79.12</td>
      <td>40.51</td>
      <td>50.37</td>
      <td>74.19</td>
      <td>7.20</td>
      <td>6.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>7.0</td>
      <td>True</td>
      <td>730cbd8f3077f3d24001aab714def991f1e4e7e8</td>
      <td>ehartford/Samantha-1.11-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/beaugogh/Llama2-7b-sharegpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beaugogh__Llama2-7b-sharegpt4</td>
      <td>44.64</td>
      <td>55.72</td>
      <td>80.94</td>
      <td>47.47</td>
      <td>48.34</td>
      <td>71.19</td>
      <td>2.65</td>
      <td>6.14</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>False</td>
      <td>922d1d963ad1b042c30b774a818d9f6180c28075</td>
      <td>beaugogh/Llama2-7b-sharegpt4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HWERI/Llama2-7b-sharegpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HWERI__Llama2-7b-sharegpt4</td>
      <td>44.64</td>
      <td>55.72</td>
      <td>80.94</td>
      <td>47.47</td>
      <td>48.34</td>
      <td>71.19</td>
      <td>2.65</td>
      <td>6.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>8ecaba5dd0e9929f5858cfe9f5f8cd8ba285c9e5</td>
      <td>HWERI/Llama2-7b-sharegpt4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikael110/llama-2-7b-guanaco-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikael110__llama-2-7b-guanaco-fp16</td>
      <td>44.60</td>
      <td>54.86</td>
      <td>79.65</td>
      <td>46.38</td>
      <td>43.83</td>
      <td>75.22</td>
      <td>6.29</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>9.0</td>
      <td>True</td>
      <td>f769fed10874af73ad12115efd044cb4a64506b0</td>
      <td>Mikael110/llama-2-7b-guanaco-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-7b-gpt4-m2.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-7b-gpt4-m2.0</td>
      <td>44.60</td>
      <td>50.51</td>
      <td>76.87</td>
      <td>45.35</td>
      <td>41.34</td>
      <td>69.53</td>
      <td>4.09</td>
      <td>24.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>9.0</td>
      <td>True</td>
      <td>67729407add902e3d4d36bb105d7c011fb368ea5</td>
      <td>jondurbin/airoboros-l2-7b-gpt4-m2.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Delcos/Mistral-Pygmalion-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Delcos__Mistral-Pygmalion-7b</td>
      <td>44.58</td>
      <td>54.44</td>
      <td>78.48</td>
      <td>49.23</td>
      <td>41.82</td>
      <td>75.30</td>
      <td>6.82</td>
      <td>5.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>4e5fa9ae7f572b4841b02c3f96d8a3c7a7e59521</td>
      <td>Delcos/Mistral-Pygmalion-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mncai/chatdoctor</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mncai__chatdoctor</td>
      <td>44.56</td>
      <td>53.75</td>
      <td>78.54</td>
      <td>35.95</td>
      <td>43.55</td>
      <td>69.93</td>
      <td>0.00</td>
      <td>30.17</td>
      <td>LLaMAForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>8fdcfdda6877d7f21173dfac48b2c14499ba8264</td>
      <td>mncai/chatdoctor</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/joehuangx/spatial-vicuna-7b-v1.5-LoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_joehuangx__spatial-vicuna-7b-v1.5-LoRA</td>
      <td>44.55</td>
      <td>50.77</td>
      <td>74.63</td>
      <td>48.13</td>
      <td>49.36</td>
      <td>72.38</td>
      <td>6.90</td>
      <td>9.72</td>
      <td>?</td>
      <td>4bit</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>dc71924cfb214b91461d35178e6ea6fef7946f13</td>
      <td>joehuangx/spatial-vicuna-7b-v1.5-LoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Norquinal/llama-2-7b-claude-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Norquinal__llama-2-7b-claude-chat</td>
      <td>44.54</td>
      <td>54.44</td>
      <td>80.66</td>
      <td>46.74</td>
      <td>41.39</td>
      <td>74.90</td>
      <td>7.73</td>
      <td>5.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e65d34ed31cdcd2637f6284aa0605f30ef5a9381</td>
      <td>Norquinal/llama-2-7b-claude-chat</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PeanutJar/LLaMa-2-PeanutButter_v18_B-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PeanutJar__LLaMa-2-PeanutButter_v18_B-7B</td>
      <td>44.53</td>
      <td>54.61</td>
      <td>81.00</td>
      <td>47.07</td>
      <td>41.93</td>
      <td>74.51</td>
      <td>6.52</td>
      <td>6.10</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>bc8c239cacf1e3211f05e27be67a74d84c12aea9</td>
      <td>PeanutJar/LLaMa-2-PeanutButter_v18_B-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dfurman/llama-2-7b-instruct-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dfurman__llama-2-7b-instruct-peft</td>
      <td>44.50</td>
      <td>51.19</td>
      <td>78.92</td>
      <td>46.63</td>
      <td>48.50</td>
      <td>74.43</td>
      <td>5.99</td>
      <td>5.82</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>unknown</td>
      <td>7.00</td>
      <td>1.0</td>
      <td>False</td>
      <td>0fc43413117187e0723cdac133068ab527c80fe2</td>
      <td>dfurman/llama-2-7b-instruct-peft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-timedial-unit-080082</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-timedial-unit-080082</td>
      <td>44.47</td>
      <td>52.82</td>
      <td>76.07</td>
      <td>50.47</td>
      <td>43.54</td>
      <td>73.72</td>
      <td>7.81</td>
      <td>6.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>372c90543ebb2a317fb9b51ff3890cc270e5ce3a</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-timedial-unit-080082</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/gogpt2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gogpt2-13b</td>
      <td>44.47</td>
      <td>48.38</td>
      <td>71.78</td>
      <td>44.50</td>
      <td>44.73</td>
      <td>67.88</td>
      <td>2.05</td>
      <td>32.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>13.04</td>
      <td>4.0</td>
      <td>True</td>
      <td>16d4c4214fa8d5a962b9064a8b958076b7c79a17</td>
      <td>golaxy/gogpt2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sia-ai/llama-2-7b-1-percent-open-orca-1000-steps-v0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sia-ai__llama-2-7b-1-percent-open-orca-1000-steps-v0</td>
      <td>44.47</td>
      <td>51.28</td>
      <td>78.75</td>
      <td>44.68</td>
      <td>45.83</td>
      <td>74.11</td>
      <td>2.73</td>
      <td>13.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>a893ebef4b818de1968dd9e932da2f513d16386a</td>
      <td>sia-ai/llama-2-7b-1-percent-open-orca-1000-steps-v0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/gogpt2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gogpt2-7b</td>
      <td>44.46</td>
      <td>46.76</td>
      <td>71.53</td>
      <td>42.85</td>
      <td>47.85</td>
      <td>68.67</td>
      <td>2.27</td>
      <td>31.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.76</td>
      <td>5.0</td>
      <td>True</td>
      <td>ee60ed402dedf24b6154aef05df54512e02fc9e2</td>
      <td>golaxy/gogpt2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/gogpt-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gogpt-7b</td>
      <td>44.44</td>
      <td>48.81</td>
      <td>73.79</td>
      <td>43.03</td>
      <td>41.00</td>
      <td>69.77</td>
      <td>1.90</td>
      <td>32.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.76</td>
      <td>2.0</td>
      <td>True</td>
      <td>7eb70c0e330b7d3ff490047ddbb153bb96294882</td>
      <td>golaxy/gogpt-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-timedial-unit-080091</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-timedial-unit-080091</td>
      <td>44.44</td>
      <td>52.82</td>
      <td>76.10</td>
      <td>50.58</td>
      <td>43.40</td>
      <td>73.72</td>
      <td>7.66</td>
      <td>6.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>ae7e0fb58f4201bb14fd4e641d0d6dcc22674e0e</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-timedial-unit-080091</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-orca-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-orca-13b</td>
      <td>44.43</td>
      <td>46.33</td>
      <td>67.71</td>
      <td>47.19</td>
      <td>46.66</td>
      <td>63.77</td>
      <td>5.99</td>
      <td>33.34</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>a82467de3cb9438aa8f9e0ea8ea692f16a5724b2</td>
      <td>uukuguy/speechless-codellama-orca-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama2-13b-pretrain</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama2-13b-pretrain</td>
      <td>44.41</td>
      <td>53.92</td>
      <td>79.10</td>
      <td>51.25</td>
      <td>36.24</td>
      <td>75.53</td>
      <td>8.57</td>
      <td>6.27</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.97</td>
      <td>0.0</td>
      <td>True</td>
      <td>f87d66f9c4541c575a6fad3c19a31b11568e0dfb</td>
      <td>YeungNLP/firefly-llama2-13b-pretrain</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-dolphin-orca-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-dolphin-orca-platypus-13b</td>
      <td>44.41</td>
      <td>44.80</td>
      <td>68.60</td>
      <td>44.03</td>
      <td>46.28</td>
      <td>66.93</td>
      <td>9.55</td>
      <td>30.68</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>0c41023f8f665946a2c46c3823afee431408bcbd</td>
      <td>uukuguy/speechless-codellama-dolphin-orca-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/revolutionarybukhari/Llama-2-7b-chat-finetune-AUTOMATE</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_revolutionarybukhari__Llama-2-7b-chat-finetune-AUTOMATE</td>
      <td>44.41</td>
      <td>53.07</td>
      <td>75.59</td>
      <td>48.80</td>
      <td>44.73</td>
      <td>73.24</td>
      <td>8.64</td>
      <td>6.77</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>55862462a23ab43fb73d4c784f1518ab4645764c</td>
      <td>revolutionarybukhari/Llama-2-7b-chat-finetune-AUTOMATE</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/edor/Platypus2-mini-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_edor__Platypus2-mini-7B</td>
      <td>44.38</td>
      <td>53.33</td>
      <td>78.81</td>
      <td>45.58</td>
      <td>42.00</td>
      <td>75.14</td>
      <td>6.22</td>
      <td>9.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>4ede4a6f8a8d6cc3bfff8b98837116c74c280f63</td>
      <td>edor/Platypus2-mini-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LTC-AI-Labs/Guanaco-Vicuna-7B-L2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LTC-AI-Labs__Guanaco-Vicuna-7B-L2</td>
      <td>44.38</td>
      <td>53.24</td>
      <td>78.89</td>
      <td>46.77</td>
      <td>42.75</td>
      <td>75.37</td>
      <td>7.96</td>
      <td>5.68</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba8e755feab0bbf90675dcb9f8875a42f92112a5</td>
      <td>LTC-AI-Labs/Guanaco-Vicuna-7B-L2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PeanutJar/LLaMa-2-PeanutButter_v10-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PeanutJar__LLaMa-2-PeanutButter_v10-7B</td>
      <td>44.35</td>
      <td>55.29</td>
      <td>81.69</td>
      <td>46.97</td>
      <td>43.78</td>
      <td>70.88</td>
      <td>5.91</td>
      <td>5.93</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f98bb987216448aa3aa89e575a7494fae8b68066</td>
      <td>PeanutJar/LLaMa-2-PeanutButter_v10-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/Pygmalion_AlpacaLora-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Pygmalion_AlpacaLora-7b</td>
      <td>44.27</td>
      <td>53.24</td>
      <td>76.92</td>
      <td>35.92</td>
      <td>39.44</td>
      <td>72.22</td>
      <td>1.21</td>
      <td>30.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>1f61442e1238062095b31b4909c5e9ab26105794</td>
      <td>TehVenom/Pygmalion_AlpacaLora-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wahaha1987/llama_7b_sharegpt94k_fastchat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wahaha1987__llama_7b_sharegpt94k_fastchat</td>
      <td>44.25</td>
      <td>53.24</td>
      <td>76.94</td>
      <td>44.64</td>
      <td>45.34</td>
      <td>70.64</td>
      <td>4.32</td>
      <td>14.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>2d82abff150b7a5ae484f9cd7c64c72fd4eaf7f5</td>
      <td>wahaha1987/llama_7b_sharegpt94k_fastchat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_v2_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_v2_7b</td>
      <td>44.24</td>
      <td>50.77</td>
      <td>76.02</td>
      <td>39.50</td>
      <td>43.86</td>
      <td>71.43</td>
      <td>2.88</td>
      <td>25.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.61</td>
      <td>34.0</td>
      <td>True</td>
      <td>165850882991d7fa4eabab577a03ed84e0713bfa</td>
      <td>psmathur/orca_mini_v2_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jxhong/CAlign-alpaca-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jxhong__CAlign-alpaca-7b</td>
      <td>44.22</td>
      <td>50.94</td>
      <td>74.55</td>
      <td>38.56</td>
      <td>46.89</td>
      <td>72.06</td>
      <td>1.36</td>
      <td>25.15</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>f5cc642a10160a014e2afeefcd57d4781994c51e</td>
      <td>jxhong/CAlign-alpaca-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LTC-AI-Labs/L2-7b-Base-WVG-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LTC-AI-Labs__L2-7b-Base-WVG-Uncensored</td>
      <td>44.20</td>
      <td>53.24</td>
      <td>79.13</td>
      <td>46.65</td>
      <td>42.59</td>
      <td>75.14</td>
      <td>7.05</td>
      <td>5.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>67ede9be6ceffdf574294351cca937d88d7d448d</td>
      <td>LTC-AI-Labs/L2-7b-Base-WVG-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lazycuber/L2-7b-Guanaco-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lazycuber__L2-7b-Guanaco-Uncensored</td>
      <td>44.14</td>
      <td>50.60</td>
      <td>76.99</td>
      <td>48.93</td>
      <td>43.42</td>
      <td>75.37</td>
      <td>7.96</td>
      <td>5.68</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>False</td>
      <td>9d49378c69c00113cf7f6e66d1ddb9d9b003dddc</td>
      <td>Lazycuber/L2-7b-Guanaco-Uncensored</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</td>
      <td>44.13</td>
      <td>53.67</td>
      <td>78.21</td>
      <td>45.90</td>
      <td>46.13</td>
      <td>73.80</td>
      <td>4.70</td>
      <td>6.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>1295069e9fef63aed87d36fe108d6c934cb34ded</td>
      <td>dhmeltzer/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-timedial</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-timedial</td>
      <td>44.13</td>
      <td>52.90</td>
      <td>76.29</td>
      <td>50.47</td>
      <td>41.60</td>
      <td>73.56</td>
      <td>7.28</td>
      <td>6.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1e1709818cca48af4cd31c07c493f996854aa10f</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-timedial</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lazycuber/L2-7b-Base-Guanaco-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lazycuber__L2-7b-Base-Guanaco-Uncensored</td>
      <td>44.06</td>
      <td>52.22</td>
      <td>79.08</td>
      <td>46.63</td>
      <td>42.97</td>
      <td>74.51</td>
      <td>7.28</td>
      <td>5.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>dd51a3b26ad378e2953c947a1e4c2f8febe0cb52</td>
      <td>Lazycuber/L2-7b-Base-Guanaco-Uncensored</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/dhmeltzer/llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16</td>
      <td>44.03</td>
      <td>54.10</td>
      <td>78.74</td>
      <td>45.44</td>
      <td>43.40</td>
      <td>73.64</td>
      <td>4.55</td>
      <td>8.35</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f1f3b9fdb1e2d8d8fa913d57a8fe15d7bdf72c20</td>
      <td>dhmeltzer/llama-7b-SFT-qlora-eli5-wiki_DPO_ds_RM_top_2_1024_r_64_alpha_16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/llama-shishya-7b-ep3-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__llama-shishya-7b-ep3-v1</td>
      <td>44.02</td>
      <td>48.04</td>
      <td>76.63</td>
      <td>46.12</td>
      <td>30.90</td>
      <td>69.46</td>
      <td>0.00</td>
      <td>37.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>8dc109f45ef36cc7bbd0f5d83fb65ac8e768d1bd</td>
      <td>luffycodes/llama-shishya-7b-ep3-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b</td>
      <td>44.01</td>
      <td>53.07</td>
      <td>77.65</td>
      <td>37.23</td>
      <td>43.39</td>
      <td>70.96</td>
      <td>2.12</td>
      <td>23.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>14.0</td>
      <td>True</td>
      <td>7ea67f85ff3a7a8ec77f1819dec3e56779b764b1</td>
      <td>jondurbin/airoboros-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/quantumaikr/KoreanLM-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__KoreanLM-hf</td>
      <td>43.99</td>
      <td>51.45</td>
      <td>76.77</td>
      <td>40.61</td>
      <td>44.34</td>
      <td>69.77</td>
      <td>3.41</td>
      <td>21.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>a7261e7ae6ee76c78e1ba1ac8c59bcc3e0868bf9</td>
      <td>quantumaikr/KoreanLM-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</td>
      <td>43.96</td>
      <td>53.75</td>
      <td>78.76</td>
      <td>46.02</td>
      <td>43.31</td>
      <td>73.48</td>
      <td>4.70</td>
      <td>7.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ba5416f618ed3e11b409326e84c36fa542f0951</td>
      <td>dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Monero/WizardLM-13b-OpenAssistant-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Monero__WizardLM-13b-OpenAssistant-Uncensored</td>
      <td>43.91</td>
      <td>48.55</td>
      <td>76.03</td>
      <td>43.15</td>
      <td>49.40</td>
      <td>69.77</td>
      <td>3.03</td>
      <td>17.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>ff8e15fd68119d36ae1f0cebaa87f16e2ad3c732</td>
      <td>Monero/WizardLM-13b-OpenAssistant-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/willnguyen/lacda-2-7B-chat-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_willnguyen__lacda-2-7B-chat-v0.1</td>
      <td>43.91</td>
      <td>53.07</td>
      <td>77.57</td>
      <td>46.03</td>
      <td>44.57</td>
      <td>74.19</td>
      <td>6.29</td>
      <td>5.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>afca346816726b83e331bb4d93246ed5146e1675</td>
      <td>willnguyen/lacda-2-7B-chat-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/Llama-2-7b-hf-instruct-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__Llama-2-7b-hf-instruct-pl-lora_unload</td>
      <td>43.85</td>
      <td>53.75</td>
      <td>78.34</td>
      <td>46.80</td>
      <td>42.34</td>
      <td>73.95</td>
      <td>6.22</td>
      <td>5.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>3dfef350be9c8ce92c2d314dbe96a002bd6ca97d</td>
      <td>Aspik101/Llama-2-7b-hf-instruct-pl-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-hf-guanaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-hf-guanaco</td>
      <td>43.83</td>
      <td>52.47</td>
      <td>78.75</td>
      <td>45.33</td>
      <td>43.90</td>
      <td>74.19</td>
      <td>6.07</td>
      <td>6.11</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>6c1fc95e67b11f1011a3b2fc1aa05c7b83251e40</td>
      <td>TheTravellingEngineer/llama2-7b-hf-guanaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/conceptofmind/LLongMA-2-13b-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_conceptofmind__LLongMA-2-13b-16k</td>
      <td>43.83</td>
      <td>54.27</td>
      <td>79.63</td>
      <td>50.97</td>
      <td>37.71</td>
      <td>72.77</td>
      <td>5.99</td>
      <td>5.47</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c2defe28e2f3f10460baf8f778b00986a53aa7a2</td>
      <td>conceptofmind/LLongMA-2-13b-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elliotthwang/elliott_Llama-2-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elliotthwang__elliott_Llama-2-7b-hf</td>
      <td>43.83</td>
      <td>53.16</td>
      <td>78.33</td>
      <td>47.09</td>
      <td>42.11</td>
      <td>73.64</td>
      <td>6.90</td>
      <td>5.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>ac5d22e14c2c7a400519da5d12d88e4fe683ccfa</td>
      <td>elliotthwang/elliott_Llama-2-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__Llama-2-7B-32K-Instruct</td>
      <td>43.82</td>
      <td>51.11</td>
      <td>78.51</td>
      <td>46.11</td>
      <td>44.86</td>
      <td>73.88</td>
      <td>5.69</td>
      <td>6.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>111.0</td>
      <td>True</td>
      <td>35696b9a7ab330dcbe240ff76fb44ab1eccf45bf</td>
      <td>togethercomputer/Llama-2-7B-32K-Instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/kashif/stack-llama-2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_kashif__stack-llama-2</td>
      <td>43.81</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>10.01</td>
      <td>5.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-openrail-m</td>
      <td>6.61</td>
      <td>7.0</td>
      <td>True</td>
      <td>28a206689c0097738177840a40e455a308db2d7d</td>
      <td>kashif/stack-llama-2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/scarlett-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__scarlett-7b</td>
      <td>43.81</td>
      <td>57.17</td>
      <td>80.27</td>
      <td>36.11</td>
      <td>48.52</td>
      <td>72.14</td>
      <td>0.30</td>
      <td>12.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>other</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>0715b738e750830ba7213f26fe32fa1cc1bb15b3</td>
      <td>ajibawa-2023/scarlett-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CobraMamba/mamba-gpt-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CobraMamba__mamba-gpt-7b</td>
      <td>43.78</td>
      <td>51.19</td>
      <td>75.40</td>
      <td>47.47</td>
      <td>42.06</td>
      <td>71.67</td>
      <td>11.98</td>
      <td>6.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>cb0b04b1bff7921614efbd87d5b87bac04c58d13</td>
      <td>CobraMamba/mamba-gpt-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/quantumaikr/llama-2-7b-hf-guanaco-1k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__llama-2-7b-hf-guanaco-1k</td>
      <td>43.78</td>
      <td>51.62</td>
      <td>76.73</td>
      <td>47.45</td>
      <td>44.79</td>
      <td>72.77</td>
      <td>7.43</td>
      <td>5.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>bdb57c5c992872ced47f48cb2177a5fa159f926a</td>
      <td>quantumaikr/llama-2-7b-hf-guanaco-1k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.2</td>
      <td>43.75</td>
      <td>52.13</td>
      <td>78.14</td>
      <td>38.64</td>
      <td>41.79</td>
      <td>71.67</td>
      <td>2.12</td>
      <td>21.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>28.0</td>
      <td>True</td>
      <td>431fda60009d9b37a73211123ffb9c797764e182</td>
      <td>jondurbin/airoboros-7b-gpt4-1.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TinyPixel/elm-test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TinyPixel__elm-test</td>
      <td>43.74</td>
      <td>53.16</td>
      <td>78.98</td>
      <td>47.04</td>
      <td>39.51</td>
      <td>74.35</td>
      <td>7.51</td>
      <td>5.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>aa8f81624d897aa493474bcd96dc3feae9f7a535</td>
      <td>TinyPixel/elm-test</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/conceptofmind/LLongMA-2-13b-16k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_conceptofmind__LLongMA-2-13b-16k</td>
      <td>43.71</td>
      <td>54.27</td>
      <td>79.66</td>
      <td>50.86</td>
      <td>37.68</td>
      <td>72.61</td>
      <td>5.46</td>
      <td>5.45</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>c2defe28e2f3f10460baf8f778b00986a53aa7a2</td>
      <td>conceptofmind/LLongMA-2-13b-16k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psyche/kollama2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psyche__kollama2-7b</td>
      <td>43.71</td>
      <td>53.24</td>
      <td>78.78</td>
      <td>42.31</td>
      <td>44.56</td>
      <td>73.95</td>
      <td>5.99</td>
      <td>7.11</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>48fca4ba1e2d31ff4fbe6856b9b93ad2d97da8b7</td>
      <td>psyche/kollama2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-guanaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-guanaco</td>
      <td>43.70</td>
      <td>50.51</td>
      <td>76.72</td>
      <td>48.03</td>
      <td>43.36</td>
      <td>72.93</td>
      <td>8.57</td>
      <td>5.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>5d33696ee324899d52fc43794b46009fea08a9af</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-guanaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mrm8488/llama-2-coder-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mrm8488__llama-2-coder-7b</td>
      <td>43.66</td>
      <td>54.01</td>
      <td>78.35</td>
      <td>46.25</td>
      <td>38.49</td>
      <td>75.45</td>
      <td>7.13</td>
      <td>5.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>36.0</td>
      <td>True</td>
      <td>f21c0d5e3f9f8c5addf093358e6885afa9602296</td>
      <td>mrm8488/llama-2-coder-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HuggingFaceH4/starchat-beta</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HuggingFaceH4__starchat-beta</td>
      <td>43.65</td>
      <td>52.47</td>
      <td>80.59</td>
      <td>42.85</td>
      <td>47.22</td>
      <td>69.69</td>
      <td>5.16</td>
      <td>7.54</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>bigcode-openrail-m</td>
      <td>15.52</td>
      <td>222.0</td>
      <td>True</td>
      <td>b1bcda690655777373f57ea6614eb095ec2c886f</td>
      <td>HuggingFaceH4/starchat-beta</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TinyPixel/lima-test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TinyPixel__lima-test</td>
      <td>43.63</td>
      <td>53.07</td>
      <td>78.88</td>
      <td>46.42</td>
      <td>39.40</td>
      <td>74.03</td>
      <td>7.96</td>
      <td>5.65</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>4d6a006c6341f29b11c02f19bf9535f51b4da1b5</td>
      <td>TinyPixel/lima-test</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wenge-research/yayi-7b-llama2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wenge-research__yayi-7b-llama2</td>
      <td>43.62</td>
      <td>54.78</td>
      <td>77.94</td>
      <td>41.35</td>
      <td>44.02</td>
      <td>74.51</td>
      <td>6.67</td>
      <td>6.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>18a4ed38285c732efc583a4bd883b3a681f8d005</td>
      <td>wenge-research/yayi-7b-llama2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-mixed-datasets-time-unit</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mixed-datasets-time-unit</td>
      <td>43.61</td>
      <td>51.79</td>
      <td>76.41</td>
      <td>49.58</td>
      <td>40.33</td>
      <td>73.40</td>
      <td>7.13</td>
      <td>6.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>26626ea669172be6bc8e6b2b0bc5f14aef8061aa</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mixed-datasets-time-unit</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/Uncensored-Frank-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__Uncensored-Frank-7B</td>
      <td>43.60</td>
      <td>54.27</td>
      <td>76.52</td>
      <td>37.50</td>
      <td>43.86</td>
      <td>70.24</td>
      <td>5.00</td>
      <td>17.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>65bbcb80158a6d2e133bba99a90142caf4e2e242</td>
      <td>ajibawa-2023/Uncensored-Frank-7B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PeanutJar/LLaMa-2-PeanutButter_v18_A-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PeanutJar__LLaMa-2-PeanutButter_v18_A-7B</td>
      <td>43.58</td>
      <td>53.16</td>
      <td>78.11</td>
      <td>45.54</td>
      <td>40.37</td>
      <td>74.90</td>
      <td>7.20</td>
      <td>5.78</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>15b2fa81418792841014f589e61d1d9e30457040</td>
      <td>PeanutJar/LLaMa-2-PeanutButter_v18_A-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/RoversX/llama-2-7b-hf-small-shards-Samantha-V1-SFT</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RoversX__llama-2-7b-hf-small-shards-Samantha-V1-SFT</td>
      <td>43.57</td>
      <td>53.16</td>
      <td>77.71</td>
      <td>43.47</td>
      <td>45.28</td>
      <td>73.80</td>
      <td>6.37</td>
      <td>5.22</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>c39cee3821269e7fdffa690c2d0836c74dfebd25</td>
      <td>RoversX/llama-2-7b-hf-small-shards-Samantha-V1-SFT</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lajonbot/Llama-2-7b-chat-hf-instruct-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lajonbot__Llama-2-7b-chat-hf-instruct-pl-lora_unload</td>
      <td>43.57</td>
      <td>52.99</td>
      <td>77.49</td>
      <td>47.12</td>
      <td>42.61</td>
      <td>72.06</td>
      <td>6.90</td>
      <td>5.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>f838fda8d2b97effae1e8af4dbb6217eab14fb7e</td>
      <td>Lajonbot/Llama-2-7b-chat-hf-instruct-pl-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TinyPixel/testmodel2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TinyPixel__testmodel2</td>
      <td>43.56</td>
      <td>53.24</td>
      <td>78.78</td>
      <td>46.61</td>
      <td>39.17</td>
      <td>73.80</td>
      <td>7.66</td>
      <td>5.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>cb1111653997cee2818ffcf13a1c37237ea2934d</td>
      <td>TinyPixel/testmodel2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</td>
      <td>43.55</td>
      <td>53.67</td>
      <td>78.09</td>
      <td>45.63</td>
      <td>41.72</td>
      <td>73.56</td>
      <td>5.61</td>
      <td>6.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>2af3d3acb0466fef466512bc17b9bf57024629e8</td>
      <td>dhmeltzer/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-mixed-datasets</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mixed-datasets</td>
      <td>43.51</td>
      <td>51.71</td>
      <td>76.44</td>
      <td>50.13</td>
      <td>39.57</td>
      <td>73.24</td>
      <td>7.13</td>
      <td>6.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>9c74b9396ff6b33e7a7622e59aa1f46103d993fe</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mixed-datasets</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__Llama-2-7B-32K-Instruct</td>
      <td>43.51</td>
      <td>51.37</td>
      <td>78.47</td>
      <td>45.53</td>
      <td>45.01</td>
      <td>72.85</td>
      <td>4.70</td>
      <td>6.64</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>111.0</td>
      <td>True</td>
      <td>b050a6f17d46e32c4b90a30492f14746589f74b7</td>
      <td>togethercomputer/Llama-2-7B-32K-Instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wenge-research/yayi-7b-llama2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wenge-research__yayi-7b-llama2</td>
      <td>43.51</td>
      <td>55.03</td>
      <td>77.84</td>
      <td>40.92</td>
      <td>44.02</td>
      <td>73.72</td>
      <td>6.97</td>
      <td>6.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>f1a9e8d91e5b636cde3ea7fcf752a9f0234bd92a</td>
      <td>wenge-research/yayi-7b-llama2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/llama-2-7b-hf_open-platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__llama-2-7b-hf_open-platypus</td>
      <td>43.49</td>
      <td>51.45</td>
      <td>78.63</td>
      <td>43.60</td>
      <td>43.71</td>
      <td>74.43</td>
      <td>6.60</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>c7e776f3f3afc0fa22cb7aff0d00522e571e9b29</td>
      <td>lgaalves/llama-2-7b-hf_open-platypus</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elyza__ELYZA-japanese-Llama-2-7b-instruct</td>
      <td>43.48</td>
      <td>53.16</td>
      <td>78.25</td>
      <td>47.07</td>
      <td>39.08</td>
      <td>73.24</td>
      <td>7.88</td>
      <td>5.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>32.0</td>
      <td>True</td>
      <td>48fa08b3098a23d3671e09565499a4cfbaff1923</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TinyPixel/testmodel-3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TinyPixel__testmodel-3</td>
      <td>43.48</td>
      <td>53.24</td>
      <td>78.72</td>
      <td>46.57</td>
      <td>38.75</td>
      <td>73.88</td>
      <td>7.58</td>
      <td>5.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1fbc4d8a2c1a3d211325bdff9e7f0539fa7a2b1</td>
      <td>TinyPixel/testmodel-3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-mctaco-modified2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mctaco-modified2</td>
      <td>43.48</td>
      <td>42.92</td>
      <td>73.97</td>
      <td>48.49</td>
      <td>40.43</td>
      <td>69.69</td>
      <td>0.68</td>
      <td>28.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>8e1930bbbbdeb4f6f4639e837f09d9878bbf7831</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco-modified2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-openllama-7b-v12-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-openllama-7b-v12-bf16</td>
      <td>43.47</td>
      <td>42.06</td>
      <td>62.01</td>
      <td>46.53</td>
      <td>45.18</td>
      <td>65.04</td>
      <td>10.84</td>
      <td>32.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.63</td>
      <td>1.0</td>
      <td>True</td>
      <td>bb94ff691996484b1a9d899a6c0956ef6750d86a</td>
      <td>OpenBuddy/openbuddy-openllama-7b-v12-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/undi95/llama2-to-mistral-diff</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_undi95__llama2-to-mistral-diff</td>
      <td>43.47</td>
      <td>53.41</td>
      <td>78.56</td>
      <td>46.43</td>
      <td>38.71</td>
      <td>74.03</td>
      <td>7.51</td>
      <td>5.61</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>16c279c5e7d12b8a6ff7771881808ef253a406b9</td>
      <td>undi95/llama2-to-mistral-diff</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-v4</td>
      <td>43.47</td>
      <td>53.41</td>
      <td>78.56</td>
      <td>46.43</td>
      <td>38.71</td>
      <td>74.03</td>
      <td>7.51</td>
      <td>5.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>405c54ec7aea0735996ef5ff6ede6c35ab930381</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-l2-7b-gpt4-1.4.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-l2-7b-gpt4-1.4.1</td>
      <td>43.46</td>
      <td>55.12</td>
      <td>79.60</td>
      <td>45.17</td>
      <td>40.29</td>
      <td>74.27</td>
      <td>2.81</td>
      <td>6.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>10.0</td>
      <td>True</td>
      <td>77bdd1f049f27876c38b68782fc240518208f391</td>
      <td>jondurbin/airoboros-l2-7b-gpt4-1.4.1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/meta-llama/Llama-2-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-7b-hf</td>
      <td>43.43</td>
      <td>53.07</td>
      <td>78.59</td>
      <td>46.87</td>
      <td>38.76</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.59</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>679.0</td>
      <td>False</td>
      <td>e8f058fa738b6b308540024e9aa12e274e291f75</td>
      <td>meta-llama/Llama-2-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bongchoi/test-llama2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bongchoi__test-llama2-7b</td>
      <td>43.43</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.86</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.61</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>ebe2e68699cb7ab6bb22688f265c89be2ac0fa6d</td>
      <td>bongchoi/test-llama2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/yeen214/test_llama2_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeen214__test_llama2_7b</td>
      <td>43.43</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.86</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>69a4886f51ed752216cdd7f41a584d14240126f9</td>
      <td>yeen214/test_llama2_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TaylorAI/Flash-Llama-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TaylorAI__Flash-Llama-7B</td>
      <td>43.42</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.60</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>27c84ef23d850582453e1cc2dcea13de48da090f</td>
      <td>TaylorAI/Flash-Llama-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NewstaR/Starlight-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NewstaR__Starlight-7B</td>
      <td>43.42</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1f7436c458ebc3d8d31b91091c1a7a48e942cd3b</td>
      <td>NewstaR/Starlight-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ibranze/araproje-llama2-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ibranze__araproje-llama2-7b-hf</td>
      <td>43.42</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>7fe54f507e762b0f62265813aef908765b1298c0</td>
      <td>ibranze/araproje-llama2-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-v4</td>
      <td>43.42</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>405c54ec7aea0735996ef5ff6ede6c35ab930381</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-v2</td>
      <td>43.42</td>
      <td>53.07</td>
      <td>78.57</td>
      <td>46.80</td>
      <td>38.75</td>
      <td>74.03</td>
      <td>7.13</td>
      <td>5.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>1c97650d4b919e2c6a2829778caa3a109935a58c</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ToolBench/ToolLLaMA-7b-LoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ToolBench__ToolLLaMA-7b-LoRA</td>
      <td>43.42</td>
      <td>52.99</td>
      <td>78.62</td>
      <td>46.87</td>
      <td>38.67</td>
      <td>74.35</td>
      <td>6.82</td>
      <td>5.61</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.00</td>
      <td>7.0</td>
      <td>False</td>
      <td>67f2e8af850049a86fb9ee8ef581deb0f51e58e6</td>
      <td>ToolBench/ToolLLaMA-7b-LoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/clibrain/Llama-2-7b-ft-instruct-es</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_clibrain__Llama-2-7b-ft-instruct-es</td>
      <td>43.40</td>
      <td>53.67</td>
      <td>77.83</td>
      <td>46.58</td>
      <td>38.82</td>
      <td>75.22</td>
      <td>5.69</td>
      <td>5.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>13.0</td>
      <td>True</td>
      <td>b62f431c88b232204ea7046f9d906ae1daa68437</td>
      <td>clibrain/Llama-2-7b-ft-instruct-es</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_fireballoon__baichuan-vicuna-chinese-7b</td>
      <td>43.39</td>
      <td>43.52</td>
      <td>71.12</td>
      <td>46.87</td>
      <td>42.45</td>
      <td>66.85</td>
      <td>5.53</td>
      <td>27.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>61.0</td>
      <td>True</td>
      <td>6cdb9e75cd473e31e87067c2a0b646083247d9ab</td>
      <td>fireballoon/baichuan-vicuna-chinese-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__Qwen-VL-LLaMAfied-7B-Chat</td>
      <td>43.39</td>
      <td>47.35</td>
      <td>69.97</td>
      <td>44.12</td>
      <td>42.87</td>
      <td>65.67</td>
      <td>0.00</td>
      <td>33.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>ccbd599ac46bcfbf7020be393afeecef404bce2b</td>
      <td>JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/georgesung/llama2_7b_chat_uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_georgesung__llama2_7b_chat_uncensored</td>
      <td>43.39</td>
      <td>53.58</td>
      <td>78.66</td>
      <td>44.49</td>
      <td>41.34</td>
      <td>74.11</td>
      <td>5.84</td>
      <td>5.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>152.0</td>
      <td>True</td>
      <td>e9a972b12c6b59bfbcf30fe3779c2c933ce755bd</td>
      <td>georgesung/llama2_7b_chat_uncensored</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TehVenom/Pygmalion-Vicuna-1.1-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Pygmalion-Vicuna-1.1-7b</td>
      <td>43.35</td>
      <td>52.82</td>
      <td>78.66</td>
      <td>43.61</td>
      <td>42.21</td>
      <td>71.98</td>
      <td>6.22</td>
      <td>7.97</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>25.0</td>
      <td>True</td>
      <td>bdac596568769d1ba4af8df9a611eee9723adf29</td>
      <td>TehVenom/Pygmalion-Vicuna-1.1-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/heegyu/LIMA2-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__LIMA2-7b-hf</td>
      <td>43.32</td>
      <td>53.24</td>
      <td>80.60</td>
      <td>43.22</td>
      <td>44.74</td>
      <td>69.93</td>
      <td>3.87</td>
      <td>7.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>6a1aa59cb7624f059728840ce68b20b1070ebdcb</td>
      <td>heegyu/LIMA2-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dotvignesh/perry-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dotvignesh__perry-7b</td>
      <td>43.30</td>
      <td>51.79</td>
      <td>76.43</td>
      <td>46.18</td>
      <td>40.08</td>
      <td>72.53</td>
      <td>10.31</td>
      <td>5.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>f35ae37b436637cd3e14d086324ccdaccfd69045</td>
      <td>dotvignesh/perry-7b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/quantumaikr/QuantumLM-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_quantumaikr__QuantumLM-7B</td>
      <td>43.29</td>
      <td>50.26</td>
      <td>76.10</td>
      <td>45.27</td>
      <td>46.25</td>
      <td>71.51</td>
      <td>7.66</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>f44998432fb90d88094ddf42e57ec458877a197f</td>
      <td>quantumaikr/QuantumLM-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/rameshm/llama-2-13b-mathgpt-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_rameshm__llama-2-13b-mathgpt-v4</td>
      <td>43.26</td>
      <td>50.94</td>
      <td>75.56</td>
      <td>43.78</td>
      <td>41.96</td>
      <td>69.14</td>
      <td>14.71</td>
      <td>6.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>c5072a762070c6b3756385c63805348c155004b5</td>
      <td>rameshm/llama-2-13b-mathgpt-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</td>
      <td>43.25</td>
      <td>53.41</td>
      <td>77.90</td>
      <td>43.56</td>
      <td>40.81</td>
      <td>74.59</td>
      <td>5.08</td>
      <td>7.37</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ca41503b383c654aee8d5496e70fbdfaa33db10</td>
      <td>dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/qualis2006/llama-2-7b-int4-python-code-18k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_qualis2006__llama-2-7b-int4-python-code-18k</td>
      <td>43.22</td>
      <td>52.13</td>
      <td>78.55</td>
      <td>46.25</td>
      <td>37.69</td>
      <td>74.98</td>
      <td>6.22</td>
      <td>6.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>aed968a4b3f3b716064eb8b50c5ae24b38007627</td>
      <td>qualis2006/llama-2-7b-int4-python-code-18k</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/LeoLM/leo-hessianai-7b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LeoLM__leo-hessianai-7b-chat</td>
      <td>43.18</td>
      <td>52.56</td>
      <td>77.61</td>
      <td>45.58</td>
      <td>44.89</td>
      <td>69.93</td>
      <td>5.16</td>
      <td>6.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>7c343a501f5cd3b768d2f78d9941b760fd66815d</td>
      <td>LeoLM/leo-hessianai-7b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/luffycodes/llama-shishya-7b-ep3-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_luffycodes__llama-shishya-7b-ep3-v2</td>
      <td>43.17</td>
      <td>47.35</td>
      <td>75.88</td>
      <td>43.84</td>
      <td>30.16</td>
      <td>68.75</td>
      <td>0.00</td>
      <td>36.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>679c6cb9e869df686b1ae415ed440e6cfc05f80b</td>
      <td>luffycodes/llama-shishya-7b-ep3-v2</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/openchat/opencoderplus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openchat__opencoderplus</td>
      <td>43.17</td>
      <td>50.60</td>
      <td>78.22</td>
      <td>42.73</td>
      <td>50.72</td>
      <td>66.14</td>
      <td>4.62</td>
      <td>9.14</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>15.52</td>
      <td>100.0</td>
      <td>True</td>
      <td>845e9e4452dd4440760b3d5f680400fc014e91b5</td>
      <td>openchat/opencoderplus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-RetroRodeo-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-RetroRodeo-13b</td>
      <td>43.15</td>
      <td>53.84</td>
      <td>79.63</td>
      <td>48.93</td>
      <td>38.73</td>
      <td>73.80</td>
      <td>0.00</td>
      <td>7.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>2.0</td>
      <td>True</td>
      <td>102f9fdad903f5eaffe1ed8173ae56081072e429</td>
      <td>PocketDoc/Dans-RetroRodeo-13b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-orca-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-orca-13b</td>
      <td>43.14</td>
      <td>44.37</td>
      <td>65.20</td>
      <td>43.46</td>
      <td>45.94</td>
      <td>64.01</td>
      <td>5.99</td>
      <td>33.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>6fdfeabe817235df3d560a6e6465c3722bc3a4ba</td>
      <td>uukuguy/speechless-codellama-orca-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/Guanaco-7B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__Guanaco-7B-Uncensored</td>
      <td>43.13</td>
      <td>52.13</td>
      <td>78.77</td>
      <td>43.42</td>
      <td>44.45</td>
      <td>73.09</td>
      <td>4.25</td>
      <td>5.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>4.0</td>
      <td>True</td>
      <td>db068e363e66e5d4b131e1d7a42a3a849e406a9b</td>
      <td>Fredithefish/Guanaco-7B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lmsys/longchat-7b-v1.5-32k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lmsys__longchat-7b-v1.5-32k</td>
      <td>43.06</td>
      <td>51.71</td>
      <td>74.97</td>
      <td>43.16</td>
      <td>44.42</td>
      <td>68.67</td>
      <td>4.78</td>
      <td>13.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>44.0</td>
      <td>True</td>
      <td>16deb633ef4d6a18d5750239edc5a85ffeaf3918</td>
      <td>lmsys/longchat-7b-v1.5-32k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/vicuna-7B-physics</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__vicuna-7B-physics</td>
      <td>43.05</td>
      <td>49.49</td>
      <td>75.88</td>
      <td>46.58</td>
      <td>49.31</td>
      <td>69.38</td>
      <td>4.25</td>
      <td>6.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>2147983e9493347c3424c07403f65e7a81c0b19f</td>
      <td>FelixChao/vicuna-7B-physics</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/venkycs/llama-v2-7b-32kC-Security</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_venkycs__llama-v2-7b-32kC-Security</td>
      <td>42.99</td>
      <td>49.83</td>
      <td>77.33</td>
      <td>44.41</td>
      <td>47.96</td>
      <td>71.74</td>
      <td>3.87</td>
      <td>5.81</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>5.0</td>
      <td>False</td>
      <td>0ae2abdc539a79ad84b141f894d614adf3754882</td>
      <td>venkycs/llama-v2-7b-32kC-Security</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elyza__ELYZA-japanese-Llama-2-7b-fast-instruct</td>
      <td>42.96</td>
      <td>53.75</td>
      <td>77.55</td>
      <td>46.85</td>
      <td>38.84</td>
      <td>71.59</td>
      <td>6.29</td>
      <td>5.84</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.66</td>
      <td>42.0</td>
      <td>True</td>
      <td>89de33d1ad568855853196802aeaecd799c6586f</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b-fast-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/GeneZC/MiniChat-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_GeneZC__MiniChat-3B</td>
      <td>42.94</td>
      <td>44.03</td>
      <td>67.19</td>
      <td>39.17</td>
      <td>45.67</td>
      <td>65.27</td>
      <td>10.54</td>
      <td>28.73</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>3.0</td>
      <td>True</td>
      <td>123d23bd291bb2d5fdb3b91dc1570d0b11654a78</td>
      <td>GeneZC/MiniChat-3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.4</td>
      <td>42.94</td>
      <td>53.92</td>
      <td>80.33</td>
      <td>38.61</td>
      <td>41.05</td>
      <td>72.77</td>
      <td>3.71</td>
      <td>10.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>cae1ab8991f66bbe66ae95ed23a87846e7343047</td>
      <td>jondurbin/airoboros-7b-gpt4-1.4</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/llm-agents/tora-7b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llm-agents__tora-7b-v1.0</td>
      <td>42.89</td>
      <td>52.47</td>
      <td>78.68</td>
      <td>45.90</td>
      <td>37.90</td>
      <td>73.56</td>
      <td>2.50</td>
      <td>9.24</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>717edbee98945192b1a396fc9c337c5b32d6c79c</td>
      <td>llm-agents/tora-7b-v1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/vibhorag101/llama-2-7b-chat-hf-phr_mental_health-2048</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vibhorag101__llama-2-7b-chat-hf-phr_mental_health-2048</td>
      <td>42.84</td>
      <td>52.39</td>
      <td>75.39</td>
      <td>39.77</td>
      <td>42.89</td>
      <td>71.19</td>
      <td>5.91</td>
      <td>12.30</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>81d424a431ab7fa4ff725925b6d0e4269d4563e4</td>
      <td>vibhorag101/llama-2-7b-chat-hf-phr_mental_health-2048</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hiyouga/Baichuan2-7B-Base-LLaMAfied</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hiyouga__Baichuan2-7B-Base-LLaMAfied</td>
      <td>42.83</td>
      <td>49.57</td>
      <td>73.45</td>
      <td>54.86</td>
      <td>37.54</td>
      <td>70.72</td>
      <td>7.81</td>
      <td>5.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.99</td>
      <td>4.0</td>
      <td>True</td>
      <td>dc5bda435771212fc73a8c6556fbdf4fcd87f96d</td>
      <td>hiyouga/Baichuan2-7B-Base-LLaMAfied</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/itsliupeng/llama2_7b_code</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_itsliupeng__llama2_7b_code</td>
      <td>42.81</td>
      <td>52.13</td>
      <td>75.71</td>
      <td>48.05</td>
      <td>38.76</td>
      <td>71.51</td>
      <td>8.11</td>
      <td>5.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>0e6d1edd87c8753b55d280179c8fb0e65ebf5fa2</td>
      <td>itsliupeng/llama2_7b_code</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/notstoic/PygmalionCoT-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_notstoic__PygmalionCoT-7b</td>
      <td>42.79</td>
      <td>51.45</td>
      <td>76.92</td>
      <td>33.35</td>
      <td>48.13</td>
      <td>68.90</td>
      <td>3.26</td>
      <td>17.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.74</td>
      <td>15.0</td>
      <td>True</td>
      <td>c03ac527360663d17bb142405251028eec843ed9</td>
      <td>notstoic/PygmalionCoT-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/GOAT-AI/GOAT-7B-Community</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_GOAT-AI__GOAT-7B-Community</td>
      <td>42.74</td>
      <td>48.81</td>
      <td>74.63</td>
      <td>49.58</td>
      <td>42.48</td>
      <td>72.30</td>
      <td>4.47</td>
      <td>6.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>34.0</td>
      <td>True</td>
      <td>a7073a0f5142ce04aaa1603b0812b358f62a8de8</td>
      <td>GOAT-AI/GOAT-7B-Community</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dhmeltzer__llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</td>
      <td>42.74</td>
      <td>54.35</td>
      <td>78.06</td>
      <td>45.35</td>
      <td>37.11</td>
      <td>73.40</td>
      <td>4.62</td>
      <td>6.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>684c4f4612fadae47c2c7db9fe9e9be4aaafc7e2</td>
      <td>dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/meta-llama/Llama-2-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_meta-llama__Llama-2-7b-hf</td>
      <td>42.67</td>
      <td>53.07</td>
      <td>77.74</td>
      <td>43.80</td>
      <td>38.98</td>
      <td>74.59</td>
      <td>5.38</td>
      <td>5.13</td>
      <td>?</td>
      <td>4bit</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9</td>
      <td>meta-llama/Llama-2-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-v3</td>
      <td>42.65</td>
      <td>52.22</td>
      <td>76.78</td>
      <td>45.89</td>
      <td>38.38</td>
      <td>73.40</td>
      <td>6.22</td>
      <td>5.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5269bc93a7f98e192e34553cec1302877ca4327</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-mctaco-modified1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mctaco-modified1</td>
      <td>42.65</td>
      <td>40.87</td>
      <td>73.40</td>
      <td>47.42</td>
      <td>39.87</td>
      <td>69.46</td>
      <td>1.29</td>
      <td>26.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>a7749ff092ef03900de34b69d41c767a6a48ea9e</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco-modified1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-7b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-7b-instruct</td>
      <td>42.62</td>
      <td>50.34</td>
      <td>77.91</td>
      <td>32.35</td>
      <td>35.08</td>
      <td>70.48</td>
      <td>2.81</td>
      <td>29.40</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-3.0</td>
      <td>6.65</td>
      <td>437.0</td>
      <td>True</td>
      <td>925e0d80e50e77aaddaf9c3ced41ca4ea23a1025</td>
      <td>mosaicml/mpt-7b-instruct</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/llm-agents/tora-code-34b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llm-agents__tora-code-34b-v1.0</td>
      <td>42.61</td>
      <td>50.43</td>
      <td>75.54</td>
      <td>46.78</td>
      <td>39.66</td>
      <td>68.19</td>
      <td>13.12</td>
      <td>4.58</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>4.0</td>
      <td>True</td>
      <td>cbb33eea774cc03d4363c424d81e8c9d58332274</td>
      <td>llm-agents/tora-code-34b-v1.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/LeoLM/leo-hessianai-7b-chat-bilingual</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LeoLM__leo-hessianai-7b-chat-bilingual</td>
      <td>42.58</td>
      <td>51.02</td>
      <td>76.03</td>
      <td>44.68</td>
      <td>47.16</td>
      <td>70.72</td>
      <td>2.73</td>
      <td>5.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>4.0</td>
      <td>True</td>
      <td>5ee98fd03b310e3081f0c9986c5153b27ec5dce6</td>
      <td>LeoLM/leo-hessianai-7b-chat-bilingual</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/vicuna-7b-v1.3-instruct-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__vicuna-7b-v1.3-instruct-pl-lora_unload</td>
      <td>42.57</td>
      <td>48.04</td>
      <td>76.28</td>
      <td>47.42</td>
      <td>44.40</td>
      <td>70.09</td>
      <td>6.22</td>
      <td>5.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e4b19d9d6168b32402da4ab2b5ec7ff27cf40d9b</td>
      <td>Aspik101/vicuna-7b-v1.3-instruct-pl-lora_unload</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/speechlessai/speechless-codellama-airoboros-orca-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_speechlessai__speechless-codellama-airoboros-orca-platypus-13b</td>
      <td>42.55</td>
      <td>44.88</td>
      <td>67.70</td>
      <td>43.16</td>
      <td>40.88</td>
      <td>66.14</td>
      <td>1.82</td>
      <td>33.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f01d3ab70cc23e31dcf5d6418406b08dc2003153</td>
      <td>speechlessai/speechless-codellama-airoboros-orca-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Charlie911/vicuna-7b-v1.5-lora-mctaco-modified4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Charlie911__vicuna-7b-v1.5-lora-mctaco-modified4</td>
      <td>42.54</td>
      <td>40.70</td>
      <td>73.08</td>
      <td>47.26</td>
      <td>41.59</td>
      <td>67.88</td>
      <td>0.08</td>
      <td>27.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>715b03c8573df06f3825d1c08b307e2a83fa8bf9</td>
      <td>Charlie911/vicuna-7b-v1.5-lora-mctaco-modified4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elyza__ELYZA-japanese-Llama-2-7b</td>
      <td>42.53</td>
      <td>52.22</td>
      <td>76.42</td>
      <td>44.60</td>
      <td>37.92</td>
      <td>72.69</td>
      <td>8.34</td>
      <td>5.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>32.0</td>
      <td>True</td>
      <td>976887c5891284db204320860bb84b71d598063e</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/llama2-7b-chat-hf-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__llama2-7b-chat-hf-v3</td>
      <td>42.51</td>
      <td>51.96</td>
      <td>76.70</td>
      <td>45.36</td>
      <td>38.31</td>
      <td>73.56</td>
      <td>5.99</td>
      <td>5.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5269bc93a7f98e192e34553cec1302877ca4327</td>
      <td>TheTravellingEngineer/llama2-7b-chat-hf-v3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PocketDoc/Dans-CreepingSenseOfDoom</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PocketDoc__Dans-CreepingSenseOfDoom</td>
      <td>42.48</td>
      <td>53.33</td>
      <td>78.90</td>
      <td>48.09</td>
      <td>37.84</td>
      <td>73.32</td>
      <td>0.00</td>
      <td>5.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>3.0</td>
      <td>True</td>
      <td>efc7cbc5d0461c137e8ea0c83e54bc5357188783</td>
      <td>PocketDoc/Dans-CreepingSenseOfDoom</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/shibing624/chinese-alpaca-plus-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shibing624__chinese-alpaca-plus-7b-hf</td>
      <td>42.46</td>
      <td>49.23</td>
      <td>70.48</td>
      <td>38.39</td>
      <td>39.72</td>
      <td>70.09</td>
      <td>0.68</td>
      <td>28.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.68</td>
      <td>46.0</td>
      <td>True</td>
      <td>0deb5a13732f1e3e3240ea83f403c57283fe2dc8</td>
      <td>shibing624/chinese-alpaca-plus-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Linly-AI__Chinese-LLaMA-2-7B-hf</td>
      <td>42.44</td>
      <td>48.04</td>
      <td>73.25</td>
      <td>35.04</td>
      <td>39.92</td>
      <td>70.17</td>
      <td>6.22</td>
      <td>24.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.64</td>
      <td>15.0</td>
      <td>True</td>
      <td>a2d55220b3d0693825fe69e1174653dc6cc4a920</td>
      <td>Linly-AI/Chinese-LLaMA-2-7B-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/vicuna-7B-chemical</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__vicuna-7B-chemical</td>
      <td>42.40</td>
      <td>49.83</td>
      <td>74.42</td>
      <td>44.10</td>
      <td>51.70</td>
      <td>67.17</td>
      <td>3.34</td>
      <td>6.26</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>fbf6476ebfa856ffe743e41f8d4413c15b2127c9</td>
      <td>FelixChao/vicuna-7B-chemical</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jphme/orca_mini_v2_ger_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jphme__orca_mini_v2_ger_7b</td>
      <td>42.33</td>
      <td>49.83</td>
      <td>75.50</td>
      <td>39.10</td>
      <td>45.74</td>
      <td>71.59</td>
      <td>4.17</td>
      <td>10.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>175965f50907c6a8cd40f1a4b10d28342969c066</td>
      <td>jphme/orca_mini_v2_ger_7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TheBloke/Llama-2-7B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Llama-2-7B-GPTQ</td>
      <td>42.33</td>
      <td>52.05</td>
      <td>77.59</td>
      <td>43.99</td>
      <td>39.32</td>
      <td>72.93</td>
      <td>5.00</td>
      <td>5.45</td>
      <td>LlamaForCausalLM</td>
      <td>GPTQ</td>
      <td>llama2</td>
      <td>9.05</td>
      <td>58.0</td>
      <td>True</td>
      <td>ecd7ab9f6adc36ecbe0d751eeea0d90ae1863c3b</td>
      <td>TheBloke/Llama-2-7B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LLMs/AlpacaGPT4-7B-elina</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LLMs__AlpacaGPT4-7B-elina</td>
      <td>42.30</td>
      <td>55.03</td>
      <td>78.79</td>
      <td>37.50</td>
      <td>41.53</td>
      <td>72.69</td>
      <td>4.55</td>
      <td>6.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>5.0</td>
      <td>True</td>
      <td>bbece5e3f8ee9be09c8defc536a95c6ef780c681</td>
      <td>LLMs/AlpacaGPT4-7B-elina</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.3</td>
      <td>42.30</td>
      <td>52.47</td>
      <td>77.98</td>
      <td>41.97</td>
      <td>35.73</td>
      <td>72.30</td>
      <td>0.99</td>
      <td>14.64</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>7b5f77827636bbf3174c48ca16e774c89d71d7bd</td>
      <td>jondurbin/airoboros-7b-gpt4-1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Juniplayground/Mist_LLaMA-2-7B-1024_V3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Juniplayground__Mist_LLaMA-2-7B-1024_V3</td>
      <td>42.20</td>
      <td>51.37</td>
      <td>77.74</td>
      <td>41.34</td>
      <td>41.21</td>
      <td>73.32</td>
      <td>4.85</td>
      <td>5.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>05ec8f4a568777e1e543acdf8a587e080fb18fba</td>
      <td>Juniplayground/Mist_LLaMA-2-7B-1024_V3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-gpt-3.5-turbo-100k-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-gpt-3.5-turbo-100k-7b</td>
      <td>42.13</td>
      <td>53.07</td>
      <td>76.16</td>
      <td>33.63</td>
      <td>45.07</td>
      <td>70.80</td>
      <td>3.56</td>
      <td>12.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>11.0</td>
      <td>True</td>
      <td>53887996c0f17f7711d182537505a895fb404542</td>
      <td>jondurbin/airoboros-gpt-3.5-turbo-100k-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/LLMs/Stable-Vicuna-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LLMs__Stable-Vicuna-13B</td>
      <td>42.09</td>
      <td>53.41</td>
      <td>78.57</td>
      <td>50.37</td>
      <td>48.36</td>
      <td>56.99</td>
      <td>0.00</td>
      <td>6.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>12.85</td>
      <td>5.0</td>
      <td>True</td>
      <td>51f3d9eaa71de287c96195abd0ff954839857b19</td>
      <td>LLMs/Stable-Vicuna-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/openthaigpt/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openthaigpt__openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf</td>
      <td>42.05</td>
      <td>50.85</td>
      <td>74.89</td>
      <td>40.02</td>
      <td>47.23</td>
      <td>69.06</td>
      <td>3.87</td>
      <td>8.42</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>cdffb3488c5cb1a9aa5039a6b3bc72af24827db0</td>
      <td>openthaigpt/openthaigpt-1.0.0-alpha-7b-chat-ckpt-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-7b-8k-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-7b-8k-instruct</td>
      <td>42.01</td>
      <td>45.90</td>
      <td>74.43</td>
      <td>42.09</td>
      <td>35.13</td>
      <td>65.90</td>
      <td>10.08</td>
      <td>20.54</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-3.0</td>
      <td>6.65</td>
      <td>23.0</td>
      <td>True</td>
      <td>736f68aceeb61298a5de3cf5ae81d0bc2697edf4</td>
      <td>mosaicml/mpt-7b-8k-instruct</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TigerResearch/tigerbot-7b-base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TigerResearch__tigerbot-7b-base</td>
      <td>41.99</td>
      <td>47.70</td>
      <td>72.08</td>
      <td>45.11</td>
      <td>42.27</td>
      <td>69.61</td>
      <td>10.84</td>
      <td>6.30</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.73</td>
      <td>0.0</td>
      <td>True</td>
      <td>300831494aa1eb16e59799310a09531f60dcc904</td>
      <td>TigerResearch/tigerbot-7b-base</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/chavinlo/alpaca-native</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chavinlo__alpaca-native</td>
      <td>41.96</td>
      <td>52.30</td>
      <td>77.09</td>
      <td>41.60</td>
      <td>37.58</td>
      <td>69.46</td>
      <td>1.44</td>
      <td>14.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>249.0</td>
      <td>True</td>
      <td>cc7773cac2478231807c56ef2f02292d98f85cf5</td>
      <td>chavinlo/alpaca-native</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jerryjalapeno/nart-100k-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jerryjalapeno__nart-100k-7b</td>
      <td>41.91</td>
      <td>54.10</td>
      <td>78.47</td>
      <td>34.98</td>
      <td>36.74</td>
      <td>70.48</td>
      <td>3.56</td>
      <td>15.02</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>6.61</td>
      <td>13.0</td>
      <td>True</td>
      <td>50e61b8e6cc17cb3fbcb490fe3dc7e2c8b248378</td>
      <td>jerryjalapeno/nart-100k-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/WizardLM-7B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__WizardLM-7B-Uncensored</td>
      <td>41.90</td>
      <td>47.87</td>
      <td>73.08</td>
      <td>35.42</td>
      <td>41.49</td>
      <td>68.43</td>
      <td>3.26</td>
      <td>23.76</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>339.0</td>
      <td>True</td>
      <td>14c23f9fa775ab5ce49010418f00df06d92b0b13</td>
      <td>ehartford/WizardLM-7B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/dolphin-llama2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__dolphin-llama2-7b</td>
      <td>41.88</td>
      <td>46.59</td>
      <td>67.52</td>
      <td>48.37</td>
      <td>49.72</td>
      <td>63.77</td>
      <td>5.69</td>
      <td>11.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>64.0</td>
      <td>True</td>
      <td>85aa4f67191fd016ab7ea8c389fddb5d9e5a9a52</td>
      <td>ehartford/dolphin-llama2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DevaMalla/llama_7b_qlora_pds-eval</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DevaMalla__llama_7b_qlora_pds-eval</td>
      <td>41.87</td>
      <td>53.92</td>
      <td>78.13</td>
      <td>32.98</td>
      <td>45.60</td>
      <td>72.61</td>
      <td>4.17</td>
      <td>5.66</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>d20419e1d9e9a6a59ced3edf5169e8e7b3e8394c</td>
      <td>DevaMalla/llama_7b_qlora_pds-eval</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/goims</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__goims</td>
      <td>41.85</td>
      <td>49.49</td>
      <td>72.67</td>
      <td>43.85</td>
      <td>44.80</td>
      <td>69.69</td>
      <td>6.29</td>
      <td>6.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.76</td>
      <td>0.0</td>
      <td>True</td>
      <td>9ef1045ca31f670d9cbf820af904b33a097cd787</td>
      <td>golaxy/goims</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-7b-8k-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-7b-8k-chat</td>
      <td>41.80</td>
      <td>48.04</td>
      <td>77.62</td>
      <td>41.88</td>
      <td>43.68</td>
      <td>71.03</td>
      <td>4.40</td>
      <td>5.91</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.65</td>
      <td>26.0</td>
      <td>True</td>
      <td>ef97b878a279cd1765fbed7b8321fb3cff1aa5b5</td>
      <td>mosaicml/mpt-7b-8k-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WeOpenML/Alpaca-7B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WeOpenML__Alpaca-7B-v1</td>
      <td>41.79</td>
      <td>49.06</td>
      <td>75.71</td>
      <td>33.76</td>
      <td>36.28</td>
      <td>71.51</td>
      <td>0.15</td>
      <td>26.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>be5cb84a84a859dd6e5e3efc4648d6d5d1a5d188</td>
      <td>WeOpenML/Alpaca-7B-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_elyza__ELYZA-japanese-Llama-2-7b-fast</td>
      <td>41.72</td>
      <td>51.88</td>
      <td>75.46</td>
      <td>44.34</td>
      <td>36.45</td>
      <td>71.59</td>
      <td>6.29</td>
      <td>6.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.66</td>
      <td>15.0</td>
      <td>True</td>
      <td>e326078aa122fb1c4973997952d7b8630071776a</td>
      <td>elyza/ELYZA-japanese-Llama-2-7b-fast</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LeoLM/leo-hessianai-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LeoLM__leo-hessianai-7b</td>
      <td>41.71</td>
      <td>51.96</td>
      <td>75.84</td>
      <td>42.85</td>
      <td>37.94</td>
      <td>72.14</td>
      <td>5.61</td>
      <td>5.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>22.0</td>
      <td>True</td>
      <td>88c5ac07006ea8f1b5d10aa4f03f0d624dd27e56</td>
      <td>LeoLM/leo-hessianai-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/synapsoft/Llama-2-7b-hf-flan2022-1.2M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_synapsoft__Llama-2-7b-hf-flan2022-1.2M</td>
      <td>41.68</td>
      <td>23.29</td>
      <td>78.46</td>
      <td>42.33</td>
      <td>37.97</td>
      <td>75.53</td>
      <td>4.47</td>
      <td>29.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>1.0</td>
      <td>True</td>
      <td>792f946a1413a7c58378d7a350b7d75b9df80561</td>
      <td>synapsoft/Llama-2-7b-hf-flan2022-1.2M</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/project-baize/baize-healthcare-lora-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_project-baize__baize-healthcare-lora-7B</td>
      <td>41.66</td>
      <td>54.10</td>
      <td>77.32</td>
      <td>37.09</td>
      <td>39.96</td>
      <td>72.85</td>
      <td>4.40</td>
      <td>5.93</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.00</td>
      <td>16.0</td>
      <td>False</td>
      <td>e3eb8bb0d8840431afe24760d964f8ba57edd83e</td>
      <td>project-baize/baize-healthcare-lora-7B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/AlpinDale/pygmalion-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AlpinDale__pygmalion-instruct</td>
      <td>41.62</td>
      <td>52.56</td>
      <td>77.65</td>
      <td>35.94</td>
      <td>42.13</td>
      <td>72.06</td>
      <td>5.08</td>
      <td>5.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>6.74</td>
      <td>5.0</td>
      <td>True</td>
      <td>1665b271316dfee05b2a8daf8b9d6c22ed0aef60</td>
      <td>AlpinDale/pygmalion-instruct</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/microsoft/phi-1_5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_microsoft__phi-1_5</td>
      <td>41.60</td>
      <td>52.90</td>
      <td>63.79</td>
      <td>43.89</td>
      <td>40.89</td>
      <td>72.22</td>
      <td>12.43</td>
      <td>5.04</td>
      <td>MixFormerSequentialForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>other</td>
      <td>0.00</td>
      <td>916.0</td>
      <td>True</td>
      <td>ea95720a352172db6fcbcd89032bfb1cb8481797</td>
      <td>microsoft/phi-1_5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bigcode/starcoderplus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigcode__starcoderplus</td>
      <td>41.58</td>
      <td>48.72</td>
      <td>77.30</td>
      <td>43.72</td>
      <td>37.85</td>
      <td>70.01</td>
      <td>8.04</td>
      <td>5.43</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>15.52</td>
      <td>181.0</td>
      <td>False</td>
      <td>95be82087c33f14ee9941c812a154a9dd66efe72</td>
      <td>bigcode/starcoderplus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aiplanet/effi-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aiplanet__effi-7b</td>
      <td>41.53</td>
      <td>55.12</td>
      <td>78.07</td>
      <td>35.91</td>
      <td>39.71</td>
      <td>72.53</td>
      <td>3.18</td>
      <td>6.15</td>
      <td>?</td>
      <td>4bit</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>False</td>
      <td>d58c62ee27cae60392bd0bd53e1fd05ea82e273b</td>
      <td>aiplanet/effi-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DevaMalla/llama_7b_qlora_cds</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DevaMalla__llama_7b_qlora_cds</td>
      <td>41.51</td>
      <td>52.47</td>
      <td>77.76</td>
      <td>32.38</td>
      <td>46.14</td>
      <td>71.74</td>
      <td>4.09</td>
      <td>5.95</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b6b5c65c5c1cce34d24c8f790bb0cc011e0f0808</td>
      <td>DevaMalla/llama_7b_qlora_cds</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Neko-Institute-of-Science/metharme-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Neko-Institute-of-Science__metharme-7b</td>
      <td>41.50</td>
      <td>53.67</td>
      <td>78.62</td>
      <td>35.91</td>
      <td>39.16</td>
      <td>72.53</td>
      <td>5.00</td>
      <td>5.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>12.0</td>
      <td>True</td>
      <td>62ca156891feead8db117be8f5f35687b6274e6e</td>
      <td>Neko-Institute-of-Science/metharme-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/guanaco-7B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__guanaco-7B-HF</td>
      <td>41.37</td>
      <td>52.99</td>
      <td>80.05</td>
      <td>35.32</td>
      <td>39.20</td>
      <td>71.43</td>
      <td>5.08</td>
      <td>5.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>True</td>
      <td>293c24105fa15afa127a2ec3905fdc2a0a3a6dac</td>
      <td>TheBloke/guanaco-7B-HF</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/openlm-research/open_llama_13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openlm-research__open_llama_13b</td>
      <td>41.37</td>
      <td>51.19</td>
      <td>75.23</td>
      <td>43.75</td>
      <td>38.08</td>
      <td>72.06</td>
      <td>3.26</td>
      <td>5.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>434.0</td>
      <td>True</td>
      <td>b6d7fde8392250730d24cc2fcfa3b7e5f9a03ce8</td>
      <td>openlm-research/open_llama_13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hyunseoki/ko-ref-llama2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hyunseoki__ko-ref-llama2-13b</td>
      <td>41.32</td>
      <td>48.38</td>
      <td>73.56</td>
      <td>34.83</td>
      <td>35.82</td>
      <td>69.14</td>
      <td>0.00</td>
      <td>27.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>c5d09631c88ab5012b48187ecd90ae773cd4bbd9</td>
      <td>hyunseoki/ko-ref-llama2-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/titan087/OpenLlama13B-Guanaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_titan087__OpenLlama13B-Guanaco</td>
      <td>41.32</td>
      <td>51.19</td>
      <td>75.24</td>
      <td>43.76</td>
      <td>38.40</td>
      <td>71.74</td>
      <td>2.96</td>
      <td>5.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>42ed3023ae1afe861f533570be881a03b10fc860</td>
      <td>titan087/OpenLlama13B-Guanaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WeOpenML/PandaLM-Alpaca-7B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WeOpenML__PandaLM-Alpaca-7B-v1</td>
      <td>41.31</td>
      <td>50.85</td>
      <td>77.36</td>
      <td>35.91</td>
      <td>36.63</td>
      <td>71.90</td>
      <td>0.91</td>
      <td>15.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>7fe5cb1a7009fdade8dfcfec335527997a730fcf</td>
      <td>WeOpenML/PandaLM-Alpaca-7B-v1</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/JosephusCheung/Guanaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__Guanaco</td>
      <td>41.30</td>
      <td>50.17</td>
      <td>72.69</td>
      <td>30.30</td>
      <td>37.64</td>
      <td>68.67</td>
      <td>0.00</td>
      <td>29.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>6.61</td>
      <td>213.0</td>
      <td>True</td>
      <td>bed6f3bd18f07a4a379525645cbd86d622b12836</td>
      <td>JosephusCheung/Guanaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FelixChao/CodeLlama13B-Finetune-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FelixChao__CodeLlama13B-Finetune-v1</td>
      <td>41.24</td>
      <td>45.82</td>
      <td>69.36</td>
      <td>45.05</td>
      <td>44.97</td>
      <td>66.93</td>
      <td>10.99</td>
      <td>5.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>40ff78ce37efcaf83718534c494829a573b9d719</td>
      <td>FelixChao/CodeLlama13B-Finetune-v1</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/ausboss/llama7b-wizardlm-unfiltered</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ausboss__llama7b-wizardlm-unfiltered</td>
      <td>41.21</td>
      <td>52.99</td>
      <td>77.89</td>
      <td>36.41</td>
      <td>37.75</td>
      <td>72.30</td>
      <td>4.32</td>
      <td>6.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>5.0</td>
      <td>True</td>
      <td>2123beec77083c414b2ae51dd25b7a870b0b936c</td>
      <td>ausboss/llama7b-wizardlm-unfiltered</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/LLaMA-2-7B-32K</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__LLaMA-2-7B-32K</td>
      <td>41.13</td>
      <td>47.53</td>
      <td>76.14</td>
      <td>43.33</td>
      <td>39.23</td>
      <td>71.90</td>
      <td>4.32</td>
      <td>5.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>465.0</td>
      <td>True</td>
      <td>aef6d8946ae1015bdb65c478a2dd73b58daaef47</td>
      <td>togethercomputer/LLaMA-2-7B-32K</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/koala-7B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__koala-7B-HF</td>
      <td>41.08</td>
      <td>47.10</td>
      <td>73.58</td>
      <td>25.53</td>
      <td>45.96</td>
      <td>69.93</td>
      <td>3.64</td>
      <td>21.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>18.0</td>
      <td>True</td>
      <td>d102fe3b68f1a5a50d547e4fd1c8b33b783c993b</td>
      <td>TheBloke/koala-7B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/shibing624/chinese-llama-plus-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shibing624__chinese-llama-plus-13b-hf</td>
      <td>41.06</td>
      <td>46.25</td>
      <td>71.88</td>
      <td>40.74</td>
      <td>39.89</td>
      <td>73.09</td>
      <td>0.53</td>
      <td>15.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.94</td>
      <td>17.0</td>
      <td>True</td>
      <td>f17a52b8067d551a814069d2c710e1f5c487a3ce</td>
      <td>shibing624/chinese-llama-plus-13b-hf</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TigerResearch/tigerbot-7b-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TigerResearch__tigerbot-7b-sft</td>
      <td>41.01</td>
      <td>41.64</td>
      <td>60.56</td>
      <td>29.89</td>
      <td>58.18</td>
      <td>63.54</td>
      <td>6.29</td>
      <td>27.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.07</td>
      <td>13.0</td>
      <td>False</td>
      <td>98b847905d63f74624e834db1ff95ee2814cbbd3</td>
      <td>TigerResearch/tigerbot-7b-sft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_GeorgiaTechResearchInstitute__galpaca-30b</td>
      <td>40.99</td>
      <td>49.57</td>
      <td>58.20</td>
      <td>43.78</td>
      <td>41.16</td>
      <td>62.51</td>
      <td>2.81</td>
      <td>28.89</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>29.97</td>
      <td>55.0</td>
      <td>True</td>
      <td>a1f0c4bedd65b485a0d4d3a3bd60d7a4599f1eaf</td>
      <td>GeorgiaTechResearchInstitute/galpaca-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DevaMalla/llama_7b_lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DevaMalla__llama_7b_lora</td>
      <td>40.97</td>
      <td>54.86</td>
      <td>79.10</td>
      <td>33.63</td>
      <td>34.74</td>
      <td>72.77</td>
      <td>5.53</td>
      <td>6.12</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>7f4cbd810b4bef0d75c1fd3f551146b4ea97d9fd</td>
      <td>DevaMalla/llama_7b_lora</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/VMware/open-llama-7b-open-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_VMware__open-llama-7b-open-instruct</td>
      <td>40.90</td>
      <td>49.74</td>
      <td>73.67</td>
      <td>31.52</td>
      <td>34.65</td>
      <td>65.43</td>
      <td>0.53</td>
      <td>30.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-3.0</td>
      <td>6.61</td>
      <td>25.0</td>
      <td>True</td>
      <td>fdf9f034163cce67e04d55172155f0e07b1b19a0</td>
      <td>VMware/open-llama-7b-open-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ajibawa-2023/carl-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ajibawa-2023__carl-7b</td>
      <td>40.87</td>
      <td>53.50</td>
      <td>78.29</td>
      <td>33.96</td>
      <td>40.29</td>
      <td>68.59</td>
      <td>2.35</td>
      <td>9.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>de4c7af9598bebc47dd43253c972be719f3195d6</td>
      <td>ajibawa-2023/carl-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/project-baize/baize-v2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_project-baize__baize-v2-7b</td>
      <td>40.86</td>
      <td>48.98</td>
      <td>75.06</td>
      <td>39.60</td>
      <td>41.39</td>
      <td>71.11</td>
      <td>4.17</td>
      <td>5.67</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>23.0</td>
      <td>True</td>
      <td>e4731c2c2671e2d0b47b5eba08c753ca21671fab</td>
      <td>project-baize/baize-v2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-platypus-13b</td>
      <td>40.81</td>
      <td>46.16</td>
      <td>68.88</td>
      <td>44.55</td>
      <td>44.98</td>
      <td>66.14</td>
      <td>9.40</td>
      <td>5.54</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>7a771bd8899b9ef4ba9680e96f84dc85810a67d6</td>
      <td>uukuguy/speechless-codellama-platypus-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/BigTranslate-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__BigTranslate-13B-GPTQ</td>
      <td>40.74</td>
      <td>45.31</td>
      <td>75.10</td>
      <td>31.18</td>
      <td>40.60</td>
      <td>70.96</td>
      <td>0.00</td>
      <td>22.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>17.99</td>
      <td>15.0</td>
      <td>True</td>
      <td>f2968552d2f522023f3289747234aea5508980e2</td>
      <td>TheBloke/BigTranslate-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DevaMalla/llama_7b_qlora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DevaMalla__llama_7b_qlora</td>
      <td>40.73</td>
      <td>55.12</td>
      <td>78.26</td>
      <td>35.71</td>
      <td>33.98</td>
      <td>72.06</td>
      <td>4.55</td>
      <td>5.46</td>
      <td>?</td>
      <td>4bit</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>7f94b0be78193abc54722cf723541c3800426f7b</td>
      <td>DevaMalla/llama_7b_qlora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.4.1-qlora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jondurbin__airoboros-7b-gpt4-1.4.1-qlora</td>
      <td>40.72</td>
      <td>52.73</td>
      <td>77.89</td>
      <td>38.77</td>
      <td>36.07</td>
      <td>70.32</td>
      <td>2.27</td>
      <td>6.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>91ffa900ed637cf5fd904d96e6985b6f7857ad64</td>
      <td>jondurbin/airoboros-7b-gpt4-1.4.1-qlora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DevaMalla/llama7b_alpaca_1gpu_bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DevaMalla__llama7b_alpaca_1gpu_bf16</td>
      <td>40.71</td>
      <td>52.73</td>
      <td>78.78</td>
      <td>36.26</td>
      <td>33.71</td>
      <td>72.93</td>
      <td>4.55</td>
      <td>5.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>305683c1b95f6888b8668dbc6b56d9efa5d07fef</td>
      <td>DevaMalla/llama7b_alpaca_1gpu_bf16</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/stabilityai/stablelm-3b-4e1t</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__stablelm-3b-4e1t</td>
      <td>40.69</td>
      <td>46.59</td>
      <td>75.94</td>
      <td>45.23</td>
      <td>37.20</td>
      <td>71.19</td>
      <td>3.34</td>
      <td>5.36</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>2.80</td>
      <td>192.0</td>
      <td>False</td>
      <td>a4750ace0db6f08d7bbba0aa52a585f231ea3cde</td>
      <td>stabilityai/stablelm-3b-4e1t</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FreedomIntelligence__phoenix-inst-chat-7b</td>
      <td>40.69</td>
      <td>44.71</td>
      <td>63.23</td>
      <td>39.06</td>
      <td>47.08</td>
      <td>62.83</td>
      <td>1.29</td>
      <td>26.62</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.07</td>
      <td>44.0</td>
      <td>True</td>
      <td>5ed4d9570e0f76e1becb05bf467a7b4ff7b66055</td>
      <td>FreedomIntelligence/phoenix-inst-chat-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-7b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-7b-chat</td>
      <td>40.65</td>
      <td>46.50</td>
      <td>75.51</td>
      <td>37.62</td>
      <td>40.16</td>
      <td>68.43</td>
      <td>4.09</td>
      <td>12.20</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.65</td>
      <td>485.0</td>
      <td>True</td>
      <td>64e5c9c9fb53a8e89690c2dee75a5add37f7113e</td>
      <td>mosaicml/mpt-7b-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wenge-research/yayi-13b-llama2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wenge-research__yayi-13b-llama2</td>
      <td>40.55</td>
      <td>48.55</td>
      <td>74.82</td>
      <td>38.68</td>
      <td>42.19</td>
      <td>69.69</td>
      <td>4.02</td>
      <td>5.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>12.85</td>
      <td>6.0</td>
      <td>True</td>
      <td>9fc1bc4409b9e71f54213245a91c2742fbf7b3d0</td>
      <td>wenge-research/yayi-13b-llama2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/yhyhy3/open_llama_7b_v2_med_instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yhyhy3__open_llama_7b_v2_med_instruct</td>
      <td>40.53</td>
      <td>46.50</td>
      <td>76.91</td>
      <td>42.32</td>
      <td>40.33</td>
      <td>69.30</td>
      <td>2.05</td>
      <td>6.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>cabb47abd422a2d67161e2d038265ee23be45fb8</td>
      <td>yhyhy3/open_llama_7b_v2_med_instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/cmarkea/bloomz-7b1-mt-sft-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cmarkea__bloomz-7b1-mt-sft-chat</td>
      <td>40.39</td>
      <td>44.03</td>
      <td>62.60</td>
      <td>38.64</td>
      <td>44.34</td>
      <td>63.30</td>
      <td>0.53</td>
      <td>29.33</td>
      <td>BloomForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.07</td>
      <td>9.0</td>
      <td>True</td>
      <td>8c2dc302780fe320ee3428f3db2ee7ff3684dcef</td>
      <td>cmarkea/bloomz-7b1-mt-sft-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/illuin/test-custom-llama</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_illuin__test-custom-llama</td>
      <td>40.37</td>
      <td>52.30</td>
      <td>77.49</td>
      <td>36.61</td>
      <td>33.81</td>
      <td>72.06</td>
      <td>4.02</td>
      <td>6.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>d985610bef080473e40f01c53266083c5f0c3169</td>
      <td>illuin/test-custom-llama</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Vmware/open-llama-7b-v2-open-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Vmware__open-llama-7b-v2-open-instruct</td>
      <td>40.34</td>
      <td>39.76</td>
      <td>70.31</td>
      <td>35.16</td>
      <td>39.53</td>
      <td>64.33</td>
      <td>7.43</td>
      <td>25.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>b8fbe09571a71603ab517fe897a1281005060b62</td>
      <td>Vmware/open-llama-7b-v2-open-instruct</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/stabilityai/stablelm-base-alpha-7b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__stablelm-base-alpha-7b-v2</td>
      <td>40.31</td>
      <td>47.35</td>
      <td>77.08</td>
      <td>45.10</td>
      <td>36.46</td>
      <td>68.51</td>
      <td>2.58</td>
      <td>5.08</td>
      <td>StableLMAlphaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>6.89</td>
      <td>36.0</td>
      <td>True</td>
      <td>eb3b56fee1ad4b1efe6625bbbc7a277df8ab5b96</td>
      <td>stabilityai/stablelm-base-alpha-7b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Neko-Institute-of-Science__pygmalion-7b</td>
      <td>40.29</td>
      <td>51.37</td>
      <td>77.81</td>
      <td>35.68</td>
      <td>34.54</td>
      <td>72.22</td>
      <td>4.62</td>
      <td>5.79</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>38.0</td>
      <td>True</td>
      <td>6473f9996d758fde48a181f37cc5de575aff1606</td>
      <td>Neko-Institute-of-Science/pygmalion-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-llama2-7b-pretrain</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-llama2-7b-pretrain</td>
      <td>40.26</td>
      <td>48.63</td>
      <td>74.83</td>
      <td>41.04</td>
      <td>39.08</td>
      <td>70.24</td>
      <td>3.26</td>
      <td>4.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>444c85ef809f8793d84b0813ab78bec50700cfcf</td>
      <td>YeungNLP/firefly-llama2-7b-pretrain</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-tora-code-7b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-tora-code-7b-v1.0</td>
      <td>40.10</td>
      <td>42.66</td>
      <td>65.16</td>
      <td>38.56</td>
      <td>42.06</td>
      <td>62.90</td>
      <td>0.91</td>
      <td>28.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7b1f87a096045f1bba8f68c62e062102218717b</td>
      <td>uukuguy/speechless-tora-code-7b-v1.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Writer/palmyra-med-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Writer__palmyra-med-20b</td>
      <td>40.02</td>
      <td>46.93</td>
      <td>73.51</td>
      <td>44.34</td>
      <td>35.47</td>
      <td>65.35</td>
      <td>2.65</td>
      <td>11.88</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>20.26</td>
      <td>14.0</td>
      <td>True</td>
      <td>407810f75698c95000dc0ae1a9a0457be625e972</td>
      <td>Writer/palmyra-med-20b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloomz-7b1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloomz-7b1</td>
      <td>40.01</td>
      <td>42.49</td>
      <td>63.01</td>
      <td>37.85</td>
      <td>45.20</td>
      <td>64.64</td>
      <td>0.08</td>
      <td>26.80</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.07</td>
      <td>110.0</td>
      <td>True</td>
      <td>2f4c4f3ebcf171dbbe2bae989ea2d2f3d3486a97</td>
      <td>bigscience/bloomz-7b1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-13b-Instruct-hf</td>
      <td>40.01</td>
      <td>44.54</td>
      <td>64.93</td>
      <td>38.89</td>
      <td>45.88</td>
      <td>68.03</td>
      <td>12.66</td>
      <td>5.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>66.0</td>
      <td>True</td>
      <td>b9f91b7351ecd589118d883afa23d5c93a38c612</td>
      <td>codellama/CodeLlama-13b-Instruct-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__CodeLlama-13B-Instruct-fp16</td>
      <td>40.01</td>
      <td>44.62</td>
      <td>64.94</td>
      <td>38.77</td>
      <td>45.88</td>
      <td>68.03</td>
      <td>12.66</td>
      <td>5.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>28.0</td>
      <td>True</td>
      <td>521c208c7251ccd3e44ccd9500b6bed419bca565</td>
      <td>TheBloke/CodeLlama-13B-Instruct-fp16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-platypus-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-platypus-13b</td>
      <td>39.96</td>
      <td>45.31</td>
      <td>68.63</td>
      <td>42.82</td>
      <td>42.38</td>
      <td>65.59</td>
      <td>9.10</td>
      <td>5.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>81cb1bca46ce646b8339501537837e02116de1b8</td>
      <td>uukuguy/speechless-codellama-platypus-13b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloomz-7b1-mt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloomz-7b1-mt</td>
      <td>39.96</td>
      <td>43.86</td>
      <td>62.91</td>
      <td>37.35</td>
      <td>45.65</td>
      <td>63.06</td>
      <td>0.00</td>
      <td>26.90</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.07</td>
      <td>120.0</td>
      <td>True</td>
      <td>76875e6ea8df98157fb032c48ad6e354fd6a077b</td>
      <td>bigscience/bloomz-7b1-mt</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Planner-7B-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Planner-7B-fp16</td>
      <td>39.93</td>
      <td>51.02</td>
      <td>77.82</td>
      <td>35.71</td>
      <td>34.33</td>
      <td>71.43</td>
      <td>3.56</td>
      <td>5.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>afb4604a06c8541960fb51240259777764c4ce7e</td>
      <td>TheBloke/Planner-7B-fp16</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llama-7b</td>
      <td>39.93</td>
      <td>51.02</td>
      <td>77.82</td>
      <td>35.71</td>
      <td>34.33</td>
      <td>71.43</td>
      <td>3.56</td>
      <td>5.62</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.74</td>
      <td>0.0</td>
      <td>False</td>
      <td>f356572651e58fb337d610470d4b36976e7fb802</td>
      <td>huggingface/llama-7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/DevaMalla/llama-base-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DevaMalla__llama-base-7b</td>
      <td>39.91</td>
      <td>50.94</td>
      <td>77.80</td>
      <td>35.67</td>
      <td>34.34</td>
      <td>71.43</td>
      <td>3.56</td>
      <td>5.62</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e01d89d8e444f7d751ea58feaf22ff8c9af69d2a</td>
      <td>DevaMalla/llama-base-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/yeontaek/WizardCoder-Python-13B-LoRa</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeontaek__WizardCoder-Python-13B-LoRa</td>
      <td>39.78</td>
      <td>47.78</td>
      <td>69.60</td>
      <td>38.76</td>
      <td>43.97</td>
      <td>65.43</td>
      <td>7.81</td>
      <td>5.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>32ffc44ffdf1adfe2d8ef219327fbd534f3d5955</td>
      <td>yeontaek/WizardCoder-Python-13B-LoRa</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ashercn97/giraffe-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ashercn97__giraffe-7b</td>
      <td>39.73</td>
      <td>47.18</td>
      <td>75.53</td>
      <td>38.89</td>
      <td>38.48</td>
      <td>68.98</td>
      <td>2.65</td>
      <td>6.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>9af88449bed5be4709befcfbbba123ee75805479</td>
      <td>ashercn97/giraffe-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jlevin/guanaco-unchained-llama-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jlevin__guanaco-unchained-llama-2-7b</td>
      <td>39.52</td>
      <td>47.35</td>
      <td>72.16</td>
      <td>41.76</td>
      <td>41.49</td>
      <td>64.48</td>
      <td>3.41</td>
      <td>6.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>43f3de8bcef63eec03a1b00079c08b5932c1a429</td>
      <td>jlevin/guanaco-unchained-llama-2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Enno-Ai/ennodata-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Enno-Ai__ennodata-7b</td>
      <td>39.48</td>
      <td>51.02</td>
      <td>77.62</td>
      <td>33.95</td>
      <td>33.53</td>
      <td>70.96</td>
      <td>3.71</td>
      <td>5.59</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>7872a492ebbb3c6a899f9acbd34dfd5f7e674fdd</td>
      <td>Enno-Ai/ennodata-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/OpenAssistant/codellama-13b-oasst-sft-v10</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__codellama-13b-oasst-sft-v10</td>
      <td>39.47</td>
      <td>45.39</td>
      <td>62.36</td>
      <td>35.36</td>
      <td>45.02</td>
      <td>67.80</td>
      <td>13.19</td>
      <td>7.17</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>42.0</td>
      <td>True</td>
      <td>612dab2a8b2d77edb4fd36cfc28b3ffbbb20ffc1</td>
      <td>OpenAssistant/codellama-13b-oasst-sft-v10</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/beomi/llama-2-ko-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beomi__llama-2-ko-7b</td>
      <td>39.43</td>
      <td>48.46</td>
      <td>75.28</td>
      <td>39.56</td>
      <td>34.49</td>
      <td>72.14</td>
      <td>1.97</td>
      <td>4.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.86</td>
      <td>85.0</td>
      <td>True</td>
      <td>d5c58cc2cae21b4fb96aaad2658acc898ab22d99</td>
      <td>beomi/llama-2-ko-7b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/csitfun/llama-7b-logicot</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_csitfun__llama-7b-logicot</td>
      <td>39.37</td>
      <td>47.01</td>
      <td>72.56</td>
      <td>38.93</td>
      <td>43.63</td>
      <td>67.56</td>
      <td>0.00</td>
      <td>5.92</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>8e9c93c09e6a6c7d504c88d6ca598144829bced8</td>
      <td>csitfun/llama-7b-logicot</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/VMware/open-llama-0.7T-7B-open-instruct-v1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_VMware__open-llama-0.7T-7B-open-instruct-v1.1</td>
      <td>39.33</td>
      <td>46.67</td>
      <td>67.67</td>
      <td>28.55</td>
      <td>37.60</td>
      <td>65.43</td>
      <td>0.76</td>
      <td>28.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc</td>
      <td>6.61</td>
      <td>4.0</td>
      <td>True</td>
      <td>75741b55ad462330e3498d1506f438f835152177</td>
      <td>VMware/open-llama-0.7T-7B-open-instruct-v1.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/shareAI/CodeLLaMA-chat-13b-Chinese</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shareAI__CodeLLaMA-chat-13b-Chinese</td>
      <td>39.21</td>
      <td>43.26</td>
      <td>63.87</td>
      <td>34.29</td>
      <td>48.97</td>
      <td>67.88</td>
      <td>10.77</td>
      <td>5.41</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>12.85</td>
      <td>13.0</td>
      <td>True</td>
      <td>675b3e35a9601683c2cb4ec7f1b11d2869842f36</td>
      <td>shareAI/CodeLLaMA-chat-13b-Chinese</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/LongAlpaca-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__LongAlpaca-13B</td>
      <td>39.16</td>
      <td>42.58</td>
      <td>72.03</td>
      <td>34.91</td>
      <td>36.85</td>
      <td>64.09</td>
      <td>0.00</td>
      <td>23.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>4.0</td>
      <td>True</td>
      <td>e80966ae720de9a844441a4a2bbc661106969915</td>
      <td>Yukang/LongAlpaca-13B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Writer/palmyra-20b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Writer__palmyra-20b-chat</td>
      <td>38.97</td>
      <td>43.52</td>
      <td>72.83</td>
      <td>35.18</td>
      <td>43.17</td>
      <td>66.46</td>
      <td>3.94</td>
      <td>7.70</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>20.26</td>
      <td>5.0</td>
      <td>True</td>
      <td>3b7442b7e2240846bc9cfac545bd8861c1660aa2</td>
      <td>Writer/palmyra-20b-chat</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/Project-Baize-v2-7B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Project-Baize-v2-7B-GPTQ</td>
      <td>38.96</td>
      <td>45.99</td>
      <td>73.44</td>
      <td>35.46</td>
      <td>39.92</td>
      <td>69.69</td>
      <td>2.50</td>
      <td>5.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>9.04</td>
      <td>4.0</td>
      <td>True</td>
      <td>5dc039834e1ea42ac334458b2e3090fe3705cc59</td>
      <td>TheBloke/Project-Baize-v2-7B-GPTQ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-34b-Instruct-hf</td>
      <td>38.77</td>
      <td>40.78</td>
      <td>35.66</td>
      <td>39.72</td>
      <td>44.29</td>
      <td>74.51</td>
      <td>31.01</td>
      <td>5.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>154.0</td>
      <td>True</td>
      <td>c109b9dde086b31725fa09ff7effdc04c03c033d</td>
      <td>codellama/CodeLlama-34b-Instruct-hf</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/mosaicml/mpt-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-7b</td>
      <td>38.74</td>
      <td>47.70</td>
      <td>77.57</td>
      <td>30.80</td>
      <td>33.44</td>
      <td>72.14</td>
      <td>4.02</td>
      <td>5.55</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>1083.0</td>
      <td>True</td>
      <td>72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7</td>
      <td>mosaicml/mpt-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/anas-awadalla/mpt-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_anas-awadalla__mpt-7b</td>
      <td>38.74</td>
      <td>47.70</td>
      <td>77.57</td>
      <td>30.80</td>
      <td>33.44</td>
      <td>72.14</td>
      <td>4.02</td>
      <td>5.55</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.65</td>
      <td>1.0</td>
      <td>True</td>
      <td>b772e556c8e8a17d087db6935e7cd019e5eefb0f</td>
      <td>anas-awadalla/mpt-7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/openlm-research/open_llama_7b_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openlm-research__open_llama_7b_v2</td>
      <td>38.72</td>
      <td>43.69</td>
      <td>72.20</td>
      <td>41.29</td>
      <td>35.54</td>
      <td>69.38</td>
      <td>3.49</td>
      <td>5.49</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>86.0</td>
      <td>True</td>
      <td>e5961def23172a2384543940e773ab676033c963</td>
      <td>openlm-research/open_llama_7b_v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/gogpt-7b-bloom</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gogpt-7b-bloom</td>
      <td>38.61</td>
      <td>44.62</td>
      <td>62.56</td>
      <td>33.81</td>
      <td>40.61</td>
      <td>62.90</td>
      <td>0.00</td>
      <td>25.77</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>7.07</td>
      <td>3.0</td>
      <td>True</td>
      <td>8f9996f852db583b982efbd671465d18ad13ffae</td>
      <td>golaxy/gogpt-7b-bloom</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/GPT-JT-6B-v0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__GPT-JT-6B-v0</td>
      <td>38.37</td>
      <td>42.06</td>
      <td>67.96</td>
      <td>49.34</td>
      <td>38.89</td>
      <td>64.80</td>
      <td>1.21</td>
      <td>4.31</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>2.0</td>
      <td>True</td>
      <td>41bd1937dbc51f9e589d310bddab5b4c1409e783</td>
      <td>togethercomputer/GPT-JT-6B-v0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hyunseoki/ko-ref-llama2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hyunseoki__ko-ref-llama2-7b</td>
      <td>38.36</td>
      <td>42.66</td>
      <td>66.58</td>
      <td>30.41</td>
      <td>38.62</td>
      <td>66.22</td>
      <td>0.00</td>
      <td>24.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>1ee08c79ae7393473754b77e82b1472ef63d5dd2</td>
      <td>hyunseoki/ko-ref-llama2-7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/tiiuae/falcon-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_tiiuae__falcon-7b</td>
      <td>38.27</td>
      <td>47.87</td>
      <td>78.13</td>
      <td>27.79</td>
      <td>34.26</td>
      <td>72.38</td>
      <td>2.65</td>
      <td>4.82</td>
      <td>RWForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.92</td>
      <td>885.0</td>
      <td>True</td>
      <td>378337427557d1df3e742264a2901a49f25d4eb1</td>
      <td>tiiuae/falcon-7b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/WizardLM-30B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-30B-GPTQ</td>
      <td>38.24</td>
      <td>28.84</td>
      <td>26.08</td>
      <td>24.62</td>
      <td>49.14</td>
      <td>76.32</td>
      <td>34.42</td>
      <td>28.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>19.0</td>
      <td>True</td>
      <td>e2e97475a9775d2fe7afba098aee37e694b9220f</td>
      <td>TheBloke/WizardLM-30B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ziqingyang/chinese-llama-2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ziqingyang__chinese-llama-2-7b</td>
      <td>38.23</td>
      <td>44.45</td>
      <td>69.50</td>
      <td>37.47</td>
      <td>37.00</td>
      <td>68.98</td>
      <td>1.44</td>
      <td>8.77</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.70</td>
      <td>64.0</td>
      <td>True</td>
      <td>557b5cbd48a4a4eb5a08e975c4b6e11ac1ed4cbc</td>
      <td>ziqingyang/chinese-llama-2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/wenge-research/yayi-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wenge-research__yayi-7b</td>
      <td>37.99</td>
      <td>46.33</td>
      <td>61.72</td>
      <td>36.34</td>
      <td>43.70</td>
      <td>62.27</td>
      <td>0.91</td>
      <td>14.67</td>
      <td>BloomForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>7.07</td>
      <td>28.0</td>
      <td>True</td>
      <td>00be6c9e41a8367a855c6f18ebfa08f5ecdb2cc4</td>
      <td>wenge-research/yayi-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Writer/InstructPalmyra-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Writer__InstructPalmyra-20b</td>
      <td>37.98</td>
      <td>47.10</td>
      <td>73.00</td>
      <td>28.26</td>
      <td>41.81</td>
      <td>64.72</td>
      <td>2.58</td>
      <td>8.36</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.26</td>
      <td>36.0</td>
      <td>True</td>
      <td>c78df447c70d4677b128b1df864b9fff8338d900</td>
      <td>Writer/InstructPalmyra-20b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/codellama/CodeLlama-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-13b-hf</td>
      <td>37.91</td>
      <td>40.87</td>
      <td>63.35</td>
      <td>32.81</td>
      <td>43.79</td>
      <td>67.17</td>
      <td>12.13</td>
      <td>5.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>48.0</td>
      <td>True</td>
      <td>55876f398020b287ac845b34ca08089acf4f4bc3</td>
      <td>codellama/CodeLlama-13b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/CodeLlama-13b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__CodeLlama-13b-hf</td>
      <td>37.91</td>
      <td>40.87</td>
      <td>63.35</td>
      <td>32.81</td>
      <td>43.79</td>
      <td>67.17</td>
      <td>12.13</td>
      <td>5.25</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b7cfbbce945b966607d15ae275704922a6d04afc</td>
      <td>NousResearch/CodeLlama-13b-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/matsuo-lab/weblab-10b-instruction-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_matsuo-lab__weblab-10b-instruction-sft</td>
      <td>37.76</td>
      <td>40.10</td>
      <td>65.30</td>
      <td>26.66</td>
      <td>36.79</td>
      <td>64.09</td>
      <td>1.82</td>
      <td>29.56</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>10.47</td>
      <td>68.0</td>
      <td>True</td>
      <td>112a5ad9f556078ab14a5cd93511b9db4a0d4413</td>
      <td>matsuo-lab/weblab-10b-instruction-sft</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-raven-14b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-raven-14b</td>
      <td>37.74</td>
      <td>44.62</td>
      <td>71.25</td>
      <td>25.92</td>
      <td>41.93</td>
      <td>66.69</td>
      <td>2.12</td>
      <td>11.61</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.89</td>
      <td>47.0</td>
      <td>True</td>
      <td>359c0649b4f1d10a26ebea32908035bc00d152ee</td>
      <td>RWKV/rwkv-raven-14b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__CodeLlama-34B-Instruct-fp16</td>
      <td>37.69</td>
      <td>40.78</td>
      <td>35.66</td>
      <td>39.72</td>
      <td>44.29</td>
      <td>74.51</td>
      <td>23.05</td>
      <td>5.78</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>9.0</td>
      <td>True</td>
      <td>a4d0ce949de4d5b5f74691641efb5b70736a32a8</td>
      <td>TheBloke/CodeLlama-34B-Instruct-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lyogavin/Anima-7B-100K</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lyogavin__Anima-7B-100K</td>
      <td>37.66</td>
      <td>46.59</td>
      <td>72.28</td>
      <td>33.40</td>
      <td>37.84</td>
      <td>67.09</td>
      <td>0.68</td>
      <td>5.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>14.0</td>
      <td>True</td>
      <td>e303cf09e553c38ca5e0c0816d83631801ca5776</td>
      <td>lyogavin/Anima-7B-100K</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/gpt-neox-20b-full-precision</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__gpt-neox-20b-full-precision</td>
      <td>37.62</td>
      <td>48.81</td>
      <td>74.44</td>
      <td>26.16</td>
      <td>36.89</td>
      <td>68.27</td>
      <td>2.65</td>
      <td>6.15</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>20.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>20b347273d90da7c2c9eb4c32d4173dba862a0d2</td>
      <td>dvruette/gpt-neox-20b-full-precision</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/GPT-JT-6B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__GPT-JT-6B-v1</td>
      <td>37.60</td>
      <td>40.87</td>
      <td>67.15</td>
      <td>47.19</td>
      <td>37.07</td>
      <td>65.27</td>
      <td>1.21</td>
      <td>4.42</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>302.0</td>
      <td>True</td>
      <td>f34aa35f906895602c1f86f5685e598afdea8051</td>
      <td>togethercomputer/GPT-JT-6B-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__GPT-NeoXT-Chat-Base-20B</td>
      <td>37.59</td>
      <td>45.65</td>
      <td>74.03</td>
      <td>29.92</td>
      <td>34.51</td>
      <td>67.09</td>
      <td>6.90</td>
      <td>5.05</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.24</td>
      <td>691.0</td>
      <td>True</td>
      <td>d386708e84d862a65f7d2b4989f64750cb657227</td>
      <td>togethercomputer/GPT-NeoXT-Chat-Base-20B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/fairseq-dense-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__fairseq-dense-13B</td>
      <td>37.53</td>
      <td>40.36</td>
      <td>75.51</td>
      <td>27.07</td>
      <td>32.83</td>
      <td>67.96</td>
      <td>0.00</td>
      <td>18.96</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.84</td>
      <td>13.0</td>
      <td>True</td>
      <td>785793f6b216afd9fc664fc63e8e6c776a016825</td>
      <td>KoboldAI/fairseq-dense-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Xilabs/calypso-3b-alpha-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Xilabs__calypso-3b-alpha-v2</td>
      <td>37.52</td>
      <td>41.55</td>
      <td>71.48</td>
      <td>25.82</td>
      <td>35.73</td>
      <td>65.27</td>
      <td>0.68</td>
      <td>22.08</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>3.32</td>
      <td>4.0</td>
      <td>True</td>
      <td>933fb9db10f131f7ea54f4e6024ed2acf41c711a</td>
      <td>Xilabs/calypso-3b-alpha-v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/tiiuae/falcon-7b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_tiiuae__falcon-7b-instruct</td>
      <td>37.47</td>
      <td>46.16</td>
      <td>70.85</td>
      <td>25.84</td>
      <td>44.08</td>
      <td>67.96</td>
      <td>1.90</td>
      <td>5.54</td>
      <td>FalconForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.92</td>
      <td>676.0</td>
      <td>True</td>
      <td>cf4b3c42ce2fdfe24f753f0f0d179202fea59c99</td>
      <td>tiiuae/falcon-7b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/heegyu/RedTulu-Uncensored-3B-0719</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__RedTulu-Uncensored-3B-0719</td>
      <td>37.47</td>
      <td>40.02</td>
      <td>62.55</td>
      <td>30.37</td>
      <td>37.59</td>
      <td>62.35</td>
      <td>2.27</td>
      <td>27.10</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>c92bf022cddc3f57b4552ec3391df487295a2f87</td>
      <td>heegyu/RedTulu-Uncensored-3B-0719</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/oasst-gpt-neox-20b-1000-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-gpt-neox-20b-1000-steps</td>
      <td>37.44</td>
      <td>48.55</td>
      <td>74.61</td>
      <td>26.39</td>
      <td>35.63</td>
      <td>66.77</td>
      <td>3.11</td>
      <td>6.99</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>20.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>4aec11ef19103796fb21387ce925b63c9d61dae1</td>
      <td>dvruette/oasst-gpt-neox-20b-1000-steps</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/nomic-ai/gpt4all-j</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nomic-ai__gpt4all-j</td>
      <td>37.41</td>
      <td>41.98</td>
      <td>64.06</td>
      <td>28.20</td>
      <td>42.78</td>
      <td>64.72</td>
      <td>7.20</td>
      <td>12.94</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>250.0</td>
      <td>True</td>
      <td>73c15208cb608be2949b7c6e4ba6d88f0176c267</td>
      <td>nomic-ai/gpt4all-j</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/JosephusCheung/LL7M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_JosephusCheung__LL7M</td>
      <td>37.41</td>
      <td>44.97</td>
      <td>68.81</td>
      <td>34.44</td>
      <td>41.39</td>
      <td>64.09</td>
      <td>0.61</td>
      <td>7.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>cc-by-nc-nd-4.0</td>
      <td>6.64</td>
      <td>34.0</td>
      <td>True</td>
      <td>9b31bbf38a43d41eaf166fb3573f706b23cb1c13</td>
      <td>JosephusCheung/LL7M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_7b</td>
      <td>37.39</td>
      <td>43.94</td>
      <td>65.22</td>
      <td>29.97</td>
      <td>42.03</td>
      <td>66.06</td>
      <td>0.38</td>
      <td>14.14</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>6.61</td>
      <td>17.0</td>
      <td>True</td>
      <td>6ed0dca683685cb5b9e7df599f87d311f00ba6db</td>
      <td>psmathur/orca_mini_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/landmark-attention-llama7b-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__landmark-attention-llama7b-fp16</td>
      <td>37.39</td>
      <td>47.35</td>
      <td>65.81</td>
      <td>31.59</td>
      <td>42.63</td>
      <td>68.03</td>
      <td>1.59</td>
      <td>4.70</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>8.0</td>
      <td>False</td>
      <td>bf8bdcb0c30cceb0ceda33cf5fde683807e39a58</td>
      <td>TheBloke/landmark-attention-llama7b-fp16</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/tiiuae/falcon-7b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_tiiuae__falcon-7b-instruct</td>
      <td>37.38</td>
      <td>45.82</td>
      <td>70.78</td>
      <td>25.66</td>
      <td>44.07</td>
      <td>68.03</td>
      <td>1.74</td>
      <td>5.56</td>
      <td>RWForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>6.92</td>
      <td>676.0</td>
      <td>True</td>
      <td>eb410fb6ffa9028e97adb801f0d6ec46d02f8b07</td>
      <td>tiiuae/falcon-7b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-1024-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-gm-oasst1-en-1024-20b</td>
      <td>37.34</td>
      <td>48.04</td>
      <td>72.76</td>
      <td>25.96</td>
      <td>39.92</td>
      <td>66.30</td>
      <td>2.50</td>
      <td>5.91</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.24</td>
      <td>4.0</td>
      <td>True</td>
      <td>1a5b8d25587eab67d837621a6c9423e7ef6df289</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-1024-20b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Phind/Phind-CodeLlama-34B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Phind__Phind-CodeLlama-34B-v1</td>
      <td>37.34</td>
      <td>27.13</td>
      <td>28.28</td>
      <td>28.94</td>
      <td>44.94</td>
      <td>72.61</td>
      <td>20.47</td>
      <td>39.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>317.0</td>
      <td>True</td>
      <td>b073c9bb418ae52ca76b4ab48ac2dfbc8622f434</td>
      <td>Phind/Phind-CodeLlama-34B-v1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-66b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-66b</td>
      <td>37.31</td>
      <td>46.33</td>
      <td>76.25</td>
      <td>26.99</td>
      <td>35.43</td>
      <td>70.01</td>
      <td>1.14</td>
      <td>5.01</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>65.72</td>
      <td>171.0</td>
      <td>True</td>
      <td>7259969061237fe940036d22bea0fd349e4485e9</td>
      <td>facebook/opt-66b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/llm-agents/tora-code-13b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llm-agents__tora-code-13b-v1.0</td>
      <td>37.24</td>
      <td>44.45</td>
      <td>69.29</td>
      <td>36.67</td>
      <td>34.98</td>
      <td>62.59</td>
      <td>8.19</td>
      <td>4.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>4bf5b528d95a507b435c24a8986afe80d5951782</td>
      <td>llm-agents/tora-code-13b-v1.0</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Salesforce/codegen-16B-nl</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Salesforce__codegen-16B-nl</td>
      <td>37.22</td>
      <td>46.76</td>
      <td>71.87</td>
      <td>32.35</td>
      <td>33.95</td>
      <td>67.96</td>
      <td>2.65</td>
      <td>5.01</td>
      <td>CodeGenForCausalLM</td>
      <td>torch.float16</td>
      <td>bsd-3-clause</td>
      <td>15.72</td>
      <td>17.0</td>
      <td>True</td>
      <td>b65951b0cf7c5639f73caea801a892788608ed69</td>
      <td>Salesforce/codegen-16B-nl</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/oasst-gpt-neox-20b-3000-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-gpt-neox-20b-3000-steps</td>
      <td>37.18</td>
      <td>46.42</td>
      <td>72.08</td>
      <td>26.16</td>
      <td>35.53</td>
      <td>68.75</td>
      <td>2.88</td>
      <td>8.46</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>20.24</td>
      <td>0.0</td>
      <td>True</td>
      <td>f0462a8b7908f61202d86e6a9a2996d8339363b5</td>
      <td>dvruette/oasst-gpt-neox-20b-3000-steps</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Phind/Phind-CodeLlama-34B-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Phind__Phind-CodeLlama-34B-v2</td>
      <td>37.15</td>
      <td>24.57</td>
      <td>27.60</td>
      <td>25.76</td>
      <td>48.37</td>
      <td>71.82</td>
      <td>23.20</td>
      <td>38.70</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>365.0</td>
      <td>True</td>
      <td>949f61e203f91b412efe8f679c798f09f0ff4b0c</td>
      <td>Phind/Phind-CodeLlama-34B-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-oasst1-512-20b</td>
      <td>37.12</td>
      <td>46.93</td>
      <td>72.77</td>
      <td>26.25</td>
      <td>37.50</td>
      <td>68.03</td>
      <td>3.18</td>
      <td>5.18</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.24</td>
      <td>38.0</td>
      <td>True</td>
      <td>3bdf6f870ca14bcc5587b666fbe57488f7854d30</td>
      <td>h2oai/h2ogpt-oasst1-512-20b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/psmathur/orca_mini_13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_13b</td>
      <td>37.06</td>
      <td>42.06</td>
      <td>63.40</td>
      <td>35.43</td>
      <td>43.10</td>
      <td>64.17</td>
      <td>0.00</td>
      <td>11.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>12.85</td>
      <td>95.0</td>
      <td>True</td>
      <td>ca900c8f3145de40cd188c559b2901a2e4711546</td>
      <td>psmathur/orca_mini_13b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/openlm-research/open_llama_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openlm-research__open_llama_7b</td>
      <td>37.05</td>
      <td>47.01</td>
      <td>71.98</td>
      <td>30.49</td>
      <td>34.85</td>
      <td>67.96</td>
      <td>1.59</td>
      <td>5.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>99.0</td>
      <td>True</td>
      <td>6fb184ff23774c25bf84b3628e49c8b78372c7be</td>
      <td>openlm-research/open_llama_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/klosax/open_llama_13b_600bt_preview</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_klosax__open_llama_13b_600bt_preview</td>
      <td>36.94</td>
      <td>44.28</td>
      <td>72.43</td>
      <td>31.47</td>
      <td>34.66</td>
      <td>68.43</td>
      <td>1.97</td>
      <td>5.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>3465eaca4d293ccc6ce66888e6c8bd9032ae7071</td>
      <td>klosax/open_llama_13b_600bt_preview</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-7B-Instruct</td>
      <td>36.92</td>
      <td>44.11</td>
      <td>72.02</td>
      <td>37.62</td>
      <td>33.96</td>
      <td>64.96</td>
      <td>1.59</td>
      <td>4.21</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>103.0</td>
      <td>True</td>
      <td>95667a602ff2646bf67fe3a57c4eb9a1edec87fe</td>
      <td>togethercomputer/RedPajama-INCITE-7B-Instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-Instruct-7B-v0.1</td>
      <td>36.92</td>
      <td>44.11</td>
      <td>72.02</td>
      <td>37.62</td>
      <td>33.96</td>
      <td>64.96</td>
      <td>1.59</td>
      <td>4.21</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>103.0</td>
      <td>True</td>
      <td>95667a602ff2646bf67fe3a57c4eb9a1edec87fe</td>
      <td>togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/AlekseyKorshuk/pygmalion-6b-vicuna-chatml</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AlekseyKorshuk__pygmalion-6b-vicuna-chatml</td>
      <td>36.90</td>
      <td>40.61</td>
      <td>67.73</td>
      <td>33.92</td>
      <td>42.76</td>
      <td>63.06</td>
      <td>2.50</td>
      <td>7.71</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>2.0</td>
      <td>True</td>
      <td>ee3ada91a69a194cedfabbfeab98f1499b75cb44</td>
      <td>AlekseyKorshuk/pygmalion-6b-vicuna-chatml</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/Marx-3B-V2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__Marx-3B-V2</td>
      <td>36.89</td>
      <td>44.03</td>
      <td>72.92</td>
      <td>27.84</td>
      <td>39.92</td>
      <td>66.54</td>
      <td>1.21</td>
      <td>5.80</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>21.0</td>
      <td>True</td>
      <td>5fba568304f6f876f5b9e42026f986ea245b836b</td>
      <td>acrastt/Marx-3B-V2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__pythia-12b-sft-v8-7k-steps</td>
      <td>36.87</td>
      <td>44.03</td>
      <td>70.28</td>
      <td>26.55</td>
      <td>36.53</td>
      <td>65.27</td>
      <td>10.61</td>
      <td>4.84</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>21.0</td>
      <td>True</td>
      <td>275c9b71bfab4e271d1ed85515c61e317b6ef65e</td>
      <td>OpenAssistant/pythia-12b-sft-v8-7k-steps</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Writer/palmyra-large</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Writer__palmyra-large</td>
      <td>36.80</td>
      <td>44.97</td>
      <td>71.85</td>
      <td>28.54</td>
      <td>35.93</td>
      <td>67.88</td>
      <td>3.41</td>
      <td>5.02</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.26</td>
      <td>17.0</td>
      <td>True</td>
      <td>40086d791942cb28f55e679cd3fb6f6b5ba4effd</td>
      <td>Writer/palmyra-large</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TehVenom/Moderator-Chan_GPT-JT-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Moderator-Chan_GPT-JT-6b</td>
      <td>36.79</td>
      <td>43.69</td>
      <td>70.77</td>
      <td>35.61</td>
      <td>36.05</td>
      <td>65.59</td>
      <td>1.29</td>
      <td>4.56</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>0.0</td>
      <td>True</td>
      <td>f2b7cda25f6965c1551fa78e9e38676994bc6638</td>
      <td>TehVenom/Moderator-Chan_GPT-JT-6b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/RobbeD/OpenLlama-Platypus-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RobbeD__OpenLlama-Platypus-3B</td>
      <td>36.76</td>
      <td>41.21</td>
      <td>71.67</td>
      <td>29.86</td>
      <td>36.45</td>
      <td>65.98</td>
      <td>1.14</td>
      <td>11.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>3.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>d3a0bf8e1181be02cc9c4c4cdfedaedacaefbfac</td>
      <td>RobbeD/OpenLlama-Platypus-3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-gm-oasst1-multilang-1024-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-gm-oasst1-multilang-1024-20b</td>
      <td>36.71</td>
      <td>47.44</td>
      <td>72.58</td>
      <td>26.37</td>
      <td>34.39</td>
      <td>68.43</td>
      <td>2.20</td>
      <td>5.60</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.24</td>
      <td>9.0</td>
      <td>True</td>
      <td>b3a6bf4250a037c09e451344e2a4e987011b79de</td>
      <td>h2oai/h2ogpt-gm-oasst1-multilang-1024-20b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Fredithefish/ReasonixPajama-3B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__ReasonixPajama-3B-HF</td>
      <td>36.71</td>
      <td>39.25</td>
      <td>63.47</td>
      <td>26.09</td>
      <td>55.42</td>
      <td>63.69</td>
      <td>0.53</td>
      <td>8.52</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.91</td>
      <td>3.0</td>
      <td>True</td>
      <td>fa87c904b5921231b9f6f94b9c537cdda8783b96</td>
      <td>Fredithefish/ReasonixPajama-3B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-2.5k-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__pythia-12b-sft-v8-2.5k-steps</td>
      <td>36.67</td>
      <td>42.32</td>
      <td>70.15</td>
      <td>27.36</td>
      <td>36.75</td>
      <td>65.67</td>
      <td>9.55</td>
      <td>4.92</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>142e306db8e279a07c557ea5a919ab7e7a4af17c</td>
      <td>OpenAssistant/pythia-12b-sft-v8-2.5k-steps</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-30b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-30b</td>
      <td>36.57</td>
      <td>43.26</td>
      <td>74.07</td>
      <td>26.66</td>
      <td>35.16</td>
      <td>70.64</td>
      <td>1.14</td>
      <td>5.05</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>29.98</td>
      <td>133.0</td>
      <td>True</td>
      <td>ceea0a90ac0f6fae7c2c34bcb40477438c152546</td>
      <td>facebook/opt-30b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/AlekseyKorshuk/chatml-pyg-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AlekseyKorshuk__chatml-pyg-v1</td>
      <td>36.56</td>
      <td>37.88</td>
      <td>63.29</td>
      <td>32.77</td>
      <td>42.61</td>
      <td>62.51</td>
      <td>5.16</td>
      <td>11.72</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>79d5a4d53953ca1c26bc2155f168b7e2108f377f</td>
      <td>AlekseyKorshuk/chatml-pyg-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/Marx-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__Marx-3B</td>
      <td>36.50</td>
      <td>43.17</td>
      <td>72.68</td>
      <td>28.46</td>
      <td>39.09</td>
      <td>65.59</td>
      <td>1.29</td>
      <td>5.22</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>11.0</td>
      <td>True</td>
      <td>c0dcc44989cf4e006efae31abbcef7e8be8547c0</td>
      <td>acrastt/Marx-3B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/togethercomputer/GPT-JT-Moderation-6B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__GPT-JT-Moderation-6B</td>
      <td>36.42</td>
      <td>40.53</td>
      <td>67.66</td>
      <td>41.63</td>
      <td>37.33</td>
      <td>62.67</td>
      <td>0.99</td>
      <td>4.15</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>30.0</td>
      <td>True</td>
      <td>1297870783f6091294769014afddf94499966a78</td>
      <td>togethercomputer/GPT-JT-Moderation-6B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/glaiveai/glaive-coder-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_glaiveai__glaive-coder-7b</td>
      <td>36.42</td>
      <td>42.66</td>
      <td>64.69</td>
      <td>37.15</td>
      <td>39.88</td>
      <td>59.75</td>
      <td>5.23</td>
      <td>5.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>43.0</td>
      <td>True</td>
      <td>72a255a58480ef0713eed988312fe82f77f94f37</td>
      <td>glaiveai/glaive-coder-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/oasst-pythia-12b-pretrained-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-pythia-12b-pretrained-sft</td>
      <td>36.41</td>
      <td>45.31</td>
      <td>67.67</td>
      <td>27.81</td>
      <td>38.16</td>
      <td>65.90</td>
      <td>4.02</td>
      <td>5.98</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>11.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>c21fbece4253841f2d6e15f04f60fe1ba6f990dd</td>
      <td>dvruette/oasst-pythia-12b-pretrained-sft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/pygmalion-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__pygmalion-6b</td>
      <td>36.37</td>
      <td>40.53</td>
      <td>67.47</td>
      <td>25.73</td>
      <td>32.53</td>
      <td>62.51</td>
      <td>2.05</td>
      <td>23.75</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>690.0</td>
      <td>True</td>
      <td>30e2405100eac6bd53f75964cc7345eeafd19f7d</td>
      <td>PygmalionAI/pygmalion-6b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-rlhf-2k-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__pythia-12b-sft-v8-rlhf-2k-steps</td>
      <td>36.36</td>
      <td>43.43</td>
      <td>70.08</td>
      <td>26.12</td>
      <td>36.06</td>
      <td>64.64</td>
      <td>9.55</td>
      <td>4.63</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>a0debfed4a020d449e3d00f4e75f2c2aefb68db3</td>
      <td>OpenAssistant/pythia-12b-sft-v8-rlhf-2k-steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_harborwater__open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
      <td>36.33</td>
      <td>41.81</td>
      <td>73.01</td>
      <td>26.36</td>
      <td>38.99</td>
      <td>66.69</td>
      <td>1.90</td>
      <td>5.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>4da0c661e6df1235c9997b996c8e395b87248406</td>
      <td>harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hakurei/instruct-12b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hakurei__instruct-12b</td>
      <td>36.31</td>
      <td>42.58</td>
      <td>66.76</td>
      <td>26.79</td>
      <td>31.96</td>
      <td>63.46</td>
      <td>0.23</td>
      <td>22.38</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>17.0</td>
      <td>True</td>
      <td>ff4699b502b79c716330b6f761002588a65dcba6</td>
      <td>hakurei/instruct-12b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/harborwater/open-llama-3b-everything-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_harborwater__open-llama-3b-everything-v2</td>
      <td>36.29</td>
      <td>42.83</td>
      <td>73.28</td>
      <td>26.87</td>
      <td>37.26</td>
      <td>66.61</td>
      <td>1.59</td>
      <td>5.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>31ce2c1611d9f7d56184ceb5bff6a7e95a180c03</td>
      <td>harborwater/open-llama-3b-everything-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__oasst-sft-4-pythia-12b-epoch-3.5</td>
      <td>36.26</td>
      <td>45.73</td>
      <td>68.59</td>
      <td>26.82</td>
      <td>37.81</td>
      <td>65.90</td>
      <td>3.03</td>
      <td>5.91</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>335.0</td>
      <td>True</td>
      <td>626b8c140cfdedb119dfb78c626cd772283dee33</td>
      <td>OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CobraMamba/mamba-gpt-3b-v4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CobraMamba__mamba-gpt-3b-v4</td>
      <td>36.24</td>
      <td>42.58</td>
      <td>71.04</td>
      <td>30.04</td>
      <td>37.26</td>
      <td>65.82</td>
      <td>0.68</td>
      <td>6.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>49cdf710c1a9178ddf616da79211fdcdb2170c3f</td>
      <td>CobraMamba/mamba-gpt-3b-v4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/chargoddard/llama-2-34b-uncode</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_chargoddard__llama-2-34b-uncode</td>
      <td>36.20</td>
      <td>39.51</td>
      <td>33.90</td>
      <td>38.49</td>
      <td>40.94</td>
      <td>74.35</td>
      <td>20.77</td>
      <td>5.43</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>33.74</td>
      <td>3.0</td>
      <td>True</td>
      <td>d434d06249feb6ca511b0a09162130bcc59d84e3</td>
      <td>chargoddard/llama-2-34b-uncode</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/GeneZC/MiniMA-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_GeneZC__MiniMA-3B</td>
      <td>36.20</td>
      <td>43.43</td>
      <td>68.06</td>
      <td>28.69</td>
      <td>39.76</td>
      <td>65.98</td>
      <td>2.73</td>
      <td>4.72</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>6.0</td>
      <td>True</td>
      <td>0a2f9d6bbb3959d68fe52e07ee6f54e8242f91ec</td>
      <td>GeneZC/MiniMA-3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CobraMamba/mamba-gpt-3b-v3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CobraMamba__mamba-gpt-3b-v3</td>
      <td>36.13</td>
      <td>41.72</td>
      <td>71.05</td>
      <td>27.31</td>
      <td>37.86</td>
      <td>67.48</td>
      <td>1.21</td>
      <td>6.27</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>12.0</td>
      <td>True</td>
      <td>d860a90ef6b30c695b985dd2ff382d4bbb80e857</td>
      <td>CobraMamba/mamba-gpt-3b-v3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/kfkas/Llama-2-ko-7b-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_kfkas__Llama-2-ko-7b-Chat</td>
      <td>36.12</td>
      <td>40.44</td>
      <td>67.16</td>
      <td>30.40</td>
      <td>35.48</td>
      <td>66.85</td>
      <td>1.29</td>
      <td>11.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.67</td>
      <td>51.0</td>
      <td>True</td>
      <td>3293b98cd8204371988f898dafa9b5a297555cbe</td>
      <td>kfkas/Llama-2-ko-7b-Chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/kfkas/Llama-2-ko-7b-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_kfkas__Llama-2-ko-7b-Chat</td>
      <td>36.11</td>
      <td>40.44</td>
      <td>67.12</td>
      <td>30.19</td>
      <td>35.45</td>
      <td>66.61</td>
      <td>1.67</td>
      <td>11.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>6.67</td>
      <td>51.0</td>
      <td>True</td>
      <td>3293b98cd8204371988f898dafa9b5a297555cbe</td>
      <td>kfkas/Llama-2-ko-7b-Chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/GPT-NeoX-20B-Erebus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__GPT-NeoX-20B-Erebus</td>
      <td>36.09</td>
      <td>45.48</td>
      <td>72.79</td>
      <td>26.77</td>
      <td>32.15</td>
      <td>68.11</td>
      <td>2.27</td>
      <td>5.08</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.24</td>
      <td>70.0</td>
      <td>True</td>
      <td>1a80940a290452af71caf17a8e520955eb338e0f</td>
      <td>KoboldAI/GPT-NeoX-20B-Erebus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/fairseq-dense-6.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__fairseq-dense-6.7B</td>
      <td>36.09</td>
      <td>39.42</td>
      <td>71.26</td>
      <td>26.91</td>
      <td>32.73</td>
      <td>65.27</td>
      <td>0.00</td>
      <td>17.05</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.65</td>
      <td>2.0</td>
      <td>True</td>
      <td>d62d83b8eb7a6ba012a762752a5b5679add3b40c</td>
      <td>KoboldAI/fairseq-dense-6.7B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-Base-7B-v0.1</td>
      <td>36.09</td>
      <td>46.25</td>
      <td>71.63</td>
      <td>27.68</td>
      <td>33.03</td>
      <td>67.32</td>
      <td>1.59</td>
      <td>5.11</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>89.0</td>
      <td>True</td>
      <td>78f7e482443971f4873ba3239f0ac810a367833b</td>
      <td>togethercomputer/RedPajama-INCITE-Base-7B-v0.1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-7B-Base</td>
      <td>36.09</td>
      <td>46.25</td>
      <td>71.63</td>
      <td>27.68</td>
      <td>33.03</td>
      <td>67.32</td>
      <td>1.59</td>
      <td>5.11</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>89.0</td>
      <td>True</td>
      <td>78f7e482443971f4873ba3239f0ac810a367833b</td>
      <td>togethercomputer/RedPajama-INCITE-7B-Base</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_harborwater__open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
      <td>36.03</td>
      <td>41.21</td>
      <td>72.88</td>
      <td>25.39</td>
      <td>38.87</td>
      <td>66.61</td>
      <td>1.59</td>
      <td>5.68</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>4da0c661e6df1235c9997b996c8e395b87248406</td>
      <td>harborwater/open-llama-3b-v2-wizard-evol-instuct-v2-196k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/Griffin-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__Griffin-3B</td>
      <td>36.03</td>
      <td>41.81</td>
      <td>72.30</td>
      <td>26.36</td>
      <td>38.33</td>
      <td>67.01</td>
      <td>0.99</td>
      <td>5.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>edbea6fe86d0bc2673c10269828008a1cb451919</td>
      <td>acrastt/Griffin-3B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/gpt-neox-20b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-neox-20b</td>
      <td>36.02</td>
      <td>45.73</td>
      <td>73.45</td>
      <td>25.00</td>
      <td>31.61</td>
      <td>68.90</td>
      <td>2.43</td>
      <td>5.04</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.74</td>
      <td>433.0</td>
      <td>True</td>
      <td>9369f145ca7b66ef62760f9351af951b2d53b77f</td>
      <td>EleutherAI/gpt-neox-20b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/harborwater/wizard-orca-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_harborwater__wizard-orca-3b</td>
      <td>35.93</td>
      <td>41.72</td>
      <td>71.78</td>
      <td>24.49</td>
      <td>40.04</td>
      <td>66.93</td>
      <td>1.06</td>
      <td>5.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>ffc81b58375342f12e38a67272d95458a72e8d09</td>
      <td>harborwater/wizard-orca-3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/Puma-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__Puma-3B</td>
      <td>35.93</td>
      <td>41.30</td>
      <td>71.85</td>
      <td>27.51</td>
      <td>38.34</td>
      <td>66.38</td>
      <td>0.76</td>
      <td>5.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>1159e9cdd05c03d31331f329ba58e4e3444943be</td>
      <td>acrastt/Puma-3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/pythia-12b-pre-v8-12.5k-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__pythia-12b-pre-v8-12.5k-steps</td>
      <td>35.93</td>
      <td>41.47</td>
      <td>68.80</td>
      <td>26.58</td>
      <td>36.82</td>
      <td>65.27</td>
      <td>7.66</td>
      <td>4.89</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>6.0</td>
      <td>True</td>
      <td>37ca702e957a4b740689d67c58c284224e2fbae2</td>
      <td>OpenAssistant/pythia-12b-pre-v8-12.5k-steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/GPT-NeoX-20B-Skein</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__GPT-NeoX-20B-Skein</td>
      <td>35.93</td>
      <td>44.97</td>
      <td>72.68</td>
      <td>25.99</td>
      <td>31.64</td>
      <td>68.43</td>
      <td>2.88</td>
      <td>4.89</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>20.24</td>
      <td>9.0</td>
      <td>True</td>
      <td>dd98d514b5aff4e820922c88a73d6d5bf17f332e</td>
      <td>KoboldAI/GPT-NeoX-20B-Skein</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__oasst-sft-1-pythia-12b</td>
      <td>35.84</td>
      <td>46.42</td>
      <td>70.00</td>
      <td>26.19</td>
      <td>39.19</td>
      <td>62.19</td>
      <td>0.61</td>
      <td>6.30</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.58</td>
      <td>277.0</td>
      <td>True</td>
      <td>293df535fe7711a5726987fc2f17dfc87de452a1</td>
      <td>OpenAssistant/oasst-sft-1-pythia-12b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/dvruette/oasst-pythia-12b-6000-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-pythia-12b-6000-steps</td>
      <td>35.79</td>
      <td>45.39</td>
      <td>69.68</td>
      <td>25.97</td>
      <td>39.85</td>
      <td>63.22</td>
      <td>0.53</td>
      <td>5.90</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>11.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>e2ccc0ef8d1cc5ffc8b0e2e885f03ef50597ea8a</td>
      <td>dvruette/oasst-pythia-12b-6000-steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/AtAndDev/ShortKing-3b-v0.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AtAndDev__ShortKing-3b-v0.3</td>
      <td>35.75</td>
      <td>40.96</td>
      <td>70.72</td>
      <td>26.21</td>
      <td>38.78</td>
      <td>66.93</td>
      <td>1.21</td>
      <td>5.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-4.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>4bcf1610eb1f3959568d5acee74833c41502bf04</td>
      <td>AtAndDev/ShortKing-3b-v0.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/GPT-R</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__GPT-R</td>
      <td>35.71</td>
      <td>41.21</td>
      <td>66.89</td>
      <td>36.50</td>
      <td>34.22</td>
      <td>64.40</td>
      <td>1.59</td>
      <td>5.14</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-openrail-m</td>
      <td>5.84</td>
      <td>9.0</td>
      <td>True</td>
      <td>92b955a3ff74aa577fa0d8517dfc314847ef60af</td>
      <td>digitous/GPT-R</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Phind/Phind-CodeLlama-34B-Python-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Phind__Phind-CodeLlama-34B-Python-v1</td>
      <td>35.69</td>
      <td>24.66</td>
      <td>29.77</td>
      <td>27.95</td>
      <td>45.27</td>
      <td>68.82</td>
      <td>21.53</td>
      <td>31.85</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>231.0</td>
      <td>True</td>
      <td>3aabef8c9bc1b3ec2fffed053645bc1e2d829b6c</td>
      <td>Phind/Phind-CodeLlama-34B-Python-v1</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/dvruette/oasst-pythia-12b-flash-attn-5000-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-pythia-12b-flash-attn-5000-steps</td>
      <td>35.69</td>
      <td>44.97</td>
      <td>69.75</td>
      <td>26.64</td>
      <td>38.89</td>
      <td>63.14</td>
      <td>0.99</td>
      <td>5.48</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>11.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>5227ec9c9def4b0bdf6c7ad95d9f77cbf458283d</td>
      <td>dvruette/oasst-pythia-12b-flash-attn-5000-steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_princeton-nlp__Sheared-LLaMA-2.7B</td>
      <td>35.66</td>
      <td>41.72</td>
      <td>71.01</td>
      <td>26.92</td>
      <td>37.32</td>
      <td>67.01</td>
      <td>1.06</td>
      <td>4.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.62</td>
      <td>24.0</td>
      <td>True</td>
      <td>16347024c4df6cd114720958964a850fc287cac0</td>
      <td>princeton-nlp/Sheared-LLaMA-2.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/l3utterfly/open-llama-3b-v2-layla</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_l3utterfly__open-llama-3b-v2-layla</td>
      <td>35.63</td>
      <td>38.23</td>
      <td>66.43</td>
      <td>28.56</td>
      <td>44.40</td>
      <td>62.83</td>
      <td>1.06</td>
      <td>7.88</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>465669ddafad25393ac3cfe94d3726cced112b30</td>
      <td>l3utterfly/open-llama-3b-v2-layla</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-1024-12b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-gm-oasst1-en-1024-12b</td>
      <td>35.60</td>
      <td>43.09</td>
      <td>69.75</td>
      <td>25.87</td>
      <td>38.00</td>
      <td>66.14</td>
      <td>1.06</td>
      <td>5.27</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.59</td>
      <td>5.0</td>
      <td>True</td>
      <td>e547fffafb382fd39ef5de35ba3b5afc1b43e74d</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-1024-12b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/harborwater/open-llama-3b-everythingLM-2048</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_harborwater__open-llama-3b-everythingLM-2048</td>
      <td>35.58</td>
      <td>42.75</td>
      <td>71.72</td>
      <td>27.16</td>
      <td>34.26</td>
      <td>66.30</td>
      <td>1.52</td>
      <td>5.35</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>1.0</td>
      <td>True</td>
      <td>1f9e8d48163feb63ed190eaa982f393542a75d30</td>
      <td>harborwater/open-llama-3b-everythingLM-2048</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psmathur/orca_mini_3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psmathur__orca_mini_3b</td>
      <td>35.50</td>
      <td>41.55</td>
      <td>61.52</td>
      <td>26.79</td>
      <td>42.42</td>
      <td>61.80</td>
      <td>0.08</td>
      <td>14.33</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>3.32</td>
      <td>129.0</td>
      <td>True</td>
      <td>fd2754e80ce80757a3a68a840d7d287dd7def676</td>
      <td>psmathur/orca_mini_3b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/amazon/LightGPT</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_amazon__LightGPT</td>
      <td>35.49</td>
      <td>39.93</td>
      <td>63.82</td>
      <td>28.45</td>
      <td>36.69</td>
      <td>64.48</td>
      <td>3.87</td>
      <td>11.19</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>64.0</td>
      <td>True</td>
      <td>1f6ffd8f162030396a3bc1ca2e3504896dbe6434</td>
      <td>amazon/LightGPT</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Rallio67/7B-redpajama-conditional-alpha</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Rallio67__7B-redpajama-conditional-alpha</td>
      <td>35.46</td>
      <td>42.58</td>
      <td>69.91</td>
      <td>26.53</td>
      <td>36.42</td>
      <td>67.17</td>
      <td>0.76</td>
      <td>4.86</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.65</td>
      <td>7.0</td>
      <td>True</td>
      <td>9a3f69a1eba3618930f222d4e013d534102a2af5</td>
      <td>Rallio67/7B-redpajama-conditional-alpha</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Javalion-R</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Javalion-R</td>
      <td>35.42</td>
      <td>41.72</td>
      <td>68.02</td>
      <td>30.81</td>
      <td>34.44</td>
      <td>65.43</td>
      <td>2.65</td>
      <td>4.85</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>5.0</td>
      <td>True</td>
      <td>b881231ab6ea85da2a9a139f282df85d1d18b002</td>
      <td>digitous/Javalion-R</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/dvruette/oasst-pythia-12b-reference</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-pythia-12b-reference</td>
      <td>35.41</td>
      <td>43.00</td>
      <td>67.91</td>
      <td>28.33</td>
      <td>36.57</td>
      <td>64.96</td>
      <td>1.21</td>
      <td>5.91</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>11.58</td>
      <td>0.0</td>
      <td>True</td>
      <td>c5a9b7fad884e6c45ce5d2ca551aa1c03db6865f</td>
      <td>dvruette/oasst-pythia-12b-reference</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-oasst1-512-12b</td>
      <td>35.41</td>
      <td>42.32</td>
      <td>70.24</td>
      <td>26.01</td>
      <td>36.41</td>
      <td>66.22</td>
      <td>1.67</td>
      <td>5.00</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.59</td>
      <td>26.0</td>
      <td>True</td>
      <td>c6bb0fe363e0105839d34ca757793b61c9606f95</td>
      <td>h2oai/h2ogpt-oasst1-512-12b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/cmarkea/bloomz-3b-sft-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cmarkea__bloomz-3b-sft-chat</td>
      <td>35.35</td>
      <td>36.86</td>
      <td>54.34</td>
      <td>31.49</td>
      <td>39.69</td>
      <td>58.88</td>
      <td>0.38</td>
      <td>25.82</td>
      <td>BloomForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>3.00</td>
      <td>8.0</td>
      <td>True</td>
      <td>a35b6ae6809891e253b45fb5795979c33992e548</td>
      <td>cmarkea/bloomz-3b-sft-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Javelin-R</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Javelin-R</td>
      <td>35.33</td>
      <td>41.64</td>
      <td>69.01</td>
      <td>30.70</td>
      <td>34.50</td>
      <td>64.80</td>
      <td>1.67</td>
      <td>5.01</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>2.0</td>
      <td>True</td>
      <td>4c4a5caf5d9049a47f5565b72e5a53dede08ac8b</td>
      <td>digitous/Javelin-R</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/RWKV/rwkv-raven-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-raven-7b</td>
      <td>35.31</td>
      <td>39.42</td>
      <td>66.48</td>
      <td>23.64</td>
      <td>38.56</td>
      <td>62.90</td>
      <td>0.30</td>
      <td>15.84</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.19</td>
      <td>16.0</td>
      <td>True</td>
      <td>a2dfc9f659be13556a25d9e38da642c6f67aeee3</td>
      <td>RWKV/rwkv-raven-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/vihangd/smartyplats-3b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vihangd__smartyplats-3b-v2</td>
      <td>35.30</td>
      <td>41.04</td>
      <td>71.19</td>
      <td>24.32</td>
      <td>36.66</td>
      <td>66.93</td>
      <td>1.59</td>
      <td>5.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>920609897049f674bc4a9678579f6869f6cbed13</td>
      <td>vihangd/smartyplats-3b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-33B-V1.0-Uncensored-GPTQ</td>
      <td>35.29</td>
      <td>27.39</td>
      <td>26.03</td>
      <td>25.81</td>
      <td>48.90</td>
      <td>77.90</td>
      <td>24.56</td>
      <td>16.45</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>37.0</td>
      <td>True</td>
      <td>1c65902c620fcdf6b9c8e36ce17f21360e186a1e</td>
      <td>TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/openlm-research/open_llama_3b_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openlm-research__open_llama_3b_v2</td>
      <td>35.26</td>
      <td>40.27</td>
      <td>71.60</td>
      <td>27.12</td>
      <td>34.78</td>
      <td>67.01</td>
      <td>0.91</td>
      <td>5.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.32</td>
      <td>61.0</td>
      <td>True</td>
      <td>bce5d60d3b0c68318862270ec4e794d83308d80a</td>
      <td>openlm-research/open_llama_3b_v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/openllama_3b_EvolInstruct_lora_merged</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__openllama_3b_EvolInstruct_lora_merged</td>
      <td>35.26</td>
      <td>40.27</td>
      <td>71.60</td>
      <td>27.12</td>
      <td>34.78</td>
      <td>67.01</td>
      <td>0.91</td>
      <td>5.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-4.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>c55e3e114951346f273c519d266170e4d52781e9</td>
      <td>KnutJaegersberg/openllama_3b_EvolInstruct_lora_merged</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardCoder-Python-7B-V1.0</td>
      <td>35.26</td>
      <td>41.81</td>
      <td>65.06</td>
      <td>32.29</td>
      <td>36.32</td>
      <td>61.72</td>
      <td>4.70</td>
      <td>4.93</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>44.0</td>
      <td>True</td>
      <td>e40673a27a4aefcff2c6d2b3b1e0681a38703e4e</td>
      <td>WizardLM/WizardCoder-Python-7B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Pirr/pythia-13b-deduped-green_devil</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Pirr__pythia-13b-deduped-green_devil</td>
      <td>35.24</td>
      <td>42.32</td>
      <td>68.89</td>
      <td>26.01</td>
      <td>35.56</td>
      <td>66.93</td>
      <td>2.12</td>
      <td>4.84</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>11.58</td>
      <td>9.0</td>
      <td>True</td>
      <td>7faeb395c26189eeab9bf3a98994696687ad31a3</td>
      <td>Pirr/pythia-13b-deduped-green_devil</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloomz-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloomz-3b</td>
      <td>35.22</td>
      <td>36.86</td>
      <td>54.95</td>
      <td>32.91</td>
      <td>40.34</td>
      <td>57.14</td>
      <td>0.00</td>
      <td>24.36</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>3.00</td>
      <td>70.0</td>
      <td>True</td>
      <td>31eefcb2bcd69632925adf07e090debafe95436d</td>
      <td>bigscience/bloomz-3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/Bean-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__Bean-3B</td>
      <td>35.20</td>
      <td>40.36</td>
      <td>72.00</td>
      <td>26.43</td>
      <td>36.11</td>
      <td>65.67</td>
      <td>0.53</td>
      <td>5.28</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>4a1ce189a3fb1d58b3fa47ebe30b3c037592670c</td>
      <td>acrastt/Bean-3B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/TheBloke/CodeLlama-34B-Python-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__CodeLlama-34B-Python-fp16</td>
      <td>35.19</td>
      <td>38.14</td>
      <td>34.80</td>
      <td>32.95</td>
      <td>43.57</td>
      <td>72.14</td>
      <td>20.02</td>
      <td>4.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>11.0</td>
      <td>True</td>
      <td>875f9d97fb6c9619d8867887dd1d80918ff0f593</td>
      <td>TheBloke/CodeLlama-34B-Python-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Javelin-GPTJ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Javelin-GPTJ</td>
      <td>35.16</td>
      <td>42.66</td>
      <td>70.45</td>
      <td>26.20</td>
      <td>36.08</td>
      <td>64.17</td>
      <td>1.82</td>
      <td>4.77</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>4.0</td>
      <td>True</td>
      <td>bee7068ab002784420a1a30170db3906185359f2</td>
      <td>digitous/Javelin-GPTJ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/llm-agents/tora-code-7b-v1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_llm-agents__tora-code-7b-v1.0</td>
      <td>35.16</td>
      <td>40.70</td>
      <td>65.86</td>
      <td>33.34</td>
      <td>34.84</td>
      <td>61.56</td>
      <td>4.93</td>
      <td>4.90</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>7.0</td>
      <td>True</td>
      <td>777501b69bb0ba2675abdcaf7b1309ab05320c2e</td>
      <td>llm-agents/tora-code-7b-v1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Janin-R</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Janin-R</td>
      <td>35.14</td>
      <td>40.44</td>
      <td>67.36</td>
      <td>31.24</td>
      <td>34.49</td>
      <td>65.35</td>
      <td>2.27</td>
      <td>4.80</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>f6963f77098d8421ff4a1cf4d36f1e94c6c8f44b</td>
      <td>digitous/Janin-R</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TaylorAI/Flash-Llama-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TaylorAI__Flash-Llama-3B</td>
      <td>35.13</td>
      <td>40.10</td>
      <td>71.56</td>
      <td>26.88</td>
      <td>34.74</td>
      <td>66.61</td>
      <td>0.91</td>
      <td>5.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>3.32</td>
      <td>2.0</td>
      <td>True</td>
      <td>b4c7bb49171ff6955cfc1f7e33143383c57f7606</td>
      <td>TaylorAI/Flash-Llama-3B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-34b-Python-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-34b-Python-hf</td>
      <td>35.12</td>
      <td>40.19</td>
      <td>36.82</td>
      <td>34.79</td>
      <td>44.28</td>
      <td>71.19</td>
      <td>14.33</td>
      <td>4.26</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>33.74</td>
      <td>61.0</td>
      <td>True</td>
      <td>3dd8ab05bbd273b9f77088b1d4015b7f1848793d</td>
      <td>codellama/CodeLlama-34b-Python-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/Dolly_Shygmalion-6b-Dev_V8P2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Dolly_Shygmalion-6b-Dev_V8P2</td>
      <td>35.12</td>
      <td>41.38</td>
      <td>67.67</td>
      <td>28.48</td>
      <td>36.86</td>
      <td>64.33</td>
      <td>1.97</td>
      <td>5.15</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>5.0</td>
      <td>True</td>
      <td>6413b1d9e8b58df9d3aac91a862e8d505d8c6716</td>
      <td>TehVenom/Dolly_Shygmalion-6b-Dev_V8P2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/CodeLlama-34b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__CodeLlama-34b-hf</td>
      <td>35.11</td>
      <td>37.54</td>
      <td>31.84</td>
      <td>37.20</td>
      <td>38.89</td>
      <td>73.40</td>
      <td>21.61</td>
      <td>5.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>33.48</td>
      <td>3.0</td>
      <td>True</td>
      <td>4e61ec70eb258047f5bc689fa6a66f7753da52b8</td>
      <td>NousResearch/CodeLlama-34b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/gpt-j-6B-Dolly</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__gpt-j-6B-Dolly</td>
      <td>35.10</td>
      <td>41.30</td>
      <td>65.97</td>
      <td>26.78</td>
      <td>37.91</td>
      <td>64.72</td>
      <td>0.91</td>
      <td>8.10</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>83d8c754aac12f838d7c847d4352a09396c383d0</td>
      <td>Corianas/gpt-j-6B-Dolly</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-7b-Instruct-hf</td>
      <td>35.06</td>
      <td>36.52</td>
      <td>55.44</td>
      <td>34.54</td>
      <td>41.25</td>
      <td>64.56</td>
      <td>7.96</td>
      <td>5.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>76.0</td>
      <td>True</td>
      <td>7affc442e639b8aa1c4b3e98a10a2f45a21b8b4f</td>
      <td>codellama/CodeLlama-7b-Instruct-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/vihangd/smartyplats-3b-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vihangd__smartyplats-3b-v1</td>
      <td>35.06</td>
      <td>40.53</td>
      <td>70.85</td>
      <td>25.31</td>
      <td>36.53</td>
      <td>65.75</td>
      <td>1.06</td>
      <td>5.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>89272b9edb323f5ace09e097a6449554c0dcd4e7</td>
      <td>vihangd/smartyplats-3b-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Danielbrdz/CodeBarcenas-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Danielbrdz__CodeBarcenas-7b</td>
      <td>35.03</td>
      <td>42.32</td>
      <td>63.43</td>
      <td>33.39</td>
      <td>38.51</td>
      <td>60.38</td>
      <td>2.50</td>
      <td>4.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>fe7a232baac5394e821f349cb7ef31dbd4ca2078</td>
      <td>Danielbrdz/CodeBarcenas-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/GPT-J-6B-Skein</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__GPT-J-6B-Skein</td>
      <td>35.00</td>
      <td>42.58</td>
      <td>68.69</td>
      <td>24.88</td>
      <td>38.70</td>
      <td>63.85</td>
      <td>1.44</td>
      <td>4.86</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>12.0</td>
      <td>True</td>
      <td>acfe27303f74129930fef5e6fadbc5f58c6b8590</td>
      <td>KoboldAI/GPT-J-6B-Skein</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/facebook/opt-iml-max-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-iml-max-1.3b</td>
      <td>34.99</td>
      <td>30.72</td>
      <td>53.81</td>
      <td>27.61</td>
      <td>38.34</td>
      <td>60.22</td>
      <td>0.53</td>
      <td>33.70</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>1.32</td>
      <td>34.0</td>
      <td>True</td>
      <td>d60fa58f50def19751da2075791da359ca19d273</td>
      <td>facebook/opt-iml-max-1.3b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/YeungNLP/firefly-bloom-7b1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-bloom-7b1</td>
      <td>34.99</td>
      <td>40.44</td>
      <td>61.20</td>
      <td>26.83</td>
      <td>40.83</td>
      <td>64.56</td>
      <td>0.68</td>
      <td>10.37</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.07</td>
      <td>1.0</td>
      <td>True</td>
      <td>6b4385dc45c47d509b6400c41a2ff3665ad1d189</td>
      <td>YeungNLP/firefly-bloom-7b1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-13b</td>
      <td>34.99</td>
      <td>39.93</td>
      <td>71.20</td>
      <td>24.90</td>
      <td>34.10</td>
      <td>68.51</td>
      <td>0.99</td>
      <td>5.28</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>59.0</td>
      <td>True</td>
      <td>e515202d1e7750da62d245fbccb2723b9c1790f5</td>
      <td>facebook/opt-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dvruette/oasst-pythia-6.9b-4000-steps</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dvruette__oasst-pythia-6.9b-4000-steps</td>
      <td>34.98</td>
      <td>41.64</td>
      <td>64.24</td>
      <td>26.26</td>
      <td>40.43</td>
      <td>61.80</td>
      <td>0.53</td>
      <td>9.97</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>0e201b6f344ac6382dda40d389e1c9144a87d027</td>
      <td>dvruette/oasst-pythia-6.9b-4000-steps</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/LongAlpaca-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__LongAlpaca-7B</td>
      <td>34.98</td>
      <td>42.66</td>
      <td>65.89</td>
      <td>27.28</td>
      <td>40.16</td>
      <td>60.14</td>
      <td>0.00</td>
      <td>8.69</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.74</td>
      <td>6.0</td>
      <td>True</td>
      <td>bebfcb894b3f5170ce54e3bb98b6e565fae7b6c0</td>
      <td>Yukang/LongAlpaca-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt</td>
      <td>34.96</td>
      <td>41.30</td>
      <td>62.44</td>
      <td>27.55</td>
      <td>42.00</td>
      <td>64.56</td>
      <td>1.52</td>
      <td>5.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>29604e6e19822531b0d49d3f19abef603a97d0ec</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-1024-open-llama-7b-preview-400bt</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Javalion-GPTJ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Javalion-GPTJ</td>
      <td>34.96</td>
      <td>41.89</td>
      <td>68.69</td>
      <td>26.85</td>
      <td>35.44</td>
      <td>65.27</td>
      <td>1.67</td>
      <td>4.89</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>3ce176bc0f91cae416c78e99f964f54b12472de0</td>
      <td>digitous/Javalion-GPTJ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/PPO_Shygmalion-V8p4_Dev-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__PPO_Shygmalion-V8p4_Dev-6b</td>
      <td>34.94</td>
      <td>40.70</td>
      <td>67.04</td>
      <td>29.31</td>
      <td>35.57</td>
      <td>63.93</td>
      <td>2.58</td>
      <td>5.45</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>3.0</td>
      <td>True</td>
      <td>fa3d503bca50c947e7a5bbde4bdd82f699f65c02</td>
      <td>TehVenom/PPO_Shygmalion-V8p4_Dev-6b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/PPO_Pygway-V8p4_Dev-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__PPO_Pygway-V8p4_Dev-6b</td>
      <td>34.94</td>
      <td>40.36</td>
      <td>67.15</td>
      <td>29.30</td>
      <td>35.26</td>
      <td>64.40</td>
      <td>2.65</td>
      <td>5.45</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>7.0</td>
      <td>True</td>
      <td>f30709dba36c665869f9ac8cd0cef5a8a2e7c8df</td>
      <td>TehVenom/PPO_Pygway-V8p4_Dev-6b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__WizardVicuna-Uncensored-3B-instruct-PL-lora_unload</td>
      <td>34.93</td>
      <td>41.98</td>
      <td>66.82</td>
      <td>25.69</td>
      <td>39.67</td>
      <td>64.88</td>
      <td>0.68</td>
      <td>4.82</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>3.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>e471ec778771f29992293d1660cc108f29c9c69e</td>
      <td>Aspik101/WizardVicuna-Uncensored-3B-instruct-PL-lora_unload</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Salesforce/codegen-6B-nl</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Salesforce__codegen-6B-nl</td>
      <td>34.92</td>
      <td>42.32</td>
      <td>68.59</td>
      <td>25.93</td>
      <td>34.47</td>
      <td>66.46</td>
      <td>2.20</td>
      <td>4.46</td>
      <td>CodeGenForCausalLM</td>
      <td>torch.float16</td>
      <td>bsd-3-clause</td>
      <td>6.85</td>
      <td>4.0</td>
      <td>True</td>
      <td>dff91c0aea702edbea3528344d01d8b9aaee6e39</td>
      <td>Salesforce/codegen-6B-nl</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/Dolly_Shygmalion-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Dolly_Shygmalion-6b</td>
      <td>34.90</td>
      <td>41.89</td>
      <td>68.48</td>
      <td>27.58</td>
      <td>33.91</td>
      <td>65.35</td>
      <td>2.12</td>
      <td>4.93</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>14.0</td>
      <td>True</td>
      <td>108fabf8a916900525492c294c50998d7c09f10b</td>
      <td>TehVenom/Dolly_Shygmalion-6b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Skegma-GPTJ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Skegma-GPTJ</td>
      <td>34.87</td>
      <td>43.77</td>
      <td>69.22</td>
      <td>25.37</td>
      <td>34.67</td>
      <td>64.64</td>
      <td>1.52</td>
      <td>4.91</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>0.0</td>
      <td>True</td>
      <td>4dff006b2ea7e8d9b067dfe8af8ca1a16bc44dce</td>
      <td>digitous/Skegma-GPTJ</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-4-14b-pile</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-4-14b-pile</td>
      <td>34.87</td>
      <td>44.45</td>
      <td>71.07</td>
      <td>26.12</td>
      <td>32.04</td>
      <td>65.43</td>
      <td>0.38</td>
      <td>4.58</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.89</td>
      <td>2.0</td>
      <td>True</td>
      <td>4effb0fa9d15c2f383a1d159f4a40df0e09eb6d5</td>
      <td>RWKV/rwkv-4-14b-pile</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/gpt-j-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-j-6b</td>
      <td>34.87</td>
      <td>41.38</td>
      <td>67.54</td>
      <td>26.78</td>
      <td>35.96</td>
      <td>65.98</td>
      <td>1.82</td>
      <td>4.62</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>1290.0</td>
      <td>True</td>
      <td>47e169305d2e8376be1d31e765533382721b2cc1</td>
      <td>EleutherAI/gpt-j-6b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/NousResearch/CodeLlama-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NousResearch__CodeLlama-7b-hf</td>
      <td>34.86</td>
      <td>39.85</td>
      <td>59.58</td>
      <td>30.47</td>
      <td>38.62</td>
      <td>64.88</td>
      <td>5.46</td>
      <td>5.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>3.0</td>
      <td>True</td>
      <td>855c92912ea4a8eb5f0be1db4bf776ffd0815dac</td>
      <td>NousResearch/CodeLlama-7b-hf</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/codellama/CodeLlama-7b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-7b-hf</td>
      <td>34.85</td>
      <td>39.93</td>
      <td>60.80</td>
      <td>31.12</td>
      <td>37.82</td>
      <td>64.01</td>
      <td>5.16</td>
      <td>5.12</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>145.0</td>
      <td>True</td>
      <td>be52f4ad322f5a47da121c761aeb5ba20ed77b17</td>
      <td>codellama/CodeLlama-7b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/heegyu/WizardVicuna-Uncensored-3B-0719</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__WizardVicuna-Uncensored-3B-0719</td>
      <td>34.81</td>
      <td>41.38</td>
      <td>66.19</td>
      <td>26.53</td>
      <td>39.35</td>
      <td>63.77</td>
      <td>1.14</td>
      <td>5.31</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>4.0</td>
      <td>True</td>
      <td>36841c80535bc3e8403e3cc084e8e65884c75076</td>
      <td>heegyu/WizardVicuna-Uncensored-3B-0719</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__Pythia-Chat-Base-7B</td>
      <td>34.80</td>
      <td>40.02</td>
      <td>68.67</td>
      <td>27.44</td>
      <td>34.63</td>
      <td>64.01</td>
      <td>4.09</td>
      <td>4.75</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>61.0</td>
      <td>True</td>
      <td>97aa918c383820e1a69f042801091d7deb996c20</td>
      <td>togethercomputer/Pythia-Chat-Base-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/Dolly_Malion-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__Dolly_Malion-6b</td>
      <td>34.80</td>
      <td>42.83</td>
      <td>68.43</td>
      <td>27.13</td>
      <td>33.03</td>
      <td>65.43</td>
      <td>1.74</td>
      <td>4.97</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>f239eb8d24fe26db3b0a9a69115dc305fc9351af</td>
      <td>TehVenom/Dolly_Malion-6b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/GPT-J-Pyg_PPO-6B-Dev-V8p4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__GPT-J-Pyg_PPO-6B-Dev-V8p4</td>
      <td>34.78</td>
      <td>40.19</td>
      <td>66.43</td>
      <td>30.39</td>
      <td>34.76</td>
      <td>64.01</td>
      <td>1.90</td>
      <td>5.80</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-openrail-m</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>930dc82245c607ce43558a0e6c0225e77b341ea6</td>
      <td>TehVenom/GPT-J-Pyg_PPO-6B-Dev-V8p4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4</td>
      <td>34.78</td>
      <td>41.64</td>
      <td>66.23</td>
      <td>27.26</td>
      <td>36.10</td>
      <td>64.40</td>
      <td>0.68</td>
      <td>7.15</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc</td>
      <td>2.91</td>
      <td>2.0</td>
      <td>True</td>
      <td>c588a5924749b86a6cb36a687dafa544c189bb6f</td>
      <td>Fredithefish/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-30B-Uncensored-GPTQ</td>
      <td>34.77</td>
      <td>29.44</td>
      <td>26.47</td>
      <td>24.35</td>
      <td>49.15</td>
      <td>73.16</td>
      <td>21.08</td>
      <td>19.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>107.0</td>
      <td>True</td>
      <td>43c701ddbe0bceac26c860307e06763cc5203500</td>
      <td>TheBloke/WizardLM-30B-Uncensored-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-Chat-3B-v1</td>
      <td>34.76</td>
      <td>42.83</td>
      <td>67.62</td>
      <td>26.23</td>
      <td>34.44</td>
      <td>65.51</td>
      <td>0.53</td>
      <td>6.16</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>115.0</td>
      <td>True</td>
      <td>f0e0995eba801096ed04cb87931d96a8316871af</td>
      <td>togethercomputer/RedPajama-INCITE-Chat-3B-v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/ChanMalion</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__ChanMalion</td>
      <td>34.74</td>
      <td>41.89</td>
      <td>68.25</td>
      <td>27.29</td>
      <td>33.89</td>
      <td>65.35</td>
      <td>1.67</td>
      <td>4.85</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>9.0</td>
      <td>True</td>
      <td>2667b0e0b705ed23f81f3e2b69673d722e8f4964</td>
      <td>TehVenom/ChanMalion</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/GPT-J-Pyg_PPO-6B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__GPT-J-Pyg_PPO-6B</td>
      <td>34.73</td>
      <td>42.06</td>
      <td>67.51</td>
      <td>28.52</td>
      <td>31.95</td>
      <td>64.72</td>
      <td>2.81</td>
      <td>5.57</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-openrail-m</td>
      <td>5.84</td>
      <td>6.0</td>
      <td>True</td>
      <td>cde5bab3ae16e1704c5fec54a6a7ff1169c935e6</td>
      <td>TehVenom/GPT-J-Pyg_PPO-6B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/databricks/dolly-v2-12b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_databricks__dolly-v2-12b</td>
      <td>34.72</td>
      <td>42.41</td>
      <td>72.53</td>
      <td>25.92</td>
      <td>33.83</td>
      <td>60.85</td>
      <td>1.21</td>
      <td>6.29</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>11.58</td>
      <td>1872.0</td>
      <td>True</td>
      <td>19308160448536e378e3db21a73a751579ee7fdd</td>
      <td>databricks/dolly-v2-12b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-13B-Nerybus-Mix</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-13B-Nerybus-Mix</td>
      <td>34.70</td>
      <td>39.85</td>
      <td>70.60</td>
      <td>24.90</td>
      <td>34.02</td>
      <td>67.88</td>
      <td>0.38</td>
      <td>5.26</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>25.0</td>
      <td>True</td>
      <td>c27a7e2360dd313406719980851e89abf46ebb13</td>
      <td>KoboldAI/OPT-13B-Nerybus-Mix</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-13B-Erebus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-13B-Erebus</td>
      <td>34.69</td>
      <td>40.02</td>
      <td>70.07</td>
      <td>25.32</td>
      <td>34.93</td>
      <td>66.54</td>
      <td>0.76</td>
      <td>5.23</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>149.0</td>
      <td>True</td>
      <td>8a949353677d2b971910a6c4afcc70e95d838c2a</td>
      <td>KoboldAI/OPT-13B-Erebus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-7B-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-Chat-7B-v0.1</td>
      <td>34.68</td>
      <td>42.06</td>
      <td>70.82</td>
      <td>26.94</td>
      <td>36.09</td>
      <td>59.83</td>
      <td>0.45</td>
      <td>6.56</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>87.0</td>
      <td>True</td>
      <td>47b94a739e2f3164b438501c8684acc5d5acc146</td>
      <td>togethercomputer/RedPajama-INCITE-Chat-7B-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-7B-Chat</td>
      <td>34.68</td>
      <td>42.06</td>
      <td>70.82</td>
      <td>26.94</td>
      <td>36.09</td>
      <td>59.83</td>
      <td>0.45</td>
      <td>6.56</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>87.0</td>
      <td>True</td>
      <td>47b94a739e2f3164b438501c8684acc5d5acc146</td>
      <td>togethercomputer/RedPajama-INCITE-7B-Chat</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-12b-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-12b-deduped</td>
      <td>34.67</td>
      <td>41.38</td>
      <td>70.26</td>
      <td>25.63</td>
      <td>33.00</td>
      <td>66.46</td>
      <td>1.44</td>
      <td>4.55</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.59</td>
      <td>48.0</td>
      <td>True</td>
      <td>39c1bd94f9dbe4ebd1d191f364cb33a2e5c47707</td>
      <td>EleutherAI/pythia-12b-deduped</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Janin-GPTJ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Janin-GPTJ</td>
      <td>34.66</td>
      <td>40.87</td>
      <td>67.29</td>
      <td>27.40</td>
      <td>36.25</td>
      <td>64.25</td>
      <td>1.97</td>
      <td>4.56</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>5.84</td>
      <td>0.0</td>
      <td>True</td>
      <td>a6773861798f2abea3849514aa6f60961518af9c</td>
      <td>digitous/Janin-GPTJ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-13B-Nerys-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-13B-Nerys-v2</td>
      <td>34.63</td>
      <td>39.68</td>
      <td>70.53</td>
      <td>25.36</td>
      <td>33.50</td>
      <td>67.88</td>
      <td>0.23</td>
      <td>5.24</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>12.85</td>
      <td>9.0</td>
      <td>True</td>
      <td>b0aa4f3630356f7801ca083c00b03d03da13b8bb</td>
      <td>KoboldAI/OPT-13B-Nerys-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/GPT-J-6B-Shinen</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__GPT-J-6B-Shinen</td>
      <td>34.62</td>
      <td>39.85</td>
      <td>67.06</td>
      <td>27.72</td>
      <td>36.94</td>
      <td>64.09</td>
      <td>1.97</td>
      <td>4.71</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>5.84</td>
      <td>15.0</td>
      <td>True</td>
      <td>afa5a11b24cb23eee708e17c83b920a788e9e07b</td>
      <td>KoboldAI/GPT-J-6B-Shinen</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/heegyu/WizardVicuna-3B-0719</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__WizardVicuna-3B-0719</td>
      <td>34.57</td>
      <td>40.70</td>
      <td>65.45</td>
      <td>25.44</td>
      <td>40.71</td>
      <td>63.85</td>
      <td>0.76</td>
      <td>5.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>62d3d450b8ab2bd2fb9f82383b55d1ecae33a401</td>
      <td>heegyu/WizardVicuna-3B-0719</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/GPT-J-6B-Janeway</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__GPT-J-6B-Janeway</td>
      <td>34.57</td>
      <td>40.87</td>
      <td>67.11</td>
      <td>27.45</td>
      <td>35.74</td>
      <td>64.72</td>
      <td>1.36</td>
      <td>4.76</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>5.84</td>
      <td>11.0</td>
      <td>True</td>
      <td>036bb03496d648ddc8cf932ad91df8ef1287116c</td>
      <td>KoboldAI/GPT-J-6B-Janeway</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/CobraMamba/mamba-gpt-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_CobraMamba__mamba-gpt-3b</td>
      <td>34.56</td>
      <td>40.53</td>
      <td>64.94</td>
      <td>25.35</td>
      <td>37.14</td>
      <td>65.04</td>
      <td>0.23</td>
      <td>8.68</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>3.0</td>
      <td>True</td>
      <td>21a8212e3641dd14924d6bdead0774b64dda8ce0</td>
      <td>CobraMamba/mamba-gpt-3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/PPO_Pygway-6b-Mix</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__PPO_Pygway-6b-Mix</td>
      <td>34.53</td>
      <td>41.81</td>
      <td>67.77</td>
      <td>28.42</td>
      <td>32.50</td>
      <td>64.40</td>
      <td>1.67</td>
      <td>5.17</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.84</td>
      <td>19.0</td>
      <td>True</td>
      <td>b31d25819e00d5031ccdb22a9584f0850dcfe39c</td>
      <td>KoboldAI/PPO_Pygway-6b-Mix</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/databricks/dolly-v2-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_databricks__dolly-v2-7b</td>
      <td>34.49</td>
      <td>44.54</td>
      <td>69.64</td>
      <td>25.18</td>
      <td>34.88</td>
      <td>60.06</td>
      <td>1.14</td>
      <td>5.97</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>6.65</td>
      <td>132.0</td>
      <td>True</td>
      <td>d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f</td>
      <td>databricks/dolly-v2-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/PPO_Shygmalion-6b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__PPO_Shygmalion-6b</td>
      <td>34.46</td>
      <td>40.27</td>
      <td>66.88</td>
      <td>27.53</td>
      <td>34.24</td>
      <td>65.35</td>
      <td>1.82</td>
      <td>5.13</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>5.0</td>
      <td>True</td>
      <td>573e4546fdccc5c8a52b9d7cb23a2e10f0f2ef51</td>
      <td>TehVenom/PPO_Shygmalion-6b</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/DanielSc4/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DanielSc4__RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1</td>
      <td>34.44</td>
      <td>41.30</td>
      <td>66.82</td>
      <td>26.10</td>
      <td>35.04</td>
      <td>65.43</td>
      <td>0.30</td>
      <td>6.08</td>
      <td>?</td>
      <td>8bit</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>a2ee88a9fa1c9ad41e0a8c15217a4b1230ec33c8</td>
      <td>DanielSc4/RedPajama-INCITE-Chat-3B-v1-RL-LoRA-8bit-test1</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/robin-33B-v2-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__robin-33B-v2-GPTQ</td>
      <td>34.39</td>
      <td>27.73</td>
      <td>26.29</td>
      <td>23.53</td>
      <td>49.54</td>
      <td>79.79</td>
      <td>27.75</td>
      <td>6.13</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>13.0</td>
      <td>True</td>
      <td>4c2588d65302e9ca634548ed81e8650fb2975686</td>
      <td>TheBloke/robin-33B-v2-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/digitous/Adventien-GPTJ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_digitous__Adventien-GPTJ</td>
      <td>34.36</td>
      <td>42.49</td>
      <td>69.21</td>
      <td>25.40</td>
      <td>36.95</td>
      <td>60.22</td>
      <td>1.59</td>
      <td>4.69</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>0.0</td>
      <td>True</td>
      <td>4fbfe9eae03a1d6ecf60fda8cf39c4123f0438bd</td>
      <td>digitous/Adventien-GPTJ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/Guanaco-3B-Uncensored-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__Guanaco-3B-Uncensored-v2</td>
      <td>34.35</td>
      <td>42.15</td>
      <td>66.72</td>
      <td>26.18</td>
      <td>35.21</td>
      <td>63.30</td>
      <td>0.30</td>
      <td>6.60</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>9.0</td>
      <td>True</td>
      <td>e07122091fd4b318dcea105b16c73144d95bc2f6</td>
      <td>Fredithefish/Guanaco-3B-Uncensored-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Guanaco-3B-Uncensored-v2-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Guanaco-3B-Uncensored-v2-GPTQ</td>
      <td>34.34</td>
      <td>41.64</td>
      <td>64.76</td>
      <td>26.25</td>
      <td>36.58</td>
      <td>64.33</td>
      <td>0.15</td>
      <td>6.71</td>
      <td>GPTNeoXForCausalLM</td>
      <td>GPTQ</td>
      <td>apache-2.0</td>
      <td>4.78</td>
      <td>8.0</td>
      <td>True</td>
      <td>c80e2f01377d551ad17c8c9bac3f52578c38d653</td>
      <td>TheBloke/Guanaco-3B-Uncensored-v2-GPTQ</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/OpenAssistant/galactica-6.7b-finetuned</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__galactica-6.7b-finetuned</td>
      <td>34.34</td>
      <td>41.55</td>
      <td>51.01</td>
      <td>38.03</td>
      <td>41.65</td>
      <td>57.70</td>
      <td>3.11</td>
      <td>7.30</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.66</td>
      <td>34.0</td>
      <td>True</td>
      <td>d86db70e16111175ff7900f71d40806ccf4b8491</td>
      <td>OpenAssistant/galactica-6.7b-finetuned</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2</td>
      <td>34.33</td>
      <td>36.43</td>
      <td>61.41</td>
      <td>25.01</td>
      <td>37.59</td>
      <td>64.64</td>
      <td>0.23</td>
      <td>15.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>4.0</td>
      <td>True</td>
      <td>fdc6ff469295d0aaabec8948525b70d6688728ac</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/RedPajama-INCITE-Chat-Instruct-3B-V1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__RedPajama-INCITE-Chat-Instruct-3B-V1</td>
      <td>34.33</td>
      <td>42.58</td>
      <td>67.48</td>
      <td>25.99</td>
      <td>33.62</td>
      <td>64.80</td>
      <td>0.91</td>
      <td>4.93</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>1.0</td>
      <td>True</td>
      <td>e19eef572d57fc734bf3ea07c7d0098b3901ec9b</td>
      <td>acrastt/RedPajama-INCITE-Chat-Instruct-3B-V1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-6.9b-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-6.9b-deduped</td>
      <td>34.32</td>
      <td>41.30</td>
      <td>67.05</td>
      <td>26.48</td>
      <td>35.19</td>
      <td>64.09</td>
      <td>1.67</td>
      <td>4.50</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>6.0</td>
      <td>True</td>
      <td>372b1c08d9b5b0fc18ce86bbf294930e26e66ed5</td>
      <td>EleutherAI/pythia-6.9b-deduped</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/ScarletPajama-3B-HF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__ScarletPajama-3B-HF</td>
      <td>34.32</td>
      <td>39.76</td>
      <td>64.89</td>
      <td>27.28</td>
      <td>37.60</td>
      <td>64.48</td>
      <td>0.23</td>
      <td>5.97</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>9.0</td>
      <td>True</td>
      <td>9dd07308b6eb3f270c5762250b6d46abd6f87b6f</td>
      <td>Fredithefish/ScarletPajama-3B-HF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ikala/bloom-zh-3b-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ikala__bloom-zh-3b-chat</td>
      <td>34.31</td>
      <td>38.82</td>
      <td>54.71</td>
      <td>31.62</td>
      <td>41.25</td>
      <td>58.64</td>
      <td>0.45</td>
      <td>14.66</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-openrail-m</td>
      <td>3.00</td>
      <td>10.0</td>
      <td>True</td>
      <td>4ea0ad223a2623fc15e8824c1c4f8e6539bc40b0</td>
      <td>ikala/bloom-zh-3b-chat</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/black_goo_recipe_a</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__black_goo_recipe_a</td>
      <td>34.25</td>
      <td>38.14</td>
      <td>66.56</td>
      <td>25.75</td>
      <td>37.46</td>
      <td>63.93</td>
      <td>0.53</td>
      <td>7.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>7067f68d4d9e7b10a1aa2c9fa97456bc04678867</td>
      <td>KnutJaegersberg/black_goo_recipe_a</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/RedPajama-INCITE-Chat-3B-ShareGPT-11K</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__RedPajama-INCITE-Chat-3B-ShareGPT-11K</td>
      <td>34.25</td>
      <td>40.61</td>
      <td>64.84</td>
      <td>26.13</td>
      <td>35.41</td>
      <td>63.54</td>
      <td>0.30</td>
      <td>8.91</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>2.0</td>
      <td>True</td>
      <td>ec33d12d08d61ed821e67b1a55ad404dc3457ebf</td>
      <td>Fredithefish/RedPajama-INCITE-Chat-3B-ShareGPT-11K</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/health360/Healix-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_health360__Healix-3B</td>
      <td>34.22</td>
      <td>37.71</td>
      <td>65.94</td>
      <td>26.02</td>
      <td>37.40</td>
      <td>65.75</td>
      <td>0.76</td>
      <td>5.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>52297e0b6845b3c1b26f336fd2a2c9b2f56ce6ba</td>
      <td>health360/Healix-3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-6.7B-Erebus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-6.7B-Erebus</td>
      <td>34.20</td>
      <td>39.16</td>
      <td>68.66</td>
      <td>24.58</td>
      <td>35.12</td>
      <td>65.98</td>
      <td>1.06</td>
      <td>4.86</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.66</td>
      <td>84.0</td>
      <td>True</td>
      <td>9c4d1af96f93224e01d2f69c303fc6d6f686bdcc</td>
      <td>KoboldAI/OPT-6.7B-Erebus</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/deacon-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__deacon-3b</td>
      <td>34.20</td>
      <td>39.68</td>
      <td>66.42</td>
      <td>27.13</td>
      <td>36.07</td>
      <td>64.64</td>
      <td>0.38</td>
      <td>5.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>3.43</td>
      <td>2.0</td>
      <td>True</td>
      <td>c96b846ce7bacf5ad231957630dc94d59f329339</td>
      <td>KnutJaegersberg/deacon-3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-6.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-6.7b</td>
      <td>34.19</td>
      <td>39.16</td>
      <td>68.66</td>
      <td>24.57</td>
      <td>35.12</td>
      <td>65.98</td>
      <td>0.99</td>
      <td>4.86</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.66</td>
      <td>77.0</td>
      <td>True</td>
      <td>a45aa65bbeb77c1558bc99bedc6779195462dab0</td>
      <td>facebook/opt-6.7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/black_goo_recipe_c</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__black_goo_recipe_c</td>
      <td>34.19</td>
      <td>38.74</td>
      <td>66.83</td>
      <td>26.57</td>
      <td>36.54</td>
      <td>64.72</td>
      <td>0.68</td>
      <td>5.23</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>78c0a6432ac0a6c2e54a2c3aac4cb70f446eb18b</td>
      <td>KnutJaegersberg/black_goo_recipe_c</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/Guanaco-3B-Uncensored</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__Guanaco-3B-Uncensored</td>
      <td>34.18</td>
      <td>42.49</td>
      <td>66.99</td>
      <td>25.55</td>
      <td>34.71</td>
      <td>63.38</td>
      <td>0.53</td>
      <td>5.62</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.78</td>
      <td>0.0</td>
      <td>True</td>
      <td>084a12f767b31c1fde681bebb14e9a291e506ea8</td>
      <td>Fredithefish/Guanaco-3B-Uncensored</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Fredithefish/CrimsonPajama</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Fredithefish__CrimsonPajama</td>
      <td>34.18</td>
      <td>40.19</td>
      <td>65.47</td>
      <td>25.95</td>
      <td>33.78</td>
      <td>65.19</td>
      <td>0.53</td>
      <td>8.16</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>4.0</td>
      <td>True</td>
      <td>ff054eeff9e3541464383d40b36d182057d01113</td>
      <td>Fredithefish/CrimsonPajama</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/bigscience/bloom-7b1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloom-7b1</td>
      <td>34.18</td>
      <td>41.13</td>
      <td>62.00</td>
      <td>26.25</td>
      <td>38.90</td>
      <td>65.43</td>
      <td>0.76</td>
      <td>4.80</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>7.07</td>
      <td>137.0</td>
      <td>True</td>
      <td>e83e90ba86f87f74aa2731cdab25ccf33976bd66</td>
      <td>bigscience/bloom-7b1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-Instruct-3B-v1</td>
      <td>34.12</td>
      <td>41.55</td>
      <td>65.48</td>
      <td>25.03</td>
      <td>36.41</td>
      <td>64.48</td>
      <td>1.36</td>
      <td>4.51</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>84.0</td>
      <td>True</td>
      <td>0c66778ee09a036886741707733620b91057909a</td>
      <td>togethercomputer/RedPajama-INCITE-Instruct-3B-v1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/heegyu/WizardVicuna-open-llama-3b-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_heegyu__WizardVicuna-open-llama-3b-v2</td>
      <td>34.11</td>
      <td>37.71</td>
      <td>66.60</td>
      <td>27.23</td>
      <td>36.80</td>
      <td>63.30</td>
      <td>0.99</td>
      <td>6.12</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>1c69905286171d7d3ef3f95f8e1bbc9150bad3cd</td>
      <td>heegyu/WizardVicuna-open-llama-3b-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Lazycuber/pyg-instruct-wizardlm</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Lazycuber__pyg-instruct-wizardlm</td>
      <td>34.06</td>
      <td>40.96</td>
      <td>66.71</td>
      <td>26.33</td>
      <td>31.93</td>
      <td>63.69</td>
      <td>1.59</td>
      <td>7.22</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>f00ef7a7b0cc6f02af2a11ac764270dfd61b9e2f</td>
      <td>Lazycuber/pyg-instruct-wizardlm</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/frank098/orca_mini_3b_juniper</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_frank098__orca_mini_3b_juniper</td>
      <td>33.99</td>
      <td>40.87</td>
      <td>61.73</td>
      <td>26.37</td>
      <td>43.19</td>
      <td>60.30</td>
      <td>0.53</td>
      <td>4.97</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>3.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>c08749034baa053834f1b709b6e7b88b914cd1fb</td>
      <td>frank098/orca_mini_3b_juniper</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-6.7B-Nerybus-Mix</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-6.7B-Nerybus-Mix</td>
      <td>33.98</td>
      <td>39.16</td>
      <td>68.63</td>
      <td>24.47</td>
      <td>34.84</td>
      <td>65.11</td>
      <td>0.76</td>
      <td>4.85</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.66</td>
      <td>18.0</td>
      <td>True</td>
      <td>9afe4dca5a9dbd71cb90d1050d142837f4c739f6</td>
      <td>KoboldAI/OPT-6.7B-Nerybus-Mix</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/euclaise/falcon_1b_stage3_2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_euclaise__falcon_1b_stage3_2</td>
      <td>33.95</td>
      <td>34.56</td>
      <td>58.37</td>
      <td>23.87</td>
      <td>39.89</td>
      <td>60.46</td>
      <td>0.00</td>
      <td>20.52</td>
      <td>FalconForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>aec2f59879ea6dfa5233611c4cf83cf3cb974d40</td>
      <td>euclaise/falcon_1b_stage3_2</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-12b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-12b</td>
      <td>33.91</td>
      <td>39.59</td>
      <td>68.82</td>
      <td>26.76</td>
      <td>31.85</td>
      <td>64.17</td>
      <td>1.74</td>
      <td>4.45</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>11.59</td>
      <td>111.0</td>
      <td>True</td>
      <td>35c9d7f32fbb108fb8b5bdd574eb03369d1eed49</td>
      <td>EleutherAI/pythia-12b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-6B-nerys-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-6B-nerys-v2</td>
      <td>33.87</td>
      <td>38.40</td>
      <td>68.57</td>
      <td>24.34</td>
      <td>34.73</td>
      <td>65.59</td>
      <td>0.68</td>
      <td>4.78</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.66</td>
      <td>21.0</td>
      <td>True</td>
      <td>9e1f1498391df2c28ce35a9290a5a24b8022a43b</td>
      <td>KoboldAI/OPT-6B-nerys-v2</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/guanaco-33B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__guanaco-33B-GPTQ</td>
      <td>33.87</td>
      <td>28.16</td>
      <td>26.34</td>
      <td>24.94</td>
      <td>48.98</td>
      <td>78.85</td>
      <td>23.81</td>
      <td>5.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>71.0</td>
      <td>True</td>
      <td>8e42e031bfc8be3bbf31dc546d7c51fb991ff6e0</td>
      <td>TheBloke/guanaco-33B-GPTQ</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/xglm-7.5B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__xglm-7.5B</td>
      <td>33.84</td>
      <td>34.13</td>
      <td>60.77</td>
      <td>27.79</td>
      <td>36.66</td>
      <td>58.72</td>
      <td>0.23</td>
      <td>18.58</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>7.49</td>
      <td>43.0</td>
      <td>True</td>
      <td>732d59308a844004bd9a4def972cc7c3896a38e0</td>
      <td>facebook/xglm-7.5B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/black_goo_recipe_d</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__black_goo_recipe_d</td>
      <td>33.83</td>
      <td>37.80</td>
      <td>66.50</td>
      <td>26.64</td>
      <td>36.46</td>
      <td>63.61</td>
      <td>0.38</td>
      <td>5.40</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>fdf7f93837808958f9463d3c683314e7f649a088</td>
      <td>KnutJaegersberg/black_goo_recipe_d</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/matsuo-lab/weblab-10b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_matsuo-lab__weblab-10b</td>
      <td>33.76</td>
      <td>39.51</td>
      <td>65.76</td>
      <td>26.29</td>
      <td>36.02</td>
      <td>62.51</td>
      <td>1.44</td>
      <td>4.81</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>10.47</td>
      <td>55.0</td>
      <td>True</td>
      <td>d6fc432983b1633a4c1568d121c60de6b8c3e511</td>
      <td>matsuo-lab/weblab-10b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-oig-oasst1-256-6_9b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-oig-oasst1-256-6_9b</td>
      <td>33.76</td>
      <td>39.93</td>
      <td>65.42</td>
      <td>26.39</td>
      <td>35.00</td>
      <td>63.38</td>
      <td>1.59</td>
      <td>4.60</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>5.0</td>
      <td>True</td>
      <td>f1c9bac89b74d3487cb092788ce828fb9520c1a7</td>
      <td>h2oai/h2ogpt-oig-oasst1-256-6_9b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/mosaicml/mpt-7b-storywriter</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mosaicml__mpt-7b-storywriter</td>
      <td>33.74</td>
      <td>45.65</td>
      <td>74.14</td>
      <td>28.80</td>
      <td>36.12</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>690.0</td>
      <td>True</td>
      <td>a5e85ae1941e31bb705adbcafce9b0dfd6f3a48b</td>
      <td>mosaicml/mpt-7b-storywriter</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_togethercomputer__RedPajama-INCITE-Base-3B-v1</td>
      <td>33.73</td>
      <td>40.19</td>
      <td>64.77</td>
      <td>27.03</td>
      <td>33.23</td>
      <td>64.72</td>
      <td>1.29</td>
      <td>4.90</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>80.0</td>
      <td>True</td>
      <td>094fbdd0c911feb485ce55de1952ab2e75277e1e</td>
      <td>togethercomputer/RedPajama-INCITE-Base-3B-v1</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/black_goo_recipe_b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__black_goo_recipe_b</td>
      <td>33.72</td>
      <td>37.63</td>
      <td>66.72</td>
      <td>25.68</td>
      <td>37.09</td>
      <td>63.77</td>
      <td>0.08</td>
      <td>5.10</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>3.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>42faec8429cee8c9f4f5db58ffa193f6f8e0d498</td>
      <td>KnutJaegersberg/black_goo_recipe_b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Writer/camel-5b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Writer__camel-5b-hf</td>
      <td>33.70</td>
      <td>35.15</td>
      <td>57.62</td>
      <td>26.07</td>
      <td>40.65</td>
      <td>61.01</td>
      <td>0.38</td>
      <td>15.00</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.05</td>
      <td>103.0</td>
      <td>True</td>
      <td>d1438e22a33b9115af0e47ab3a0fe844cbf588a6</td>
      <td>Writer/camel-5b-hf</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/GeorgiaTechResearchInstitute/galactica-6.7b-evol-instruct-70k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_GeorgiaTechResearchInstitute__galactica-6.7b-evol-instruct-70k</td>
      <td>33.68</td>
      <td>42.58</td>
      <td>49.30</td>
      <td>32.96</td>
      <td>42.10</td>
      <td>56.27</td>
      <td>0.38</td>
      <td>12.16</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>6.66</td>
      <td>16.0</td>
      <td>True</td>
      <td>14fa470051d0bc38fd871643186a9edfd3a8a9aa</td>
      <td>GeorgiaTechResearchInstitute/galactica-6.7b-evol-instruct-70k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/fairseq-dense-2.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__fairseq-dense-2.7B</td>
      <td>33.67</td>
      <td>33.79</td>
      <td>65.74</td>
      <td>26.44</td>
      <td>34.57</td>
      <td>63.93</td>
      <td>0.00</td>
      <td>11.24</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>2.78</td>
      <td>2.0</td>
      <td>True</td>
      <td>4201f4b101bad2992efc8452009317a354ec52d2</td>
      <td>KoboldAI/fairseq-dense-2.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-30B-Erebus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-30B-Erebus</td>
      <td>33.67</td>
      <td>36.69</td>
      <td>65.60</td>
      <td>24.80</td>
      <td>38.76</td>
      <td>65.11</td>
      <td>0.23</td>
      <td>4.49</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>29.97</td>
      <td>40.0</td>
      <td>True</td>
      <td>a1041efcf9599c962822274e92040710579a5bf2</td>
      <td>KoboldAI/OPT-30B-Erebus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/anhnv125/pygmalion-6b-roleplay</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_anhnv125__pygmalion-6b-roleplay</td>
      <td>33.66</td>
      <td>40.53</td>
      <td>67.47</td>
      <td>25.73</td>
      <td>32.53</td>
      <td>62.67</td>
      <td>1.14</td>
      <td>5.56</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>e49ed0bde45de0a436bff678ec4872069e8f230c</td>
      <td>anhnv125/pygmalion-6b-roleplay</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/LLongMA-3b-LIMA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__LLongMA-3b-LIMA</td>
      <td>33.66</td>
      <td>39.08</td>
      <td>67.15</td>
      <td>26.43</td>
      <td>34.71</td>
      <td>63.38</td>
      <td>0.30</td>
      <td>4.57</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>3.32</td>
      <td>2.0</td>
      <td>True</td>
      <td>333b8c41e42a46a6f3aecaf8f3fa8a17c6d83990</td>
      <td>KnutJaegersberg/LLongMA-3b-LIMA</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TehVenom/DiffMerge_Pygmalion_Main-onto-V8P4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__DiffMerge_Pygmalion_Main-onto-V8P4</td>
      <td>33.63</td>
      <td>40.53</td>
      <td>67.48</td>
      <td>25.68</td>
      <td>32.55</td>
      <td>62.51</td>
      <td>1.14</td>
      <td>5.53</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>1.0</td>
      <td>True</td>
      <td>f855780745aa34c3bdbe020e4c51253d538cb21e</td>
      <td>TehVenom/DiffMerge_Pygmalion_Main-onto-V8P4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-oig-oasst1-512-6_9b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-oig-oasst1-512-6_9b</td>
      <td>33.60</td>
      <td>40.44</td>
      <td>65.58</td>
      <td>24.90</td>
      <td>36.68</td>
      <td>62.51</td>
      <td>0.99</td>
      <td>4.08</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>16.0</td>
      <td>True</td>
      <td>029a787e0d98fcd3fecffbfbeb4a75a425474937</td>
      <td>h2oai/h2ogpt-oig-oasst1-512-6_9b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/acrastt/OmegLLaMA-3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_acrastt__OmegLLaMA-3B</td>
      <td>33.55</td>
      <td>40.36</td>
      <td>66.13</td>
      <td>28.00</td>
      <td>33.31</td>
      <td>61.64</td>
      <td>0.23</td>
      <td>5.17</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>5.0</td>
      <td>True</td>
      <td>520c5f1ceb5c90d4011887e2a8d3becf15e7e66e</td>
      <td>acrastt/OmegLLaMA-3B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Dampish/StellarX-4B-V0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Dampish__StellarX-4B-V0</td>
      <td>33.54</td>
      <td>36.95</td>
      <td>61.90</td>
      <td>26.85</td>
      <td>34.30</td>
      <td>63.85</td>
      <td>0.00</td>
      <td>10.95</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>3.83</td>
      <td>1.0</td>
      <td>True</td>
      <td>0a79832bd57a8cdadc61626fb77bdc26c85b9fa4</td>
      <td>Dampish/StellarX-4B-V0</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/openlm-research/open_llama_3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openlm-research__open_llama_3b</td>
      <td>33.52</td>
      <td>39.85</td>
      <td>62.65</td>
      <td>26.94</td>
      <td>34.97</td>
      <td>64.72</td>
      <td>0.45</td>
      <td>5.06</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.32</td>
      <td>112.0</td>
      <td>True</td>
      <td>141067009124b9c0aea62c76b3eb952174864057</td>
      <td>openlm-research/open_llama_3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ewof/koishi-instruct-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ewof__koishi-instruct-3b</td>
      <td>33.48</td>
      <td>40.96</td>
      <td>64.54</td>
      <td>26.58</td>
      <td>31.65</td>
      <td>64.09</td>
      <td>1.14</td>
      <td>5.41</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>2.91</td>
      <td>2.0</td>
      <td>True</td>
      <td>2bb7f3842398b048efa4ae2d1aafb9e2f18a8586</td>
      <td>ewof/koishi-instruct-3b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/pszemraj/pythia-6.9b-HC3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pszemraj__pythia-6.9b-HC3</td>
      <td>33.33</td>
      <td>36.52</td>
      <td>61.76</td>
      <td>26.94</td>
      <td>45.05</td>
      <td>60.77</td>
      <td>0.00</td>
      <td>2.23</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>2.0</td>
      <td>True</td>
      <td>c5c60ea656e921e6c5415f6feaebac4dd9b2aa2a</td>
      <td>pszemraj/pythia-6.9b-HC3</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-6.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-6.7b</td>
      <td>33.31</td>
      <td>40.10</td>
      <td>65.00</td>
      <td>24.64</td>
      <td>32.85</td>
      <td>64.72</td>
      <td>1.06</td>
      <td>4.78</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.65</td>
      <td>8.0</td>
      <td>True</td>
      <td>b666a6e46eeade607c73ed1334ecda3b9345e4bf</td>
      <td>EleutherAI/pythia-6.7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/pythainlp/wangchanglm-7.5B-sft-en-sharded</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pythainlp__wangchanglm-7.5B-sft-en-sharded</td>
      <td>33.21</td>
      <td>34.47</td>
      <td>59.81</td>
      <td>26.37</td>
      <td>34.15</td>
      <td>58.25</td>
      <td>0.23</td>
      <td>19.19</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>7.49</td>
      <td>0.0</td>
      <td>True</td>
      <td>dd22eaea8be3fcb8c28f61b513a89d1adac00ffd</td>
      <td>pythainlp/wangchanglm-7.5B-sft-en-sharded</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-4-7b-pile</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-4-7b-pile</td>
      <td>33.19</td>
      <td>39.68</td>
      <td>66.31</td>
      <td>24.96</td>
      <td>33.65</td>
      <td>62.35</td>
      <td>0.76</td>
      <td>4.61</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.19</td>
      <td>0.0</td>
      <td>True</td>
      <td>922e22a761427e50d7be457b31a76b1126021b8b</td>
      <td>RWKV/rwkv-4-7b-pile</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/DanielSc4/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_DanielSc4__RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1</td>
      <td>33.13</td>
      <td>38.65</td>
      <td>63.53</td>
      <td>25.16</td>
      <td>36.07</td>
      <td>60.14</td>
      <td>0.08</td>
      <td>8.24</td>
      <td>?</td>
      <td>8bit</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>f477d24b00e05fe4c5f8d5f933080994cfd90e4e</td>
      <td>DanielSc4/RedPajama-INCITE-Chat-3B-v1-FT-LoRA-8bit-test1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenBuddy/openbuddy-openllama-3b-v10-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenBuddy__openbuddy-openllama-3b-v10-bf16</td>
      <td>33.10</td>
      <td>36.26</td>
      <td>58.38</td>
      <td>23.89</td>
      <td>42.04</td>
      <td>59.67</td>
      <td>0.99</td>
      <td>10.50</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>3.34</td>
      <td>6.0</td>
      <td>True</td>
      <td>7f24d32de53aa4bc150f04ca2418604475173921</td>
      <td>OpenBuddy/openbuddy-openllama-3b-v10-bf16</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Wizard-Vicuna-13B-Uncensored-GPTQ</td>
      <td>32.99</td>
      <td>29.61</td>
      <td>25.47</td>
      <td>25.34</td>
      <td>50.25</td>
      <td>75.77</td>
      <td>9.93</td>
      <td>14.55</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>242.0</td>
      <td>True</td>
      <td>d9b00ec47ae3546398432f0693fe2d5d92bf143b</td>
      <td>TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/RWKV/rwkv-raven-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-raven-3b</td>
      <td>32.96</td>
      <td>36.69</td>
      <td>59.78</td>
      <td>24.87</td>
      <td>35.60</td>
      <td>57.46</td>
      <td>0.45</td>
      <td>15.84</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>2.86</td>
      <td>6.0</td>
      <td>True</td>
      <td>1ddeea6a7313c8ba8824645d7aa88d5449458f67</td>
      <td>RWKV/rwkv-raven-3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/euclaise/falcon_1b_stage2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_euclaise__falcon_1b_stage2</td>
      <td>32.90</td>
      <td>35.49</td>
      <td>65.56</td>
      <td>23.83</td>
      <td>38.32</td>
      <td>62.35</td>
      <td>0.00</td>
      <td>4.72</td>
      <td>FalconForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>1.0</td>
      <td>True</td>
      <td>c3ef73a8c9dc06fae4bfe4460d2f293147aecbb0</td>
      <td>euclaise/falcon_1b_stage2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/CodeLlama-13B-Python-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__CodeLlama-13B-Python-fp16</td>
      <td>32.87</td>
      <td>33.19</td>
      <td>44.50</td>
      <td>25.94</td>
      <td>43.99</td>
      <td>67.40</td>
      <td>10.08</td>
      <td>4.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>26.0</td>
      <td>True</td>
      <td>442282f4207442b828953a72c51a919c332cba5c</td>
      <td>TheBloke/CodeLlama-13B-Python-fp16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Azure99/blossom-v1-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Azure99__blossom-v1-3b</td>
      <td>32.86</td>
      <td>36.86</td>
      <td>55.10</td>
      <td>26.70</td>
      <td>43.45</td>
      <td>58.88</td>
      <td>0.38</td>
      <td>8.65</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>3235ee41e3793c98749b7bbd2bb80882a12ac889</td>
      <td>Azure99/blossom-v1-3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Devio/test-22B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Devio__test-22B</td>
      <td>32.80</td>
      <td>39.42</td>
      <td>64.51</td>
      <td>27.13</td>
      <td>37.13</td>
      <td>57.70</td>
      <td>0.38</td>
      <td>3.32</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>21.83</td>
      <td>0.0</td>
      <td>True</td>
      <td>cd72f5954ab5801dd2c1b499e59265f7504f9ee6</td>
      <td>Devio/test-22B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/euclaise/falcon_1b_stage1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_euclaise__falcon_1b_stage1</td>
      <td>32.77</td>
      <td>35.15</td>
      <td>62.40</td>
      <td>24.47</td>
      <td>40.00</td>
      <td>61.48</td>
      <td>0.00</td>
      <td>5.89</td>
      <td>FalconForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>f85d91ff3f6cadc93f7222a19b9c4930c8842366</td>
      <td>euclaise/falcon_1b_stage1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-13B</td>
      <td>32.68</td>
      <td>38.14</td>
      <td>60.01</td>
      <td>25.92</td>
      <td>39.19</td>
      <td>59.83</td>
      <td>1.29</td>
      <td>4.39</td>
      <td>GPT2Model</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>632.0</td>
      <td>True</td>
      <td>7e97fa4b15edd955094c4395d62e6f4290e365b5</td>
      <td>cerebras/Cerebras-GPT-13B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ</td>
      <td>32.63</td>
      <td>28.41</td>
      <td>26.05</td>
      <td>24.71</td>
      <td>49.54</td>
      <td>68.67</td>
      <td>5.31</td>
      <td>25.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>35.58</td>
      <td>69.0</td>
      <td>True</td>
      <td>cd07cc7c55b46524f61214012653c25226d24c0d</td>
      <td>TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_GeorgiaTechResearchInstitute__starcoder-gpteacher-code-instruct</td>
      <td>32.57</td>
      <td>32.68</td>
      <td>47.60</td>
      <td>28.63</td>
      <td>40.41</td>
      <td>55.56</td>
      <td>0.00</td>
      <td>23.11</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>bigcode-openrail-m</td>
      <td>15.52</td>
      <td>74.0</td>
      <td>True</td>
      <td>d866b68daa719239dc44979dbf39a608ed6f7bce</td>
      <td>GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/tiiuae/falcon-rw-1b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_tiiuae__falcon-rw-1b</td>
      <td>32.44</td>
      <td>35.07</td>
      <td>63.56</td>
      <td>25.28</td>
      <td>35.96</td>
      <td>62.04</td>
      <td>0.53</td>
      <td>4.64</td>
      <td>FalconForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>58.0</td>
      <td>True</td>
      <td>e4b9872bb803165eb22f0a867d4e6a64d34fce19</td>
      <td>tiiuae/falcon-rw-1b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-2.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-2.7b</td>
      <td>32.44</td>
      <td>37.37</td>
      <td>60.74</td>
      <td>25.86</td>
      <td>35.40</td>
      <td>62.12</td>
      <td>1.06</td>
      <td>4.51</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.91</td>
      <td>5.0</td>
      <td>True</td>
      <td>b9d8cace80b1a97f5ed380711aea31f2d1b24310</td>
      <td>EleutherAI/pythia-2.7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Azure99/blossom-v2-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Azure99__blossom-v2-3b</td>
      <td>32.43</td>
      <td>35.32</td>
      <td>54.10</td>
      <td>23.99</td>
      <td>43.11</td>
      <td>58.80</td>
      <td>0.53</td>
      <td>11.17</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>1a403344de52ddb7f18548a526a927714adfe4d4</td>
      <td>Azure99/blossom-v2-3b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-13b-Python-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-13b-Python-hf</td>
      <td>32.41</td>
      <td>32.59</td>
      <td>43.94</td>
      <td>27.23</td>
      <td>44.59</td>
      <td>65.04</td>
      <td>8.64</td>
      <td>4.87</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>22.0</td>
      <td>True</td>
      <td>ea1b775799b477fe22e64f8ac9107f28950b5c87</td>
      <td>codellama/CodeLlama-13b-Python-hf</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/42dot/42dot_LLM-SFT-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_42dot__42dot_LLM-SFT-1.3B</td>
      <td>32.39</td>
      <td>36.09</td>
      <td>58.96</td>
      <td>25.51</td>
      <td>39.98</td>
      <td>58.41</td>
      <td>0.68</td>
      <td>7.11</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.44</td>
      <td>12.0</td>
      <td>True</td>
      <td>7474cafe5dc60549c19f89f7c49392a8a32b9199</td>
      <td>42dot/42dot_LLM-SFT-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-2.7B-Erebus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-2.7B-Erebus</td>
      <td>32.38</td>
      <td>34.39</td>
      <td>60.91</td>
      <td>26.70</td>
      <td>37.82</td>
      <td>61.64</td>
      <td>0.30</td>
      <td>4.89</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>2.65</td>
      <td>32.0</td>
      <td>True</td>
      <td>39ca914ceb82f7f14a38484023bc04f0cd5d0a8d</td>
      <td>KoboldAI/OPT-2.7B-Erebus</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/pythainlp/wangchanglm-7.5B-sft-enth</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pythainlp__wangchanglm-7.5B-sft-enth</td>
      <td>32.37</td>
      <td>33.79</td>
      <td>58.99</td>
      <td>24.52</td>
      <td>34.90</td>
      <td>57.93</td>
      <td>0.53</td>
      <td>15.91</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>7.49</td>
      <td>6.0</td>
      <td>True</td>
      <td>eeee33ea6778a5e66184eeb4bf4294d4316b1933</td>
      <td>pythainlp/wangchanglm-7.5B-sft-enth</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Rallio67/3B-redpajama-conditional-alpha</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Rallio67__3B-redpajama-conditional-alpha</td>
      <td>32.37</td>
      <td>36.26</td>
      <td>61.90</td>
      <td>25.42</td>
      <td>36.31</td>
      <td>60.77</td>
      <td>0.61</td>
      <td>5.28</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>2.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>7e2156c14b4b7981a4cd6db7b878888a98144df0</td>
      <td>Rallio67/3B-redpajama-conditional-alpha</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-7b-Python-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-7b-Python-hf</td>
      <td>32.34</td>
      <td>31.31</td>
      <td>52.86</td>
      <td>27.32</td>
      <td>42.21</td>
      <td>63.06</td>
      <td>4.55</td>
      <td>5.07</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>54.0</td>
      <td>True</td>
      <td>ec4dd26f30674fdee00ef161b55f464ce28f9c20</td>
      <td>codellama/CodeLlama-7b-Python-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-2.7B-Nerybus-Mix</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-2.7B-Nerybus-Mix</td>
      <td>32.29</td>
      <td>33.70</td>
      <td>61.21</td>
      <td>26.60</td>
      <td>37.57</td>
      <td>62.04</td>
      <td>0.15</td>
      <td>4.79</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>2.65</td>
      <td>9.0</td>
      <td>True</td>
      <td>b4131723cfff1fa42f6cbab546c5b4bb0d19fd83</td>
      <td>KoboldAI/OPT-2.7B-Nerybus-Mix</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/euclaise/falcon_1b_stage2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_euclaise__falcon_1b_stage2</td>
      <td>32.29</td>
      <td>33.11</td>
      <td>63.19</td>
      <td>24.22</td>
      <td>38.40</td>
      <td>62.35</td>
      <td>0.00</td>
      <td>4.75</td>
      <td>FalconForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>1.0</td>
      <td>True</td>
      <td>025c77e9ee457c6771c5a36dbacd064c269642a5</td>
      <td>euclaise/falcon_1b_stage2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/MayaPH/opt-flan-iml-6.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MayaPH__opt-flan-iml-6.7b</td>
      <td>32.27</td>
      <td>30.12</td>
      <td>58.82</td>
      <td>25.12</td>
      <td>36.74</td>
      <td>64.25</td>
      <td>0.00</td>
      <td>10.84</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>6.66</td>
      <td>1.0</td>
      <td>True</td>
      <td>cbe8d60db6f3c52e653ca73e23a1c34c08127d02</td>
      <td>MayaPH/opt-flan-iml-6.7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TFLai/pythia-2.8b-4bit-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__pythia-2.8b-4bit-alpaca</td>
      <td>32.20</td>
      <td>34.73</td>
      <td>58.96</td>
      <td>25.53</td>
      <td>39.14</td>
      <td>61.64</td>
      <td>0.61</td>
      <td>4.82</td>
      <td>?</td>
      <td>4bit</td>
      <td>?</td>
      <td>2.80</td>
      <td>1.0</td>
      <td>False</td>
      <td>40e84b6d38aac92a0302c2a682498794ef0fd901</td>
      <td>TFLai/pythia-2.8b-4bit-alpaca</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-2.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-2.7b</td>
      <td>32.17</td>
      <td>33.96</td>
      <td>61.43</td>
      <td>25.43</td>
      <td>37.43</td>
      <td>61.96</td>
      <td>0.23</td>
      <td>4.77</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>2.65</td>
      <td>46.0</td>
      <td>True</td>
      <td>397f71a473a150c00f0fe3fc4a2f78ff3ccaf82d</td>
      <td>facebook/opt-2.7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/chopt-2_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__chopt-2_7b</td>
      <td>32.17</td>
      <td>36.01</td>
      <td>63.38</td>
      <td>25.44</td>
      <td>37.71</td>
      <td>57.77</td>
      <td>0.00</td>
      <td>4.86</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>2.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>45f57352c10a1fb1ec13c4bf387a15552ca1fe65</td>
      <td>aisquared/chopt-2_7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-2.7B-Nerys-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-2.7B-Nerys-v2</td>
      <td>32.16</td>
      <td>33.28</td>
      <td>61.23</td>
      <td>26.44</td>
      <td>37.23</td>
      <td>62.04</td>
      <td>0.30</td>
      <td>4.60</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>2.65</td>
      <td>5.0</td>
      <td>True</td>
      <td>91d7afd6dbf3bbd1e4ccc6b9a2618d632a8cbb92</td>
      <td>KoboldAI/OPT-2.7B-Nerys-v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/L-R/LLmRa-2.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_L-R__LLmRa-2.7B</td>
      <td>32.16</td>
      <td>37.03</td>
      <td>60.65</td>
      <td>25.58</td>
      <td>35.23</td>
      <td>61.56</td>
      <td>0.30</td>
      <td>4.76</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>2.70</td>
      <td>0.0</td>
      <td>True</td>
      <td>93201b7d778272fb3252481c1cbd56f726d43e6b</td>
      <td>L-R/LLmRa-2.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/danielhanchen/open_llama_3b_600bt_preview</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_danielhanchen__open_llama_3b_600bt_preview</td>
      <td>32.13</td>
      <td>36.86</td>
      <td>59.96</td>
      <td>25.97</td>
      <td>32.81</td>
      <td>63.69</td>
      <td>0.61</td>
      <td>5.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>d8fddf7651dfcae5aefda59d9e868c9111d8bdb3</td>
      <td>danielhanchen/open_llama_3b_600bt_preview</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-2.8b-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-2.8b-deduped</td>
      <td>32.11</td>
      <td>36.26</td>
      <td>60.66</td>
      <td>26.78</td>
      <td>35.56</td>
      <td>60.22</td>
      <td>0.83</td>
      <td>4.47</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.91</td>
      <td>12.0</td>
      <td>True</td>
      <td>7d977fed8c4ce9649816af8cd5fe36a639cbe5b2</td>
      <td>EleutherAI/pythia-2.8b-deduped</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bertin-project/bertin-gpt-j-6B-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bertin-project__bertin-gpt-j-6B-alpaca</td>
      <td>32.11</td>
      <td>36.01</td>
      <td>54.30</td>
      <td>27.66</td>
      <td>43.38</td>
      <td>55.80</td>
      <td>0.00</td>
      <td>7.59</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>5.84</td>
      <td>8.0</td>
      <td>True</td>
      <td>636b17d6044189343475d1889f076aba73036905</td>
      <td>bertin-project/bertin-gpt-j-6B-alpaca</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__wizard-vicuna-13B-GPTQ</td>
      <td>32.08</td>
      <td>28.67</td>
      <td>25.94</td>
      <td>25.84</td>
      <td>48.53</td>
      <td>74.74</td>
      <td>9.63</td>
      <td>11.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>99.0</td>
      <td>True</td>
      <td>936a51c0219744d7a9598d0c65a7d18e01660601</td>
      <td>TheBloke/wizard-vicuna-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardCoder-15B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardCoder-15B-V1.0</td>
      <td>32.05</td>
      <td>32.34</td>
      <td>47.20</td>
      <td>29.43</td>
      <td>41.56</td>
      <td>55.17</td>
      <td>2.12</td>
      <td>16.55</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-openrail-m</td>
      <td>15.52</td>
      <td>621.0</td>
      <td>True</td>
      <td>926ca1b215c4631bc5f8c3e47173381452c23e5c</td>
      <td>WizardLM/WizardCoder-15B-V1.0</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/PSanni/Deer-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PSanni__Deer-3b</td>
      <td>32.01</td>
      <td>38.48</td>
      <td>57.41</td>
      <td>25.64</td>
      <td>39.98</td>
      <td>57.46</td>
      <td>0.30</td>
      <td>4.83</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>53ea8f8862fc1820f0cd31f62953b7290fd79867</td>
      <td>PSanni/Deer-3b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/codellama/CodeLlama-7b-Python-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codellama__CodeLlama-7b-Python-hf</td>
      <td>31.93</td>
      <td>29.27</td>
      <td>50.12</td>
      <td>28.37</td>
      <td>41.61</td>
      <td>64.01</td>
      <td>5.16</td>
      <td>4.99</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>6.74</td>
      <td>54.0</td>
      <td>True</td>
      <td>ec4dd26f30674fdee00ef161b55f464ce28f9c20</td>
      <td>codellama/CodeLlama-7b-Python-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_h2oai__h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt</td>
      <td>31.78</td>
      <td>34.04</td>
      <td>50.51</td>
      <td>24.66</td>
      <td>41.80</td>
      <td>54.93</td>
      <td>0.00</td>
      <td>16.53</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>12.0</td>
      <td>True</td>
      <td>754e0c90ed5d9241fdfd5a188572b3ea2152eaa7</td>
      <td>h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b-preview-300bt</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-6.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-6.7B</td>
      <td>31.77</td>
      <td>35.07</td>
      <td>59.36</td>
      <td>25.93</td>
      <td>38.02</td>
      <td>58.72</td>
      <td>0.53</td>
      <td>4.73</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.66</td>
      <td>61.0</td>
      <td>True</td>
      <td>4f56c6e28f9a2a1c470626f1a064238806f19f09</td>
      <td>cerebras/Cerebras-GPT-6.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LoupGarou/WizardCoder-Guanaco-15B-V1.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LoupGarou__WizardCoder-Guanaco-15B-V1.1</td>
      <td>31.73</td>
      <td>32.59</td>
      <td>45.42</td>
      <td>25.88</td>
      <td>42.33</td>
      <td>56.04</td>
      <td>2.88</td>
      <td>16.98</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>[apache-2.0]</td>
      <td>15.52</td>
      <td>11.0</td>
      <td>True</td>
      <td>979531c84ec0b4e1712d6a5cec6907126a21e605</td>
      <td>LoupGarou/WizardCoder-Guanaco-15B-V1.1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/gpt-neo-2.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-neo-2.7B</td>
      <td>31.71</td>
      <td>33.36</td>
      <td>56.24</td>
      <td>26.45</td>
      <td>39.78</td>
      <td>60.06</td>
      <td>1.29</td>
      <td>4.77</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>2.72</td>
      <td>361.0</td>
      <td>True</td>
      <td>e24fa291132763e59f4a5422741b424fb5d59056</td>
      <td>EleutherAI/gpt-neo-2.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/fairseq-dense-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__fairseq-dense-1.3B</td>
      <td>31.66</td>
      <td>31.14</td>
      <td>58.39</td>
      <td>24.98</td>
      <td>37.43</td>
      <td>59.04</td>
      <td>0.00</td>
      <td>10.60</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.41</td>
      <td>4.0</td>
      <td>True</td>
      <td>20bf1732212ea81adb45b782a25ce69e65a01ad2</td>
      <td>KoboldAI/fairseq-dense-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bhenrym14__airoboros-33b-gpt4-1.4.1-PI-8192-fp16</td>
      <td>31.60</td>
      <td>32.00</td>
      <td>53.88</td>
      <td>31.43</td>
      <td>38.59</td>
      <td>56.83</td>
      <td>0.00</td>
      <td>8.44</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.53</td>
      <td>1.0</td>
      <td>True</td>
      <td>1dd7804dbbb547c1be852652ce74568ba41d4e73</td>
      <td>bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Dampish/StellarX-4B-V0.2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Dampish__StellarX-4B-V0.2</td>
      <td>31.56</td>
      <td>34.64</td>
      <td>56.74</td>
      <td>25.55</td>
      <td>38.55</td>
      <td>61.40</td>
      <td>0.00</td>
      <td>4.01</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>2.65</td>
      <td>1.0</td>
      <td>True</td>
      <td>605b6812956400dbde24ad7b8649a744a2ddfc8e</td>
      <td>Dampish/StellarX-4B-V0.2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/LaMini-GPT-1.5B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__LaMini-GPT-1.5B</td>
      <td>31.56</td>
      <td>31.40</td>
      <td>48.38</td>
      <td>29.92</td>
      <td>42.47</td>
      <td>55.88</td>
      <td>0.00</td>
      <td>12.85</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.56</td>
      <td>32.0</td>
      <td>True</td>
      <td>88ca6f5abe2335bac317e82684e574afdd6046b5</td>
      <td>MBZUAI/LaMini-GPT-1.5B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/bigscience/bloom-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloom-3b</td>
      <td>31.50</td>
      <td>35.75</td>
      <td>54.37</td>
      <td>26.59</td>
      <td>40.57</td>
      <td>57.62</td>
      <td>0.83</td>
      <td>4.74</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>3.00</td>
      <td>70.0</td>
      <td>True</td>
      <td>52bc5b43010b4844513826b8be3f78c7344c37d7</td>
      <td>bigscience/bloom-3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_princeton-nlp__Sheared-LLaMA-1.3B</td>
      <td>31.47</td>
      <td>32.85</td>
      <td>60.91</td>
      <td>25.71</td>
      <td>37.14</td>
      <td>58.64</td>
      <td>0.45</td>
      <td>4.56</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.28</td>
      <td>28.0</td>
      <td>True</td>
      <td>b1c3f74c8495e27b3963d64af0781d4a611794f3</td>
      <td>princeton-nlp/Sheared-LLaMA-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/GPT-J-6B-Adventure</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__GPT-J-6B-Adventure</td>
      <td>31.47</td>
      <td>37.12</td>
      <td>61.26</td>
      <td>25.94</td>
      <td>34.56</td>
      <td>55.96</td>
      <td>0.83</td>
      <td>4.58</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>15.0</td>
      <td>True</td>
      <td>e2c00dc99f986f2430f5d34c0214969cee786755</td>
      <td>KoboldAI/GPT-J-6B-Adventure</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/openchat_v2_openorca_preview-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__openchat_v2_openorca_preview-GPTQ</td>
      <td>31.46</td>
      <td>27.99</td>
      <td>26.06</td>
      <td>24.24</td>
      <td>50.08</td>
      <td>70.64</td>
      <td>13.27</td>
      <td>7.96</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>14.0</td>
      <td>True</td>
      <td>5a4c2ea612b71d7c00118f796db7189bc1a0c930</td>
      <td>TheBloke/openchat_v2_openorca_preview-GPTQ</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/HuggingFaceH4/starchat-alpha</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HuggingFaceH4__starchat-alpha</td>
      <td>31.43</td>
      <td>31.57</td>
      <td>49.43</td>
      <td>30.76</td>
      <td>43.66</td>
      <td>55.09</td>
      <td>2.43</td>
      <td>7.07</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>bigcode-openrail-m</td>
      <td>15.52</td>
      <td>220.0</td>
      <td>True</td>
      <td>b693a7a7d52bed1cd7cc0fe00399db838b09c74f</td>
      <td>HuggingFaceH4/starchat-alpha</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/OpenAssistant/stablelm-7b-sft-v7-epoch-3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_OpenAssistant__stablelm-7b-sft-v7-epoch-3</td>
      <td>31.38</td>
      <td>36.01</td>
      <td>55.81</td>
      <td>25.01</td>
      <td>37.02</td>
      <td>54.85</td>
      <td>0.38</td>
      <td>10.61</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>7.56</td>
      <td>65.0</td>
      <td>True</td>
      <td>4c454bfc0e3618b3d574e28ba71369607e637e91</td>
      <td>OpenAssistant/stablelm-7b-sft-v7-epoch-3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bhenrym14__airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
      <td>31.33</td>
      <td>25.34</td>
      <td>26.66</td>
      <td>23.36</td>
      <td>49.51</td>
      <td>73.72</td>
      <td>8.57</td>
      <td>12.15</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>32.53</td>
      <td>4.0</td>
      <td>True</td>
      <td>468225a547a8cb0a62758d813cf9606b58506ab4</td>
      <td>bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__chronos-wizardlm-uc-scot-st-13B-GPTQ</td>
      <td>31.30</td>
      <td>27.99</td>
      <td>26.10</td>
      <td>25.72</td>
      <td>49.68</td>
      <td>74.51</td>
      <td>6.90</td>
      <td>8.20</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>6.0</td>
      <td>True</td>
      <td>c4246e4b8d3fc77b9fe4ebb1ead61cda4b83575b</td>
      <td>TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/42dot/42dot_LLM-PLM-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_42dot__42dot_LLM-PLM-1.3B</td>
      <td>31.26</td>
      <td>32.42</td>
      <td>56.39</td>
      <td>27.09</td>
      <td>38.68</td>
      <td>58.88</td>
      <td>0.76</td>
      <td>4.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.44</td>
      <td>12.0</td>
      <td>True</td>
      <td>a72bf57eb02cd4ea4388a344b4a5893aa95698da</td>
      <td>42dot/42dot_LLM-PLM-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/AtAndDev/ShortKingv0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_AtAndDev__ShortKingv0.1</td>
      <td>31.17</td>
      <td>34.22</td>
      <td>54.59</td>
      <td>25.78</td>
      <td>41.64</td>
      <td>56.04</td>
      <td>0.45</td>
      <td>5.47</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.42</td>
      <td>2.0</td>
      <td>True</td>
      <td>6cd9b5bc13ee15b5e7e7cfb46477bc6a7c0b5d47</td>
      <td>AtAndDev/ShortKingv0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/sartmis1/starcoder-finetune-selfinstruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_sartmis1__starcoder-finetune-selfinstruct</td>
      <td>31.16</td>
      <td>31.23</td>
      <td>47.66</td>
      <td>29.52</td>
      <td>41.63</td>
      <td>57.77</td>
      <td>6.07</td>
      <td>4.22</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>b21bd307ea7417185e7dc59557c399a3e4e0092b</td>
      <td>sartmis1/starcoder-finetune-selfinstruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PY007__TinyLlama-1.1B-Chat-v0.3</td>
      <td>31.14</td>
      <td>35.07</td>
      <td>57.70</td>
      <td>25.53</td>
      <td>36.67</td>
      <td>57.70</td>
      <td>0.68</td>
      <td>4.63</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.03</td>
      <td>5.0</td>
      <td>False</td>
      <td>20dd44d78aa09480bf15ca0ecc0c0780951d49a9</td>
      <td>PY007/TinyLlama-1.1B-Chat-v0.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/lamini-neo-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__lamini-neo-1.3b</td>
      <td>31.12</td>
      <td>32.76</td>
      <td>49.13</td>
      <td>28.79</td>
      <td>41.05</td>
      <td>56.51</td>
      <td>0.15</td>
      <td>9.47</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.32</td>
      <td>11.0</td>
      <td>True</td>
      <td>a5c7ecc4d908e7a9469d080308af64ae775c733d</td>
      <td>MBZUAI/lamini-neo-1.3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/L-R/LLmRa-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_L-R__LLmRa-1.3B</td>
      <td>31.10</td>
      <td>32.68</td>
      <td>58.77</td>
      <td>23.23</td>
      <td>36.21</td>
      <td>59.04</td>
      <td>0.08</td>
      <td>7.72</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>8d5e8bb336cb886e20a7570bc00c2381792338a5</td>
      <td>L-R/LLmRa-1.3B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/wizard-mega-13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__wizard-mega-13B-GPTQ</td>
      <td>31.08</td>
      <td>27.73</td>
      <td>26.01</td>
      <td>24.97</td>
      <td>48.69</td>
      <td>74.74</td>
      <td>8.95</td>
      <td>6.48</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>102.0</td>
      <td>True</td>
      <td>848bf2514f804799dd28c188e5428d497dc983fb</td>
      <td>TheBloke/wizard-mega-13B-GPTQ</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lizhuang144/starcoder_mirror</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lizhuang144__starcoder_mirror</td>
      <td>31.07</td>
      <td>31.31</td>
      <td>45.82</td>
      <td>29.29</td>
      <td>43.38</td>
      <td>57.22</td>
      <td>5.53</td>
      <td>4.90</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>False</td>
      <td>eb5f39bac15ccab9463001aa203e33d49f4ff7cb</td>
      <td>lizhuang144/starcoder_mirror</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/xglm-4.5B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__xglm-4.5B</td>
      <td>31.05</td>
      <td>31.48</td>
      <td>57.95</td>
      <td>25.43</td>
      <td>35.84</td>
      <td>54.93</td>
      <td>0.23</td>
      <td>11.48</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>5.08</td>
      <td>11.0</td>
      <td>True</td>
      <td>dc6a67fac06c8bca7860b84656a0cb736293a7a8</td>
      <td>facebook/xglm-4.5B</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/stabilityai/stablelm-base-alpha-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__stablelm-base-alpha-7b</td>
      <td>31.02</td>
      <td>32.00</td>
      <td>51.78</td>
      <td>26.21</td>
      <td>40.19</td>
      <td>55.41</td>
      <td>0.61</td>
      <td>10.95</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>7.56</td>
      <td>208.0</td>
      <td>True</td>
      <td>38366357b5a45e002af2d254ff3d559444ec2147</td>
      <td>stabilityai/stablelm-base-alpha-7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-4-3b-pile</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-4-3b-pile</td>
      <td>31.00</td>
      <td>36.01</td>
      <td>59.66</td>
      <td>24.67</td>
      <td>32.14</td>
      <td>58.33</td>
      <td>0.68</td>
      <td>5.52</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>2.86</td>
      <td>2.0</td>
      <td>True</td>
      <td>7fdda3c5570d4a9711f8f02cc3a20941a5623cd3</td>
      <td>RWKV/rwkv-4-3b-pile</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td>31.00</td>
      <td>25.00</td>
      <td>25.00</td>
      <td>25.00</td>
      <td>25.00</td>
      <td>50.00</td>
      <td>0.21</td>
      <td>0.47</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>None</td>
      <td>N/A</td>
      <td>baseline</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/chopt-1_3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__chopt-1_3b</td>
      <td>30.94</td>
      <td>31.48</td>
      <td>56.63</td>
      <td>25.35</td>
      <td>40.19</td>
      <td>58.25</td>
      <td>0.00</td>
      <td>4.67</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>fdd3691978f557baf9d1c20d4ede900c47f7e135</td>
      <td>aisquared/chopt-1_3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Writer/palmyra-base</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Writer__palmyra-base</td>
      <td>30.84</td>
      <td>31.91</td>
      <td>55.39</td>
      <td>27.15</td>
      <td>37.57</td>
      <td>58.09</td>
      <td>0.99</td>
      <td>4.80</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>5.05</td>
      <td>33.0</td>
      <td>True</td>
      <td>df2f3bdb7cbe4295d69cf0cbc35f3ceaf451de82</td>
      <td>Writer/palmyra-base</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ehartford/CodeLlama-34b-Python-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ehartford__CodeLlama-34b-Python-hf</td>
      <td>30.82</td>
      <td>38.05</td>
      <td>34.79</td>
      <td>32.96</td>
      <td>43.57</td>
      <td>66.14</td>
      <td>0.00</td>
      <td>0.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>33.48</td>
      <td>2.0</td>
      <td>True</td>
      <td>45f38e53a579a2b39298cc57ab04078722bebec0</td>
      <td>ehartford/CodeLlama-34b-Python-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/euclaise/falcon_1b_stage3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_euclaise__falcon_1b_stage3</td>
      <td>30.81</td>
      <td>33.11</td>
      <td>54.08</td>
      <td>25.11</td>
      <td>37.92</td>
      <td>59.51</td>
      <td>0.00</td>
      <td>5.95</td>
      <td>FalconForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>593e48197e91537b203ba288260f6580b9cbcbe6</td>
      <td>euclaise/falcon_1b_stage3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HWERI/pythia-1.4b-deduped-sharegpt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HWERI__pythia-1.4b-deduped-sharegpt</td>
      <td>30.79</td>
      <td>34.30</td>
      <td>54.49</td>
      <td>24.00</td>
      <td>41.81</td>
      <td>55.25</td>
      <td>0.83</td>
      <td>4.88</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.42</td>
      <td>1.0</td>
      <td>True</td>
      <td>5b50336208840f557ef3301d841e7994caaa63bb</td>
      <td>HWERI/pythia-1.4b-deduped-sharegpt</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/beaugogh/pythia-1.4b-deduped-sharegpt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beaugogh__pythia-1.4b-deduped-sharegpt</td>
      <td>30.79</td>
      <td>34.30</td>
      <td>54.49</td>
      <td>24.00</td>
      <td>41.81</td>
      <td>55.25</td>
      <td>0.83</td>
      <td>4.88</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.42</td>
      <td>0.0</td>
      <td>True</td>
      <td>03dfdc25c111a6a4a16d3da12190697611936426</td>
      <td>beaugogh/pythia-1.4b-deduped-sharegpt</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/metharme-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__metharme-1.3b</td>
      <td>30.71</td>
      <td>34.39</td>
      <td>55.94</td>
      <td>25.07</td>
      <td>37.68</td>
      <td>56.43</td>
      <td>0.76</td>
      <td>4.73</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.52</td>
      <td>18.0</td>
      <td>True</td>
      <td>62ec4ff53042f692ef0661e54f371747214707a4</td>
      <td>PygmalionAI/metharme-1.3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-1.4b-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-1.4b-deduped</td>
      <td>30.62</td>
      <td>32.68</td>
      <td>54.96</td>
      <td>25.56</td>
      <td>38.66</td>
      <td>57.30</td>
      <td>0.83</td>
      <td>4.33</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>18.0</td>
      <td>True</td>
      <td>77f320b24ccae4aa85a5890dbb9514bd11267bb3</td>
      <td>EleutherAI/pythia-1.4b-deduped</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/gogpt-3b-bloom</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gogpt-3b-bloom</td>
      <td>30.60</td>
      <td>31.91</td>
      <td>50.32</td>
      <td>25.20</td>
      <td>41.79</td>
      <td>54.38</td>
      <td>0.15</td>
      <td>10.48</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.00</td>
      <td>5.0</td>
      <td>True</td>
      <td>fe942d5d0faca8156eaf456ecdf569993eab8062</td>
      <td>golaxy/gogpt-3b-bloom</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-13b-chat-longlora-32k-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-13b-chat-longlora-32k-sft</td>
      <td>30.42</td>
      <td>26.54</td>
      <td>26.10</td>
      <td>23.12</td>
      <td>49.16</td>
      <td>64.33</td>
      <td>0.00</td>
      <td>23.71</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>13.02</td>
      <td>19.0</td>
      <td>True</td>
      <td>6f2924e354c3ab035aa2ff7c7e28d0e5327e2667</td>
      <td>Yukang/Llama-2-13b-chat-longlora-32k-sft</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/w95/megachat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_w95__megachat</td>
      <td>30.38</td>
      <td>30.80</td>
      <td>54.35</td>
      <td>25.55</td>
      <td>39.85</td>
      <td>56.99</td>
      <td>0.99</td>
      <td>4.16</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>789b259a18ca7b168ced4995138ad6195cd2e8e8</td>
      <td>w95/megachat</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/facebook/opt-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-1.3b</td>
      <td>30.38</td>
      <td>29.52</td>
      <td>54.53</td>
      <td>24.96</td>
      <td>38.71</td>
      <td>59.75</td>
      <td>0.15</td>
      <td>5.02</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>1.32</td>
      <td>117.0</td>
      <td>True</td>
      <td>8c7b10754972749675d22364c25c428b29face51</td>
      <td>facebook/opt-1.3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/LoupGarou/WizardCoder-Guanaco-15B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_LoupGarou__WizardCoder-Guanaco-15B-V1.0</td>
      <td>30.36</td>
      <td>30.46</td>
      <td>45.59</td>
      <td>26.79</td>
      <td>46.39</td>
      <td>53.12</td>
      <td>1.44</td>
      <td>8.71</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>[apache-2.0]</td>
      <td>15.52</td>
      <td>6.0</td>
      <td>True</td>
      <td>ab5ea678d63eb2324658dcc8cfae267eabc366ef</td>
      <td>LoupGarou/WizardCoder-Guanaco-15B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/pygmalion-2.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__pygmalion-2.7b</td>
      <td>30.32</td>
      <td>32.76</td>
      <td>54.13</td>
      <td>23.28</td>
      <td>37.17</td>
      <td>56.51</td>
      <td>0.00</td>
      <td>8.41</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>creativeml-openrail-m</td>
      <td>2.65</td>
      <td>48.0</td>
      <td>True</td>
      <td>9533805293bc48e8ddfe9dc1940d8cbc5662113e</td>
      <td>PygmalionAI/pygmalion-2.7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-13b-chat-longlora-32k-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-13b-chat-longlora-32k-sft</td>
      <td>30.32</td>
      <td>26.11</td>
      <td>26.17</td>
      <td>23.12</td>
      <td>49.07</td>
      <td>64.09</td>
      <td>0.00</td>
      <td>23.66</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>13.02</td>
      <td>19.0</td>
      <td>True</td>
      <td>6f2924e354c3ab035aa2ff7c7e28d0e5327e2667</td>
      <td>Yukang/Llama-2-13b-chat-longlora-32k-sft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PY007__TinyLlama-1.1B-Chat-v0.1</td>
      <td>30.31</td>
      <td>32.00</td>
      <td>54.21</td>
      <td>26.71</td>
      <td>39.03</td>
      <td>54.93</td>
      <td>0.53</td>
      <td>4.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>22.0</td>
      <td>True</td>
      <td>7abc14e7779eabc3a028bc695342869d0410dea2</td>
      <td>PY007/TinyLlama-1.1B-Chat-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/EverythingLM-13B-16K-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__EverythingLM-13B-16K-GPTQ</td>
      <td>30.29</td>
      <td>29.27</td>
      <td>26.24</td>
      <td>25.40</td>
      <td>48.58</td>
      <td>71.35</td>
      <td>5.38</td>
      <td>5.83</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>16.23</td>
      <td>10.0</td>
      <td>True</td>
      <td>f14d3df05577f3e1ac35e2c4ec32ce0d39b97508</td>
      <td>TheBloke/EverythingLM-13B-16K-GPTQ</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/tinyllama-1.1b-chat-v0.3_platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__tinyllama-1.1b-chat-v0.3_platypus</td>
      <td>30.28</td>
      <td>30.29</td>
      <td>55.12</td>
      <td>26.13</td>
      <td>39.15</td>
      <td>55.80</td>
      <td>0.53</td>
      <td>4.94</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.03</td>
      <td>3.0</td>
      <td>True</td>
      <td>0bb6ebe1d41d394bae0ed9107ec8d776d9d76a68</td>
      <td>lgaalves/tinyllama-1.1b-chat-v0.3_platypus</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/RWKV/rwkv-raven-1b5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-raven-1b5</td>
      <td>30.17</td>
      <td>31.83</td>
      <td>52.60</td>
      <td>25.96</td>
      <td>37.09</td>
      <td>53.91</td>
      <td>0.00</td>
      <td>9.82</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.41</td>
      <td>7.0</td>
      <td>True</td>
      <td>571a3bd891ce33f2ee3fc6de09218178edb0dae2</td>
      <td>RWKV/rwkv-raven-1b5</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-1.3b</td>
      <td>30.11</td>
      <td>31.14</td>
      <td>51.43</td>
      <td>26.55</td>
      <td>39.24</td>
      <td>57.38</td>
      <td>0.99</td>
      <td>4.06</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>7.0</td>
      <td>True</td>
      <td>34b668ff0acfe56f2d541aa46b385557ee39eb3f</td>
      <td>EleutherAI/pythia-1.3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/NYTK/PULI-GPTrio</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_NYTK__PULI-GPTrio</td>
      <td>30.07</td>
      <td>30.72</td>
      <td>53.49</td>
      <td>24.73</td>
      <td>39.03</td>
      <td>57.77</td>
      <td>0.76</td>
      <td>4.03</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>7.06</td>
      <td>5.0</td>
      <td>True</td>
      <td>c85efce322a0f6d93d64f7b9096525753da6913e</td>
      <td>NYTK/PULI-GPTrio</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PY007__TinyLlama-1.1B-intermediate-step-480k-1T</td>
      <td>30.06</td>
      <td>30.89</td>
      <td>52.97</td>
      <td>25.00</td>
      <td>39.55</td>
      <td>57.30</td>
      <td>0.53</td>
      <td>4.18</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.03</td>
      <td>15.0</td>
      <td>True</td>
      <td>098830e58452a0a08f90eb0189ec5925803fd48b</td>
      <td>PY007/TinyLlama-1.1B-intermediate-step-480k-1T</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v2-1_5b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v2-1_5b</td>
      <td>30.03</td>
      <td>32.59</td>
      <td>53.98</td>
      <td>24.93</td>
      <td>38.77</td>
      <td>54.70</td>
      <td>0.23</td>
      <td>5.04</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.56</td>
      <td>10.0</td>
      <td>True</td>
      <td>97440ff1b6ef749423758e3495cdce1b5e68ee92</td>
      <td>aisquared/dlite-v2-1_5b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__stablelm-tuned-alpha-7b</td>
      <td>29.98</td>
      <td>31.91</td>
      <td>53.59</td>
      <td>24.41</td>
      <td>40.37</td>
      <td>53.12</td>
      <td>0.83</td>
      <td>5.62</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>7.56</td>
      <td>350.0</td>
      <td>True</td>
      <td>25071b093c15c0d1cb2b2876c6deb621b764fcf5</td>
      <td>stabilityai/stablelm-tuned-alpha-7b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2-xl_lima</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2-xl_lima</td>
      <td>29.95</td>
      <td>31.14</td>
      <td>51.28</td>
      <td>25.43</td>
      <td>38.74</td>
      <td>57.22</td>
      <td>0.91</td>
      <td>4.89</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.00</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7db5b1db521abd7578b95138e737637e0037ca5</td>
      <td>lgaalves/gpt2-xl_lima</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/winglian/llama-2-4b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_winglian__llama-2-4b</td>
      <td>29.91</td>
      <td>31.23</td>
      <td>53.29</td>
      <td>24.22</td>
      <td>38.72</td>
      <td>57.46</td>
      <td>0.45</td>
      <td>3.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>3.37</td>
      <td>2.0</td>
      <td>True</td>
      <td>fbba77f9894cf738ad8d7d08fc6874856fb42507</td>
      <td>winglian/llama-2-4b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt-2-xl_camel-ai-physics</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt-2-xl_camel-ai-physics</td>
      <td>29.90</td>
      <td>29.52</td>
      <td>50.62</td>
      <td>26.79</td>
      <td>39.12</td>
      <td>57.54</td>
      <td>0.15</td>
      <td>5.57</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>e20cf5a8c89441f4dc15fd2af12dbe72b7df8e60</td>
      <td>lgaalves/gpt-2-xl_camel-ai-physics</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/jzjiao/opt-1.3b-rlhf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jzjiao__opt-1.3b-rlhf</td>
      <td>29.90</td>
      <td>28.92</td>
      <td>52.77</td>
      <td>25.39</td>
      <td>37.44</td>
      <td>58.96</td>
      <td>0.15</td>
      <td>5.64</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>5b12df71b21b6b7d76ca9d56de6751f25022e854</td>
      <td>jzjiao/opt-1.3b-rlhf</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/polyglot-ko-12.8b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__polyglot-ko-12.8b</td>
      <td>29.86</td>
      <td>27.05</td>
      <td>51.68</td>
      <td>26.64</td>
      <td>34.69</td>
      <td>59.75</td>
      <td>0.15</td>
      <td>9.07</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>13.06</td>
      <td>59.0</td>
      <td>True</td>
      <td>09dfc839067bf44e7f52976eca8adbc17f04e1b0</td>
      <td>EleutherAI/polyglot-ko-12.8b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloom-1b7</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloom-1b7</td>
      <td>29.79</td>
      <td>30.63</td>
      <td>47.60</td>
      <td>27.48</td>
      <td>41.31</td>
      <td>56.04</td>
      <td>0.45</td>
      <td>5.03</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>1.72</td>
      <td>100.0</td>
      <td>True</td>
      <td>cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5</td>
      <td>bigscience/bloom-1b7</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/Quokka_2.7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__Quokka_2.7b</td>
      <td>29.72</td>
      <td>31.06</td>
      <td>47.72</td>
      <td>24.80</td>
      <td>40.14</td>
      <td>55.49</td>
      <td>0.38</td>
      <td>8.43</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.79</td>
      <td>0.0</td>
      <td>True</td>
      <td>abe5e0f574d32f3234035b6e8c5d68bbb201e03c</td>
      <td>Corianas/Quokka_2.7b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/MBZUAI/LaMini-GPT-774M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__LaMini-GPT-774M</td>
      <td>29.59</td>
      <td>27.65</td>
      <td>43.81</td>
      <td>26.30</td>
      <td>40.26</td>
      <td>56.59</td>
      <td>0.00</td>
      <td>12.51</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.77</td>
      <td>8.0</td>
      <td>True</td>
      <td>4f3bd4b37d249e6aa335be677afd39f417e05b5d</td>
      <td>MBZUAI/LaMini-GPT-774M</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PY007__TinyLlama-1.1B-intermediate-step-240k-503b</td>
      <td>29.52</td>
      <td>29.27</td>
      <td>49.71</td>
      <td>26.26</td>
      <td>40.17</td>
      <td>56.59</td>
      <td>0.30</td>
      <td>4.38</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>14.0</td>
      <td>True</td>
      <td>213ebf60d7fdd3258fa5574840b06c97a7e8cf5d</td>
      <td>PY007/TinyLlama-1.1B-intermediate-step-240k-503b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v1-1_5b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v1-1_5b</td>
      <td>29.48</td>
      <td>31.66</td>
      <td>49.69</td>
      <td>25.62</td>
      <td>37.08</td>
      <td>55.96</td>
      <td>0.08</td>
      <td>6.29</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.56</td>
      <td>1.0</td>
      <td>True</td>
      <td>4ac21faec255e3544e96aeb3591c27bdee5ebf45</td>
      <td>aisquared/dlite-v1-1_5b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/gpt-neo-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-neo-1.3B</td>
      <td>29.44</td>
      <td>31.23</td>
      <td>48.47</td>
      <td>24.82</td>
      <td>39.63</td>
      <td>56.91</td>
      <td>0.45</td>
      <td>4.60</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.37</td>
      <td>206.0</td>
      <td>True</td>
      <td>8282180b53cba30a1575e49de1530019e5931739</td>
      <td>EleutherAI/gpt-neo-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/lxe/Cerebras-GPT-2.7B-Alpaca-SP</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lxe__Cerebras-GPT-2.7B-Alpaca-SP</td>
      <td>29.40</td>
      <td>30.80</td>
      <td>48.88</td>
      <td>25.12</td>
      <td>40.24</td>
      <td>55.41</td>
      <td>0.53</td>
      <td>4.78</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>10.0</td>
      <td>True</td>
      <td>ae7f22e90cb968b0a73355aa2001d6bc7df28477</td>
      <td>lxe/Cerebras-GPT-2.7B-Alpaca-SP</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MrNJK/gpt2-xl-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MrNJK__gpt2-xl-sft</td>
      <td>29.32</td>
      <td>30.03</td>
      <td>49.17</td>
      <td>25.56</td>
      <td>38.78</td>
      <td>55.56</td>
      <td>0.76</td>
      <td>5.35</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>53250831436460254b7ee9afc4014d4d3156b372</td>
      <td>MrNJK/gpt2-xl-sft</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-4-1b5-pile</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-4-1b5-pile</td>
      <td>29.24</td>
      <td>31.83</td>
      <td>52.25</td>
      <td>25.77</td>
      <td>35.80</td>
      <td>53.83</td>
      <td>0.00</td>
      <td>5.23</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.41</td>
      <td>5.0</td>
      <td>True</td>
      <td>643585471eaf5821d94dfcb498ab5b94a36b42cf</td>
      <td>RWKV/rwkv-4-1b5-pile</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-2.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-2.7B</td>
      <td>29.16</td>
      <td>29.10</td>
      <td>49.29</td>
      <td>25.17</td>
      <td>41.37</td>
      <td>54.14</td>
      <td>0.45</td>
      <td>4.58</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>2.65</td>
      <td>41.0</td>
      <td>True</td>
      <td>4383dfd80aafdbcfd0876419d246de51e6cbf7c1</td>
      <td>cerebras/Cerebras-GPT-2.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BEE-spoke-data/TinyLlama-1.1bee</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BEE-spoke-data__TinyLlama-1.1bee</td>
      <td>29.15</td>
      <td>30.55</td>
      <td>51.80</td>
      <td>24.25</td>
      <td>39.01</td>
      <td>54.46</td>
      <td>0.23</td>
      <td>3.74</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>5889ec467cf80a83c4092b55686f8121e81bf001</td>
      <td>BEE-spoke-data/TinyLlama-1.1bee</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/shaohang/SparseOPT-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shaohang__SparseOPT-1.3B</td>
      <td>29.14</td>
      <td>27.13</td>
      <td>48.69</td>
      <td>25.60</td>
      <td>39.11</td>
      <td>58.56</td>
      <td>0.08</td>
      <td>4.83</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>06249d582b0cfefac537dd6bee2e578002ffff00</td>
      <td>shaohang/SparseOPT-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/shaohang/Sparse0.5_OPT-1.3</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_shaohang__Sparse0.5_OPT-1.3</td>
      <td>29.13</td>
      <td>27.13</td>
      <td>48.69</td>
      <td>25.60</td>
      <td>39.11</td>
      <td>58.56</td>
      <td>0.08</td>
      <td>4.72</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>06249d582b0cfefac537dd6bee2e578002ffff00</td>
      <td>shaohang/Sparse0.5_OPT-1.3</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/YeungNLP/firefly-bloom-2b6-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_YeungNLP__firefly-bloom-2b6-v2</td>
      <td>29.09</td>
      <td>27.65</td>
      <td>39.23</td>
      <td>25.24</td>
      <td>42.27</td>
      <td>54.78</td>
      <td>1.74</td>
      <td>12.76</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>2.48</td>
      <td>9.0</td>
      <td>True</td>
      <td>8334b22c39937c0404e09dd22a867e2e2a6fc9e0</td>
      <td>YeungNLP/firefly-bloom-2b6-v2</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TurkuNLP/gpt3-finnish-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TurkuNLP__gpt3-finnish-13B</td>
      <td>29.08</td>
      <td>24.66</td>
      <td>46.76</td>
      <td>23.49</td>
      <td>44.47</td>
      <td>58.01</td>
      <td>0.30</td>
      <td>5.86</td>
      <td>BloomModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>13.26</td>
      <td>10.0</td>
      <td>True</td>
      <td>ade35fd78ac2c29f7a56ffd3087321d297bb97a9</td>
      <td>TurkuNLP/gpt3-finnish-13B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v2-774m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v2-774m</td>
      <td>29.01</td>
      <td>30.12</td>
      <td>47.68</td>
      <td>25.37</td>
      <td>40.00</td>
      <td>53.99</td>
      <td>0.00</td>
      <td>5.93</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.77</td>
      <td>8.0</td>
      <td>True</td>
      <td>0ea894a33e491912cd1a65dde47b4af03f03c4f2</td>
      <td>aisquared/dlite-v2-774m</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/orca_mini_13B-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__orca_mini_13B-GPTQ</td>
      <td>28.88</td>
      <td>27.30</td>
      <td>25.85</td>
      <td>25.31</td>
      <td>48.06</td>
      <td>63.77</td>
      <td>0.08</td>
      <td>11.77</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>16.22</td>
      <td>43.0</td>
      <td>True</td>
      <td>8ec18e5c597da86fa123c08b6e6bef7da6ec7440</td>
      <td>TheBloke/orca_mini_13B-GPTQ</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-1b-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-1b-deduped</td>
      <td>28.77</td>
      <td>29.10</td>
      <td>49.65</td>
      <td>24.27</td>
      <td>38.94</td>
      <td>53.59</td>
      <td>1.14</td>
      <td>4.71</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.08</td>
      <td>13.0</td>
      <td>True</td>
      <td>7199d8fc61a6d565cd1f3c62bf11525b563e13b2</td>
      <td>EleutherAI/pythia-1b-deduped</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096</td>
      <td>28.77</td>
      <td>30.63</td>
      <td>52.63</td>
      <td>25.04</td>
      <td>34.96</td>
      <td>52.80</td>
      <td>0.00</td>
      <td>5.33</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.41</td>
      <td>0.0</td>
      <td>True</td>
      <td>657e40fe890c2baa1705b45084a93a70b98842eb</td>
      <td>KnutJaegersberg/RWKV-4-PilePlus-1B5-20230520-2942-486Gtokens-ctx4096</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloomz-560m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloomz-560m</td>
      <td>28.71</td>
      <td>23.55</td>
      <td>36.31</td>
      <td>25.10</td>
      <td>45.69</td>
      <td>53.12</td>
      <td>0.00</td>
      <td>17.24</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>81.0</td>
      <td>True</td>
      <td>a2845d7e13dd12efae154a9f1c63fcc2e0cc4b05</td>
      <td>bigscience/bloomz-560m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TFLai/gpt-neo-1.3B-4bit-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__gpt-neo-1.3B-4bit-alpaca</td>
      <td>28.65</td>
      <td>28.24</td>
      <td>46.35</td>
      <td>25.19</td>
      <td>39.26</td>
      <td>56.20</td>
      <td>0.23</td>
      <td>5.12</td>
      <td>?</td>
      <td>4bit</td>
      <td>?</td>
      <td>1.30</td>
      <td>1.0</td>
      <td>False</td>
      <td>137d483d1dc757c81c59bd190016f7c5df01f978</td>
      <td>TFLai/gpt-neo-1.3B-4bit-alpaca</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Rachneet/gpt2-xl-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Rachneet__gpt2-xl-alpaca</td>
      <td>28.54</td>
      <td>26.79</td>
      <td>43.85</td>
      <td>26.31</td>
      <td>39.40</td>
      <td>56.91</td>
      <td>0.00</td>
      <td>6.55</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>a1a19acc0ef161bfa35f460c15ed3015595714d8</td>
      <td>Rachneet/gpt2-xl-alpaca</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/w601sxs/b1ade-1b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_w601sxs__b1ade-1b</td>
      <td>28.54</td>
      <td>28.58</td>
      <td>46.08</td>
      <td>25.11</td>
      <td>41.34</td>
      <td>53.83</td>
      <td>0.61</td>
      <td>4.25</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.91</td>
      <td>0.0</td>
      <td>True</td>
      <td>b4b0fd71589e6590089e1ec14a840ecab10894ae</td>
      <td>w601sxs/b1ade-1b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/facebook/xglm-1.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__xglm-1.7B</td>
      <td>28.52</td>
      <td>25.85</td>
      <td>45.68</td>
      <td>25.10</td>
      <td>37.21</td>
      <td>53.91</td>
      <td>0.76</td>
      <td>11.14</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.73</td>
      <td>8.0</td>
      <td>True</td>
      <td>d23a5e8e2164af31a84a26756b9b17f925143050</td>
      <td>facebook/xglm-1.7B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloom-1b1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloom-1b1</td>
      <td>28.50</td>
      <td>28.33</td>
      <td>42.78</td>
      <td>26.70</td>
      <td>41.80</td>
      <td>55.01</td>
      <td>0.08</td>
      <td>4.81</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>1.06</td>
      <td>41.0</td>
      <td>True</td>
      <td>6f4195539db0eef1c9d010289f32e0645d9a2354</td>
      <td>bigscience/bloom-1b1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/cmarkea/bloomz-560m-sft-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cmarkea__bloomz-560m-sft-chat</td>
      <td>28.49</td>
      <td>27.47</td>
      <td>37.05</td>
      <td>23.93</td>
      <td>42.35</td>
      <td>53.51</td>
      <td>0.00</td>
      <td>15.13</td>
      <td>BloomForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>8.0</td>
      <td>True</td>
      <td>e2bbcbdd534c7d75b7d2f9408e74f6682cf3a05e</td>
      <td>cmarkea/bloomz-560m-sft-chat</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/Locutusque/gpt2-large-conversational</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Locutusque__gpt2-large-conversational</td>
      <td>28.45</td>
      <td>26.96</td>
      <td>44.98</td>
      <td>26.33</td>
      <td>39.60</td>
      <td>56.04</td>
      <td>0.08</td>
      <td>5.19</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.bfloat16</td>
      <td>openrail</td>
      <td>0.77</td>
      <td>2.0</td>
      <td>True</td>
      <td>6674ad1ed9f518054561b866172eb88b7a769413</td>
      <td>Locutusque/gpt2-large-conversational</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/FabbriSimo01/Bloom_1b_Quantized</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FabbriSimo01__Bloom_1b_Quantized</td>
      <td>28.45</td>
      <td>27.73</td>
      <td>42.83</td>
      <td>26.28</td>
      <td>41.82</td>
      <td>55.64</td>
      <td>0.15</td>
      <td>4.71</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>1.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>f31188966c6735bd894edacfee8371a6eaf7dbc7</td>
      <td>FabbriSimo01/Bloom_1b_Quantized</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Salesforce/codegen-6B-multi</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Salesforce__codegen-6B-multi</td>
      <td>28.38</td>
      <td>27.22</td>
      <td>41.11</td>
      <td>25.71</td>
      <td>45.65</td>
      <td>53.91</td>
      <td>0.99</td>
      <td>4.06</td>
      <td>CodeGenForCausalLM</td>
      <td>torch.float16</td>
      <td>bsd-3-clause</td>
      <td>6.85</td>
      <td>18.0</td>
      <td>True</td>
      <td>2d58b1e73791e8f0be7ea59c2720dccb6f4d0f06</td>
      <td>Salesforce/codegen-6B-multi</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__stablelm-tuned-alpha-3b</td>
      <td>28.27</td>
      <td>27.82</td>
      <td>44.06</td>
      <td>23.08</td>
      <td>42.33</td>
      <td>55.01</td>
      <td>0.53</td>
      <td>5.06</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>[cc-by-nc-sa-4.0]</td>
      <td>3.43</td>
      <td>106.0</td>
      <td>True</td>
      <td>d1c03d2114451d562416b9efe4281d319ceff99e</td>
      <td>stabilityai/stablelm-tuned-alpha-3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Devio/test-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Devio__test-3b</td>
      <td>28.25</td>
      <td>27.65</td>
      <td>44.79</td>
      <td>23.53</td>
      <td>41.42</td>
      <td>55.49</td>
      <td>0.30</td>
      <td>4.61</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>3.50</td>
      <td>0.0</td>
      <td>True</td>
      <td>b81c038ee2fa2addd285acde08b1a7ca3cb2854d</td>
      <td>Devio/test-3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Kunhao/pile-7b-250b-tokens</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Kunhao__pile-7b-250b-tokens</td>
      <td>28.23</td>
      <td>29.27</td>
      <td>46.29</td>
      <td>25.25</td>
      <td>40.49</td>
      <td>52.80</td>
      <td>0.53</td>
      <td>2.98</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>5.87</td>
      <td>0.0</td>
      <td>True</td>
      <td>caefdf7a7c177905b0b16fbe9d4c7ba08def97c2</td>
      <td>Kunhao/pile-7b-250b-tokens</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/medalpaca-13B-GPTQ-4bit</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__medalpaca-13B-GPTQ-4bit</td>
      <td>28.07</td>
      <td>29.35</td>
      <td>26.32</td>
      <td>25.44</td>
      <td>49.51</td>
      <td>53.12</td>
      <td>0.00</td>
      <td>12.75</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>16.22</td>
      <td>28.0</td>
      <td>True</td>
      <td>12190f743a19e91dfe1f5c77abc0c1bf486073dd</td>
      <td>TheBloke/medalpaca-13B-GPTQ-4bit</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikivis/gpt2-large-lora-stf4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikivis__gpt2-large-lora-stf4</td>
      <td>28.07</td>
      <td>26.88</td>
      <td>42.17</td>
      <td>25.53</td>
      <td>40.84</td>
      <td>53.59</td>
      <td>0.00</td>
      <td>7.44</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>True</td>
      <td>82eff3a62116fd589ad7319c9d75ff6b12f42f72</td>
      <td>Mikivis/gpt2-large-lora-stf4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikivis/gpt2-large-lora-sft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikivis__gpt2-large-lora-sft</td>
      <td>28.05</td>
      <td>26.79</td>
      <td>44.15</td>
      <td>25.82</td>
      <td>39.06</td>
      <td>55.09</td>
      <td>0.00</td>
      <td>5.46</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.77</td>
      <td>1.0</td>
      <td>True</td>
      <td>1c0c5a686f3c83692e033416197155557e4d3a0d</td>
      <td>Mikivis/gpt2-large-lora-sft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikivis/gpt2-large-lora-sft2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikivis__gpt2-large-lora-sft2</td>
      <td>28.04</td>
      <td>26.62</td>
      <td>42.68</td>
      <td>24.72</td>
      <td>40.31</td>
      <td>53.67</td>
      <td>0.00</td>
      <td>8.31</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>True</td>
      <td>1244efb5d20765beb54f6b4a4e1426cf6d5daf44</td>
      <td>Mikivis/gpt2-large-lora-sft2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/fairseq-dense-355M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__fairseq-dense-355M</td>
      <td>27.99</td>
      <td>25.43</td>
      <td>46.67</td>
      <td>25.30</td>
      <td>39.19</td>
      <td>52.88</td>
      <td>0.00</td>
      <td>6.48</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.40</td>
      <td>6.0</td>
      <td>True</td>
      <td>24da1ea670f0638c2df911596e95c764bcd5fb44</td>
      <td>KoboldAI/fairseq-dense-355M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v1-774m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v1-774m</td>
      <td>27.95</td>
      <td>28.07</td>
      <td>44.35</td>
      <td>25.91</td>
      <td>36.11</td>
      <td>54.62</td>
      <td>0.00</td>
      <td>6.62</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>True</td>
      <td>d3f5401d07965fb13c2cb8b458ffaed9a5a79c2d</td>
      <td>aisquared/dlite-v1-774m</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/stabilityai/stablelm-base-alpha-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_stabilityai__stablelm-base-alpha-3b</td>
      <td>27.87</td>
      <td>26.45</td>
      <td>42.24</td>
      <td>25.43</td>
      <td>40.50</td>
      <td>53.91</td>
      <td>0.45</td>
      <td>6.14</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>[cc-by-sa-4.0]</td>
      <td>3.43</td>
      <td>82.0</td>
      <td>True</td>
      <td>99567ccfe45fabe467c71393aa6716106edb83c2</td>
      <td>stabilityai/stablelm-base-alpha-3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PY007__TinyLlama-1.1B-step-50K-105b</td>
      <td>27.87</td>
      <td>25.85</td>
      <td>44.10</td>
      <td>26.78</td>
      <td>39.51</td>
      <td>54.38</td>
      <td>0.53</td>
      <td>3.91</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.10</td>
      <td>102.0</td>
      <td>True</td>
      <td>c1f1ef67c12e4bb85fe0bdf1747c645a202cc118</td>
      <td>PY007/TinyLlama-1.1B-step-50K-105b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikivis/gpt2-large-lora-sft1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikivis__gpt2-large-lora-sft1</td>
      <td>27.78</td>
      <td>24.66</td>
      <td>42.67</td>
      <td>24.89</td>
      <td>39.37</td>
      <td>54.46</td>
      <td>0.00</td>
      <td>8.43</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>True</td>
      <td>8e26a8d2dc1661d87a8652c75f00b805d63e7330</td>
      <td>Mikivis/gpt2-large-lora-sft1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-410m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-410m</td>
      <td>27.68</td>
      <td>26.19</td>
      <td>40.85</td>
      <td>27.25</td>
      <td>41.22</td>
      <td>53.12</td>
      <td>0.68</td>
      <td>4.46</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.51</td>
      <td>8.0</td>
      <td>True</td>
      <td>9879c9b5f8bea9051dcb0e68dff21493d67e9d4f</td>
      <td>EleutherAI/pythia-410m</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_rinna__bilingual-gpt-neox-4b-8k</td>
      <td>27.67</td>
      <td>28.58</td>
      <td>43.94</td>
      <td>25.38</td>
      <td>47.48</td>
      <td>47.99</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>3.95</td>
      <td>22.0</td>
      <td>True</td>
      <td>ad56d7fc86db4ad5a7036bc9f80e11cd6f435a60</td>
      <td>rinna/bilingual-gpt-neox-4b-8k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/pygmalion-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__pygmalion-1.3b</td>
      <td>27.64</td>
      <td>28.07</td>
      <td>46.96</td>
      <td>24.12</td>
      <td>37.64</td>
      <td>50.04</td>
      <td>0.00</td>
      <td>6.65</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>agpl-3.0</td>
      <td>1.52</td>
      <td>51.0</td>
      <td>True</td>
      <td>bef2c90128c00ff6f16c0f397463423b7d988e17</td>
      <td>PygmalionAI/pygmalion-1.3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/rinna/bilingual-gpt-neox-4b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_rinna__bilingual-gpt-neox-4b</td>
      <td>27.58</td>
      <td>29.18</td>
      <td>43.73</td>
      <td>23.10</td>
      <td>45.00</td>
      <td>51.85</td>
      <td>0.00</td>
      <td>0.19</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>3.95</td>
      <td>21.0</td>
      <td>True</td>
      <td>f02f6f3c8da0093f3c1ce59220409bc2fa9fbb17</td>
      <td>rinna/bilingual-gpt-neox-4b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v2-355m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v2-355m</td>
      <td>27.53</td>
      <td>28.33</td>
      <td>40.54</td>
      <td>26.77</td>
      <td>38.76</td>
      <td>52.80</td>
      <td>0.00</td>
      <td>5.53</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.36</td>
      <td>7.0</td>
      <td>True</td>
      <td>f51d310aebc16a9fe0d999d2a437b5faff635716</td>
      <td>aisquared/dlite-v2-355m</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/nicholasKluge/Aira-2-774M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nicholasKluge__Aira-2-774M</td>
      <td>27.47</td>
      <td>28.75</td>
      <td>40.80</td>
      <td>25.10</td>
      <td>41.33</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>4.26</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.77</td>
      <td>2.0</td>
      <td>False</td>
      <td>f43044cfe7bf0827a176f0d319c63251c2b29373</td>
      <td>nicholasKluge/Aira-2-774M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/player1537/dolphinette</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_player1537__dolphinette</td>
      <td>27.43</td>
      <td>24.91</td>
      <td>37.33</td>
      <td>25.37</td>
      <td>42.08</td>
      <td>54.22</td>
      <td>0.00</td>
      <td>8.10</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>20529d47b0a82343014727edd1639a9a6a6b09e6</td>
      <td>player1537/dolphinette</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-410m-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-410m-deduped</td>
      <td>27.43</td>
      <td>24.83</td>
      <td>41.29</td>
      <td>25.99</td>
      <td>40.95</td>
      <td>54.38</td>
      <td>0.30</td>
      <td>4.26</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.51</td>
      <td>13.0</td>
      <td>True</td>
      <td>c4fc8d586d62df497f1f9b69d66d3ca419992d3e</td>
      <td>EleutherAI/pythia-410m-deduped</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/gpt-2-xl-EvolInstruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__gpt-2-xl-EvolInstruct</td>
      <td>27.41</td>
      <td>27.39</td>
      <td>38.46</td>
      <td>25.67</td>
      <td>42.76</td>
      <td>53.51</td>
      <td>0.15</td>
      <td>3.91</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>3e68735b9bfbca5c2e6a8e4367f003ab3d3c1512</td>
      <td>KnutJaegersberg/gpt-2-xl-EvolInstruct</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/FabbriSimo01/Cerebras_1.3b_Quantized</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FabbriSimo01__Cerebras_1.3b_Quantized</td>
      <td>27.36</td>
      <td>25.94</td>
      <td>38.56</td>
      <td>26.79</td>
      <td>42.67</td>
      <td>53.51</td>
      <td>0.38</td>
      <td>3.71</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>e2126a42a1c8a938553dd513e4adafec41cb793e</td>
      <td>FabbriSimo01/Cerebras_1.3b_Quantized</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-1.3B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-1.3B</td>
      <td>27.35</td>
      <td>26.28</td>
      <td>38.54</td>
      <td>26.59</td>
      <td>42.70</td>
      <td>53.43</td>
      <td>0.23</td>
      <td>3.70</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.32</td>
      <td>43.0</td>
      <td>True</td>
      <td>5b95400ee8d1e3cc9f79f0dec7182ed9c1009c34</td>
      <td>cerebras/Cerebras-GPT-1.3B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/winglian/basilisk-4b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_winglian__basilisk-4b</td>
      <td>27.26</td>
      <td>25.85</td>
      <td>39.60</td>
      <td>24.61</td>
      <td>43.74</td>
      <td>53.12</td>
      <td>0.00</td>
      <td>3.89</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>3.37</td>
      <td>3.0</td>
      <td>True</td>
      <td>b91c2e5389f4f0ce2d6042fdce5927343d8dcb06</td>
      <td>winglian/basilisk-4b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/Quokka_1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__Quokka_1.3b</td>
      <td>27.10</td>
      <td>27.73</td>
      <td>37.91</td>
      <td>26.66</td>
      <td>40.14</td>
      <td>52.72</td>
      <td>0.00</td>
      <td>4.54</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.42</td>
      <td>0.0</td>
      <td>True</td>
      <td>8a8d738e841a524d658897d89b9e39e7b9272ed8</td>
      <td>Corianas/Quokka_1.3b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/SummerSigh/GPTNeo350M-Instruct-SFT</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_SummerSigh__GPTNeo350M-Instruct-SFT</td>
      <td>27.03</td>
      <td>25.94</td>
      <td>38.55</td>
      <td>25.76</td>
      <td>45.25</td>
      <td>50.20</td>
      <td>0.30</td>
      <td>3.24</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.46</td>
      <td>0.0</td>
      <td>True</td>
      <td>5e41660ced3edf13c47e933112efd280b710b977</td>
      <td>SummerSigh/GPTNeo350M-Instruct-SFT</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Corianas/1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__1.3b</td>
      <td>27.02</td>
      <td>27.30</td>
      <td>38.30</td>
      <td>26.77</td>
      <td>39.02</td>
      <td>53.04</td>
      <td>0.15</td>
      <td>4.57</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.42</td>
      <td>2.0</td>
      <td>True</td>
      <td>9831f95df82155ef95ff46a505506bf6194b131a</td>
      <td>Corianas/1.3b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TehVenom/DiffMerge-DollyGPT-Pygmalion</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TehVenom__DiffMerge-DollyGPT-Pygmalion</td>
      <td>27.01</td>
      <td>23.63</td>
      <td>34.38</td>
      <td>24.41</td>
      <td>46.48</td>
      <td>53.83</td>
      <td>0.00</td>
      <td>6.33</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>5.84</td>
      <td>2.0</td>
      <td>True</td>
      <td>6a00b371146d4bd2903890814485ee1b775162e7</td>
      <td>TehVenom/DiffMerge-DollyGPT-Pygmalion</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/nicholasKluge/Aira-2-355M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nicholasKluge__Aira-2-355M</td>
      <td>27.00</td>
      <td>27.56</td>
      <td>38.92</td>
      <td>27.26</td>
      <td>38.53</td>
      <td>53.75</td>
      <td>0.00</td>
      <td>2.99</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.36</td>
      <td>0.0</td>
      <td>False</td>
      <td>2479f5b1bb62251ec88e60182ba81390a4c19cf9</td>
      <td>nicholasKluge/Aira-2-355M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v1-355m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v1-355m</td>
      <td>26.94</td>
      <td>27.13</td>
      <td>39.07</td>
      <td>27.12</td>
      <td>37.13</td>
      <td>52.80</td>
      <td>0.00</td>
      <td>5.34</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.36</td>
      <td>1.0</td>
      <td>True</td>
      <td>c5f4b5a61e6a66a5c7613164d99a70db5bf7e9a2</td>
      <td>aisquared/dlite-v1-355m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/lamini-cerebras-1.3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__lamini-cerebras-1.3b</td>
      <td>26.91</td>
      <td>26.88</td>
      <td>37.96</td>
      <td>28.43</td>
      <td>36.45</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>8.07</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>1.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>502e70081df53edc8a9156acf5a26a11a9dad8fb</td>
      <td>MBZUAI/lamini-cerebras-1.3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-4-430m-pile</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-4-430m-pile</td>
      <td>26.89</td>
      <td>26.71</td>
      <td>40.01</td>
      <td>24.85</td>
      <td>39.58</td>
      <td>51.14</td>
      <td>0.38</td>
      <td>5.54</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.38</td>
      <td>3.0</td>
      <td>True</td>
      <td>a4f6ec80438d4262d1bbc8f385feb2ef1a4a9d6b</td>
      <td>RWKV/rwkv-4-430m-pile</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/robowaifudev/megatron-gpt2-345m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_robowaifudev__megatron-gpt2-345m</td>
      <td>26.78</td>
      <td>24.23</td>
      <td>39.18</td>
      <td>24.32</td>
      <td>41.51</td>
      <td>52.96</td>
      <td>0.23</td>
      <td>5.04</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.38</td>
      <td>4.0</td>
      <td>True</td>
      <td>b39f8d00fb9f33da4271be2035da848da896a23b</td>
      <td>robowaifudev/megatron-gpt2-345m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/postbot/emailgen-pythia-410m-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_postbot__emailgen-pythia-410m-deduped</td>
      <td>26.65</td>
      <td>27.90</td>
      <td>40.04</td>
      <td>27.35</td>
      <td>38.20</td>
      <td>52.09</td>
      <td>0.00</td>
      <td>0.99</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.51</td>
      <td>0.0</td>
      <td>True</td>
      <td>e0208b02990c49138350da791f0b6fcb8a65e738</td>
      <td>postbot/emailgen-pythia-410m-deduped</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/ahxt/llama2_xs_460M_experimental</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ahxt__llama2_xs_460M_experimental</td>
      <td>26.65</td>
      <td>24.91</td>
      <td>38.47</td>
      <td>26.17</td>
      <td>41.59</td>
      <td>49.88</td>
      <td>0.00</td>
      <td>5.51</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.41</td>
      <td>4.0</td>
      <td>True</td>
      <td>c8db281477559f5c969a9be794ce236f8a99e1a0</td>
      <td>ahxt/llama2_xs_460M_experimental</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/bloom-560m-RLHF-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__bloom-560m-RLHF-v2</td>
      <td>26.63</td>
      <td>26.45</td>
      <td>37.67</td>
      <td>23.95</td>
      <td>43.51</td>
      <td>50.91</td>
      <td>0.08</td>
      <td>3.88</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.56</td>
      <td>3.0</td>
      <td>True</td>
      <td>7128cbfcdaf67f1eff27e45d875c35e7b47618db</td>
      <td>TheTravellingEngineer/bloom-560m-RLHF-v2</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/megatron-gpt2-345m-evol_instruct_v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__megatron-gpt2-345m-evol_instruct_v2</td>
      <td>26.63</td>
      <td>26.37</td>
      <td>38.39</td>
      <td>23.60</td>
      <td>41.19</td>
      <td>52.33</td>
      <td>0.00</td>
      <td>4.54</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.36</td>
      <td>1.0</td>
      <td>True</td>
      <td>2866eeaaf62014a7a6e939d18b6e27f44df48428</td>
      <td>KnutJaegersberg/megatron-gpt2-345m-evol_instruct_v2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/xhyi/PT_GPTNEO350_ATG</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_xhyi__PT_GPTNEO350_ATG</td>
      <td>26.63</td>
      <td>25.43</td>
      <td>37.59</td>
      <td>24.79</td>
      <td>43.05</td>
      <td>51.46</td>
      <td>0.45</td>
      <td>3.64</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.36</td>
      <td>18.0</td>
      <td>True</td>
      <td>56ab08aaa6802d0f830d42c352d5d536be72811d</td>
      <td>xhyi/PT_GPTNEO350_ATG</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098</td>
      <td>26.60</td>
      <td>26.02</td>
      <td>40.39</td>
      <td>24.45</td>
      <td>37.57</td>
      <td>52.41</td>
      <td>0.23</td>
      <td>5.10</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.38</td>
      <td>0.0</td>
      <td>True</td>
      <td>e31777c9d3b8c5c9f803b23f49550c009cbdcf6d</td>
      <td>KnutJaegersberg/RWKV-4-PilePlus-430M-20230520-6162-1018Gtokens-ctx4098</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/WangZeJun/bloom-820m-chat</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WangZeJun__bloom-820m-chat</td>
      <td>26.55</td>
      <td>23.38</td>
      <td>34.16</td>
      <td>25.98</td>
      <td>40.32</td>
      <td>53.20</td>
      <td>0.00</td>
      <td>8.85</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.75</td>
      <td>2.0</td>
      <td>True</td>
      <td>f98b1f9c1bd358dd837d05d443d992c495497606</td>
      <td>WangZeJun/bloom-820m-chat</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/health360/Healix-410M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_health360__Healix-410M</td>
      <td>26.50</td>
      <td>25.09</td>
      <td>32.02</td>
      <td>24.94</td>
      <td>44.42</td>
      <td>54.14</td>
      <td>0.00</td>
      <td>4.91</td>
      <td>?</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>0.35</td>
      <td>0.0</td>
      <td>False</td>
      <td>df5a3cec54a0bdd22e1644bfe576c7b58eca6bfd</td>
      <td>health360/Healix-410M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-350M-Erebus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-350M-Erebus</td>
      <td>26.42</td>
      <td>23.81</td>
      <td>34.35</td>
      <td>26.23</td>
      <td>43.58</td>
      <td>52.57</td>
      <td>0.30</td>
      <td>4.13</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.33</td>
      <td>11.0</td>
      <td>True</td>
      <td>83ce2f4e78d308968cf7ecd03d86a1f64aea8336</td>
      <td>KoboldAI/OPT-350M-Erebus</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/bigscience/bloom-560m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigscience__bloom-560m</td>
      <td>26.40</td>
      <td>24.74</td>
      <td>37.15</td>
      <td>24.22</td>
      <td>42.44</td>
      <td>51.93</td>
      <td>0.23</td>
      <td>4.11</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>263.0</td>
      <td>True</td>
      <td>4f42c91d806a19ae1a46af6c3fb5f4990d884cd6</td>
      <td>bigscience/bloom-560m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/megatron-GPT-2-345m-EvolInstruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__megatron-GPT-2-345m-EvolInstruct</td>
      <td>26.35</td>
      <td>24.06</td>
      <td>35.12</td>
      <td>24.48</td>
      <td>41.25</td>
      <td>54.78</td>
      <td>0.38</td>
      <td>4.39</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.38</td>
      <td>4.0</td>
      <td>True</td>
      <td>dc95fda9f1e51d94870e28751e35410c66563d18</td>
      <td>KnutJaegersberg/megatron-GPT-2-345m-EvolInstruct</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-350m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-350m</td>
      <td>26.32</td>
      <td>23.55</td>
      <td>36.73</td>
      <td>26.02</td>
      <td>40.83</td>
      <td>52.64</td>
      <td>0.30</td>
      <td>4.16</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.33</td>
      <td>75.0</td>
      <td>True</td>
      <td>cb32f77e905cccbca1d970436fb0f5e6b58ee3c5</td>
      <td>facebook/opt-350m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/golaxy/gogpt-560m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_golaxy__gogpt-560m</td>
      <td>26.30</td>
      <td>26.37</td>
      <td>31.86</td>
      <td>25.29</td>
      <td>43.12</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>6.70</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.56</td>
      <td>0.0</td>
      <td>True</td>
      <td>82bd8b88b95068eee614a35b790388c5d2415705</td>
      <td>golaxy/gogpt-560m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/PygmalionAI/pygmalion-350m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_PygmalionAI__pygmalion-350m</td>
      <td>26.23</td>
      <td>25.00</td>
      <td>37.80</td>
      <td>25.68</td>
      <td>40.41</td>
      <td>50.28</td>
      <td>0.53</td>
      <td>3.89</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.33</td>
      <td>48.0</td>
      <td>True</td>
      <td>d65832d913f6b396e2ffb64c373d9383c9da9303</td>
      <td>PygmalionAI/pygmalion-350m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/OPT-350M-Nerys-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__OPT-350M-Nerys-v2</td>
      <td>26.23</td>
      <td>23.63</td>
      <td>35.49</td>
      <td>25.91</td>
      <td>42.08</td>
      <td>51.62</td>
      <td>0.68</td>
      <td>4.16</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.33</td>
      <td>4.0</td>
      <td>True</td>
      <td>59b1019c35ab17a7d77ea1ad32b45a8375ba6e89</td>
      <td>KoboldAI/OPT-350M-Nerys-v2</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/xglm-564M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__xglm-564M</td>
      <td>26.19</td>
      <td>24.57</td>
      <td>34.64</td>
      <td>25.18</td>
      <td>40.43</td>
      <td>52.25</td>
      <td>0.23</td>
      <td>6.04</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.56</td>
      <td>28.0</td>
      <td>True</td>
      <td>f3059f01b98ccc877c673149e0178c0e957660f9</td>
      <td>facebook/xglm-564M</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/bloom-560m-RLHF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__bloom-560m-RLHF</td>
      <td>26.17</td>
      <td>24.40</td>
      <td>36.96</td>
      <td>23.63</td>
      <td>40.76</td>
      <td>53.12</td>
      <td>0.30</td>
      <td>3.99</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.56</td>
      <td>1.0</td>
      <td>True</td>
      <td>b1769e92f325d8a28e7db1c21f133e6c85b84e78</td>
      <td>TheTravellingEngineer/bloom-560m-RLHF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/vikp/phi2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vikp__phi2</td>
      <td>26.11</td>
      <td>22.87</td>
      <td>30.70</td>
      <td>27.55</td>
      <td>46.10</td>
      <td>52.01</td>
      <td>0.68</td>
      <td>2.84</td>
      <td>FlashGPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>9fd01ce09da870fc66af88616d43e53db642ef46</td>
      <td>vikp/phi2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/huggingtweets/bladeecity-jerma985</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huggingtweets__bladeecity-jerma985</td>
      <td>26.07</td>
      <td>22.87</td>
      <td>30.53</td>
      <td>26.56</td>
      <td>44.99</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>5.53</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>9bf3a0db7f6bc960c51f2c0dc6fb66ed982b0180</td>
      <td>huggingtweets/bladeecity-jerma985</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/beomi/KoAlpaca-Polyglot-5.8B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beomi__KoAlpaca-Polyglot-5.8B</td>
      <td>26.03</td>
      <td>27.65</td>
      <td>35.58</td>
      <td>24.72</td>
      <td>39.74</td>
      <td>49.01</td>
      <td>0.08</td>
      <td>5.41</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.00</td>
      <td>48.0</td>
      <td>True</td>
      <td>1051dacf82ca9fba0ba4a4ff67f1d98a81ef7a2e</td>
      <td>beomi/KoAlpaca-Polyglot-5.8B</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-orca-airoboros-13b-0.10e</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-orca-airoboros-13b-0.10e</td>
      <td>26.02</td>
      <td>29.44</td>
      <td>25.71</td>
      <td>25.43</td>
      <td>49.64</td>
      <td>51.93</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb</td>
      <td>uukuguy/speechless-codellama-orca-airoboros-13b-0.10e</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/lamini-neo-125m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__lamini-neo-125m</td>
      <td>26.02</td>
      <td>24.57</td>
      <td>30.22</td>
      <td>26.74</td>
      <td>42.85</td>
      <td>52.25</td>
      <td>0.00</td>
      <td>5.48</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.12</td>
      <td>13.0</td>
      <td>True</td>
      <td>f01e73ba67da96f6645be3067158cc493b0cbbcb</td>
      <td>MBZUAI/lamini-neo-125m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KoboldAI/fairseq-dense-125M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KoboldAI__fairseq-dense-125M</td>
      <td>26.00</td>
      <td>24.06</td>
      <td>34.14</td>
      <td>23.98</td>
      <td>43.72</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>5.50</td>
      <td>XGLMForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>c8fb975220512b34e7b4a9fc570ca333ddcaf9b5</td>
      <td>KoboldAI/fairseq-dense-125M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hoskinson-center/proofGPT-v0.1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hoskinson-center__proofGPT-v0.1</td>
      <td>25.99</td>
      <td>22.87</td>
      <td>28.66</td>
      <td>25.96</td>
      <td>51.64</td>
      <td>50.43</td>
      <td>0.08</td>
      <td>2.29</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.31</td>
      <td>3.0</td>
      <td>True</td>
      <td>1e4dd330ca90c0ef6d77ca71bd49cbe3d71f26b8</td>
      <td>hoskinson-center/proofGPT-v0.1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ai-forever__rugpt3large_based_on_gpt2</td>
      <td>25.98</td>
      <td>22.61</td>
      <td>32.84</td>
      <td>24.90</td>
      <td>43.39</td>
      <td>53.12</td>
      <td>0.30</td>
      <td>4.72</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.76</td>
      <td>60.0</td>
      <td>True</td>
      <td>8201db0de8deb68f25e7309db04d163b71970494</td>
      <td>ai-forever/rugpt3large_based_on_gpt2</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TurkuNLP/gpt3-finnish-large</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TurkuNLP__gpt3-finnish-large</td>
      <td>25.95</td>
      <td>21.76</td>
      <td>32.88</td>
      <td>24.11</td>
      <td>44.35</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>7.03</td>
      <td>BloomModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.88</td>
      <td>3.0</td>
      <td>True</td>
      <td>b9a3dd97387fc70d07010d469888a918842d3449</td>
      <td>TurkuNLP/gpt3-finnish-large</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/porkorbeef/Llama-2-13b-sf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_porkorbeef__Llama-2-13b-sf</td>
      <td>25.90</td>
      <td>29.52</td>
      <td>26.49</td>
      <td>25.98</td>
      <td>48.97</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>06253ee259e6b205c4734ab6ec3fa850737b2110</td>
      <td>porkorbeef/Llama-2-13b-sf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-orca-airoboros-13b-0.10e</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-orca-airoboros-13b-0.10e</td>
      <td>25.90</td>
      <td>29.27</td>
      <td>25.74</td>
      <td>25.69</td>
      <td>49.61</td>
      <td>50.99</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>dbd1d1f7ad7b6b359f8246141650b25ca0bb8cbb</td>
      <td>uukuguy/speechless-codellama-orca-airoboros-13b-0.10e</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/bloom-1b1-RLHF</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__bloom-1b1-RLHF</td>
      <td>25.83</td>
      <td>27.99</td>
      <td>26.19</td>
      <td>26.86</td>
      <td>48.88</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>65bd72580520a1d4a0c19fcb23f68c1f28464e1b</td>
      <td>TheTravellingEngineer/bloom-1b1-RLHF</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/hoskinson-center/proofGPT-v0.1-6.7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_hoskinson-center__proofGPT-v0.1-6.7B</td>
      <td>25.82</td>
      <td>23.29</td>
      <td>28.45</td>
      <td>24.57</td>
      <td>50.87</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>2.43</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>6.65</td>
      <td>9.0</td>
      <td>True</td>
      <td>02f405f08ca0e5b1aaa90a7c3b11303b5f245102</td>
      <td>hoskinson-center/proofGPT-v0.1-6.7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/yhyhy3/med-orca-instruct-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yhyhy3__med-orca-instruct-33b</td>
      <td>25.82</td>
      <td>28.84</td>
      <td>25.63</td>
      <td>26.50</td>
      <td>49.26</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaModel</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d636881854338e571825226c712180da06be72c</td>
      <td>yhyhy3/med-orca-instruct-33b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/porkorbeef/Llama-2-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_porkorbeef__Llama-2-13b</td>
      <td>25.81</td>
      <td>29.35</td>
      <td>26.35</td>
      <td>24.94</td>
      <td>48.32</td>
      <td>51.70</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>06253ee259e6b205c4734ab6ec3fa850737b2110</td>
      <td>porkorbeef/Llama-2-13b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/gpt-neo-125m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-neo-125m</td>
      <td>25.79</td>
      <td>22.95</td>
      <td>30.26</td>
      <td>25.97</td>
      <td>45.58</td>
      <td>51.78</td>
      <td>0.30</td>
      <td>3.69</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.15</td>
      <td>132.0</td>
      <td>True</td>
      <td>6cb0d322a3a484e99667e7cb240e22f1ac036b99</td>
      <td>EleutherAI/gpt-neo-125m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/marcchew/Marcoroni-7B-LaMini-80K</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_marcchew__Marcoroni-7B-LaMini-80K</td>
      <td>25.79</td>
      <td>28.75</td>
      <td>26.13</td>
      <td>24.46</td>
      <td>49.71</td>
      <td>51.46</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>ea7a283403ec1a40570bfc25f2c4b8fcb089b6bb</td>
      <td>marcchew/Marcoroni-7B-LaMini-80K</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/doas/test5</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_doas__test5</td>
      <td>25.77</td>
      <td>28.41</td>
      <td>26.63</td>
      <td>25.36</td>
      <td>47.34</td>
      <td>52.64</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>b0dae937b7137790d8946794375e1affd51c760a</td>
      <td>doas/test5</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/yeen214/test_llama2_ko_7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeen214__test_llama2_ko_7b</td>
      <td>25.70</td>
      <td>29.95</td>
      <td>26.94</td>
      <td>25.62</td>
      <td>49.03</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>45901e1d6ccb22f5ed8aec3f9dd366823fdd1c33</td>
      <td>yeen214/test_llama2_ko_7b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/winglian/Llama-2-3b-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_winglian__Llama-2-3b-hf</td>
      <td>25.69</td>
      <td>26.96</td>
      <td>26.52</td>
      <td>23.33</td>
      <td>50.71</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>2.63</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>3.37</td>
      <td>2.0</td>
      <td>True</td>
      <td>293f071b223efd7959f9e1fac66285369aaa959d</td>
      <td>winglian/Llama-2-3b-hf</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-orca-platypus-13b-0.10e</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-orca-platypus-13b-0.10e</td>
      <td>25.68</td>
      <td>28.92</td>
      <td>25.76</td>
      <td>25.28</td>
      <td>49.22</td>
      <td>50.59</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>119abfc73f9ce541a40779f167fe21e95faed4e8</td>
      <td>uukuguy/speechless-codellama-orca-platypus-13b-0.10e</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_IDEA-CCNL__Ziya-LLaMA-13B-Pretrain-v1</td>
      <td>25.68</td>
      <td>27.99</td>
      <td>26.00</td>
      <td>27.04</td>
      <td>48.59</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>12.89</td>
      <td>20.0</td>
      <td>True</td>
      <td>826e83e411df32f358893ab21f5eae680499ae9a</td>
      <td>IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-160m-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-160m-deduped</td>
      <td>25.66</td>
      <td>24.06</td>
      <td>31.39</td>
      <td>24.86</td>
      <td>44.34</td>
      <td>51.38</td>
      <td>0.23</td>
      <td>3.38</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.21</td>
      <td>0.0</td>
      <td>True</td>
      <td>582159a2dfe3e712a8d47ae83dec95ae3bde8e7e</td>
      <td>EleutherAI/pythia-160m-deduped</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-256M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-256M</td>
      <td>25.65</td>
      <td>22.01</td>
      <td>28.99</td>
      <td>26.83</td>
      <td>45.98</td>
      <td>52.49</td>
      <td>0.00</td>
      <td>3.26</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.26</td>
      <td>20.0</td>
      <td>True</td>
      <td>d77812ac95aece1f1edef6745ae2a1b325ad01a4</td>
      <td>cerebras/Cerebras-GPT-256M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/FINDA-FIT/llama-r</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FINDA-FIT__llama-r</td>
      <td>25.64</td>
      <td>21.59</td>
      <td>30.18</td>
      <td>26.13</td>
      <td>45.38</td>
      <td>52.17</td>
      <td>0.61</td>
      <td>3.46</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.69</td>
      <td>0.0</td>
      <td>True</td>
      <td>6bdde9a227da60c2db803024d5b2e3a53a41cf0b</td>
      <td>FINDA-FIT/llama-r</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/gpt-YA-1-1_70M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__gpt-YA-1-1_70M</td>
      <td>25.64</td>
      <td>22.53</td>
      <td>27.37</td>
      <td>25.38</td>
      <td>47.09</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>6.18</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.04</td>
      <td>0.0</td>
      <td>True</td>
      <td>218e8da522cf6fb5566314f37624f27412ae2259</td>
      <td>BreadAi/gpt-YA-1-1_70M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/marcchew/LaMini-40k-Platypus2-7B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_marcchew__LaMini-40k-Platypus2-7B</td>
      <td>25.63</td>
      <td>28.50</td>
      <td>26.32</td>
      <td>27.04</td>
      <td>47.39</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>e8c03e43eab479a216b5f4f182a711c3624f38bd</td>
      <td>marcchew/LaMini-40k-Platypus2-7B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/klosax/pythia-160m-deduped-step92k-193bt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_klosax__pythia-160m-deduped-step92k-193bt</td>
      <td>25.62</td>
      <td>24.23</td>
      <td>32.33</td>
      <td>24.54</td>
      <td>43.49</td>
      <td>50.83</td>
      <td>0.38</td>
      <td>3.55</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>9eac24dad1bd7194e38ce8083a0197cee456456c</td>
      <td>klosax/pythia-160m-deduped-step92k-193bt</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Corianas/590m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__590m</td>
      <td>25.62</td>
      <td>24.15</td>
      <td>31.91</td>
      <td>26.61</td>
      <td>42.19</td>
      <td>48.38</td>
      <td>0.08</td>
      <td>6.03</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>ec721c97ef0e6ebfc578ab98b3ff6e2bd19b3e27</td>
      <td>Corianas/590m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/doas/test2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_doas__test2</td>
      <td>25.60</td>
      <td>29.61</td>
      <td>26.65</td>
      <td>24.34</td>
      <td>48.49</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f08d224deae510ebf1408ce38bc2610b1e4c77eb</td>
      <td>doas/test2</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/Deci/DeciCoder-1b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Deci__DeciCoder-1b</td>
      <td>25.60</td>
      <td>21.16</td>
      <td>31.09</td>
      <td>24.34</td>
      <td>47.05</td>
      <td>50.83</td>
      <td>1.74</td>
      <td>2.98</td>
      <td>DeciLlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.11</td>
      <td>221.0</td>
      <td>True</td>
      <td>af2ef45ef8cbe82eb7eb4074f260412bc14c7b11</td>
      <td>Deci/DeciCoder-1b</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__WizardLM-7B-uncensored-GPTQ</td>
      <td>25.59</td>
      <td>28.50</td>
      <td>25.37</td>
      <td>24.85</td>
      <td>50.86</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>9.04</td>
      <td>150.0</td>
      <td>True</td>
      <td>cc30c031fd795ee3d3a50312ab4549415bfbdb46</td>
      <td>TheBloke/WizardLM-7B-uncensored-GPTQ</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Panchovix__WizardLM-33B-V1.0-Uncensored-SuperHOT-8k</td>
      <td>25.58</td>
      <td>25.43</td>
      <td>31.97</td>
      <td>23.43</td>
      <td>47.00</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>0.19</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>7.0</td>
      <td>True</td>
      <td>b6d0002b10d43ab48aa14e365d9e7b40655ec160</td>
      <td>Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/PM_modelV2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__PM_modelV2</td>
      <td>25.58</td>
      <td>25.09</td>
      <td>26.45</td>
      <td>26.14</td>
      <td>51.36</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.45</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>4edde209eea33af491206f8651c0c47e70e08289</td>
      <td>BreadAi/PM_modelV2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ogimgio/gpt-neo-125m-neurallinguisticpioneers</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ogimgio__gpt-neo-125m-neurallinguisticpioneers</td>
      <td>25.57</td>
      <td>22.44</td>
      <td>30.36</td>
      <td>25.14</td>
      <td>45.64</td>
      <td>51.22</td>
      <td>0.08</td>
      <td>4.12</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>515fd7753c5fecbf4a2951f7cebb2846d91324b3</td>
      <td>ogimgio/gpt-neo-125m-neurallinguisticpioneers</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/uukuguy/speechless-codellama-orca-platypus-13b-0.10e</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_uukuguy__speechless-codellama-orca-platypus-13b-0.10e</td>
      <td>25.57</td>
      <td>28.75</td>
      <td>25.88</td>
      <td>25.36</td>
      <td>49.27</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>119abfc73f9ce541a40779f167fe21e95faed4e8</td>
      <td>uukuguy/speechless-codellama-orca-platypus-13b-0.10e</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_IDEA-CCNL__Ziya-LLaMA-13B-v1</td>
      <td>25.56</td>
      <td>27.73</td>
      <td>25.96</td>
      <td>27.04</td>
      <td>48.65</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>gpl-3.0</td>
      <td>12.89</td>
      <td>251.0</td>
      <td>True</td>
      <td>fccf34387d2c9f2f95ff59ae380e6de3718e41ff</td>
      <td>IDEA-CCNL/Ziya-LLaMA-13B-v1</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/codeparrot/codeparrot</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_codeparrot__codeparrot</td>
      <td>25.56</td>
      <td>21.67</td>
      <td>28.34</td>
      <td>25.55</td>
      <td>50.87</td>
      <td>50.20</td>
      <td>0.23</td>
      <td>2.04</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.53</td>
      <td>87.0</td>
      <td>True</td>
      <td>065248a99f051da363b1c2cbf05da943c8b6211b</td>
      <td>codeparrot/codeparrot</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-590M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-590M</td>
      <td>25.55</td>
      <td>23.72</td>
      <td>32.40</td>
      <td>25.97</td>
      <td>44.15</td>
      <td>48.15</td>
      <td>0.45</td>
      <td>3.99</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.59</td>
      <td>17.0</td>
      <td>True</td>
      <td>67a653304fd782a34906d59f3795a37f9e053397</td>
      <td>cerebras/Cerebras-GPT-590M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MayaPH/FinOPT-Franklin</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MayaPH__FinOPT-Franklin</td>
      <td>25.54</td>
      <td>27.73</td>
      <td>24.91</td>
      <td>23.12</td>
      <td>52.40</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>0.10</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>1.32</td>
      <td>4.0</td>
      <td>True</td>
      <td>1b13331834190bfe49a176f1661ba4d8309a5051</td>
      <td>MayaPH/FinOPT-Franklin</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/marcchew/Marcoroni-7B-LaMini-40K</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_marcchew__Marcoroni-7B-LaMini-40K</td>
      <td>25.53</td>
      <td>27.65</td>
      <td>26.23</td>
      <td>26.92</td>
      <td>47.40</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>27868e4faed5d68d059c8c57dbd3e24e4933ca28</td>
      <td>marcchew/Marcoroni-7B-LaMini-40K</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2-dolly</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2-dolly</td>
      <td>25.53</td>
      <td>22.70</td>
      <td>30.15</td>
      <td>25.81</td>
      <td>44.97</td>
      <td>51.46</td>
      <td>0.15</td>
      <td>3.45</td>
      <td>GPT2LMHeadModel</td>
      <td>4bit</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>7e75e6f4626437305e4d3e7b2aa36f617c517247</td>
      <td>lgaalves/gpt2-dolly</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/facebook/opt-125m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_facebook__opt-125m</td>
      <td>25.47</td>
      <td>22.87</td>
      <td>31.47</td>
      <td>26.02</td>
      <td>42.87</td>
      <td>51.62</td>
      <td>0.08</td>
      <td>3.36</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.12</td>
      <td>81.0</td>
      <td>True</td>
      <td>3d2b5f275bdf882b8775f902e1bfdb790e2cfc32</td>
      <td>facebook/opt-125m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/danielpark/gorani-100k-llama2-13b-instruct</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_danielpark__gorani-100k-llama2-13b-instruct</td>
      <td>25.45</td>
      <td>28.07</td>
      <td>26.30</td>
      <td>25.17</td>
      <td>48.96</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>f7d38ee654e505ad7a454f192d5e3d85cb60b3b8</td>
      <td>danielpark/gorani-100k-llama2-13b-instruct</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TFLai/gpt2-turkish-uncased</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TFLai__gpt2-turkish-uncased</td>
      <td>25.45</td>
      <td>24.49</td>
      <td>25.08</td>
      <td>26.59</td>
      <td>52.30</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>0.04</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.14</td>
      <td>1.0</td>
      <td>True</td>
      <td>4807e7df1dfb9d60c6d98e3cfeff62cb6b9a1579</td>
      <td>TFLai/gpt2-turkish-uncased</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/nthngdy/pythia-owt2-70m-100k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nthngdy__pythia-owt2-70m-100k</td>
      <td>25.45</td>
      <td>20.90</td>
      <td>28.34</td>
      <td>25.02</td>
      <td>45.12</td>
      <td>53.28</td>
      <td>0.00</td>
      <td>5.47</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>False</td>
      <td>b288893319b6cdce499148f4482043c350116560</td>
      <td>nthngdy/pythia-owt2-70m-100k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/porkorbeef/Llama-2-13b-12_153950</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_porkorbeef__Llama-2-13b-12_153950</td>
      <td>25.44</td>
      <td>28.58</td>
      <td>26.58</td>
      <td>20.79</td>
      <td>49.03</td>
      <td>53.12</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>ee9b0cf26f521b5cb2322d743880e8b6bfadb0b7</td>
      <td>porkorbeef/Llama-2-13b-12_153950</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/bigcode/tiny_starcoder_py</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigcode__tiny_starcoder_py</td>
      <td>25.43</td>
      <td>20.99</td>
      <td>28.77</td>
      <td>26.79</td>
      <td>47.68</td>
      <td>51.22</td>
      <td>0.99</td>
      <td>1.57</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>bigcode-openrail-m</td>
      <td>0.16</td>
      <td>57.0</td>
      <td>True</td>
      <td>8547527bef0bc927268c1653cce6948c5c242dd1</td>
      <td>bigcode/tiny_starcoder_py</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/marcchew/Platypus-2-7B-LaMini-14K</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_marcchew__Platypus-2-7B-LaMini-14K</td>
      <td>25.41</td>
      <td>29.52</td>
      <td>26.15</td>
      <td>23.13</td>
      <td>48.29</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>50199ba51c4d002cc86cf3fb2ac921ec52bf4828</td>
      <td>marcchew/Platypus-2-7B-LaMini-14K</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/MBZUAI/lamini-cerebras-590m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__lamini-cerebras-590m</td>
      <td>25.39</td>
      <td>24.32</td>
      <td>31.58</td>
      <td>25.57</td>
      <td>40.72</td>
      <td>47.91</td>
      <td>0.15</td>
      <td>7.45</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.59</td>
      <td>5.0</td>
      <td>True</td>
      <td>bab37eb7ba63f6ff9f0eb36a85727146b82ae5ed</td>
      <td>MBZUAI/lamini-cerebras-590m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_SebastianSchramm__Cerebras-GPT-111M-instruction</td>
      <td>25.37</td>
      <td>24.40</td>
      <td>26.05</td>
      <td>25.87</td>
      <td>49.46</td>
      <td>51.62</td>
      <td>0.00</td>
      <td>0.17</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.11</td>
      <td>1.0</td>
      <td>True</td>
      <td>09f1ec782ae2243fc605b24eb13ec8d5e4fd2734</td>
      <td>SebastianSchramm/Cerebras-GPT-111M-instruction</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-160m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-160m</td>
      <td>25.36</td>
      <td>22.78</td>
      <td>30.34</td>
      <td>24.95</td>
      <td>44.26</td>
      <td>51.54</td>
      <td>0.23</td>
      <td>3.45</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.21</td>
      <td>12.0</td>
      <td>True</td>
      <td>50f5173d932e8e61f858120bcb800b97af589f46</td>
      <td>EleutherAI/pythia-160m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/huggingtweets/jerma985</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huggingtweets__jerma985</td>
      <td>25.36</td>
      <td>21.67</td>
      <td>30.91</td>
      <td>26.57</td>
      <td>44.01</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>3.72</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>816206ad02a397161be78dcb70eeda67e0c53132</td>
      <td>huggingtweets/jerma985</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Tincando/fiction_story_generator</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Tincando__fiction_story_generator</td>
      <td>25.36</td>
      <td>23.29</td>
      <td>28.68</td>
      <td>26.72</td>
      <td>43.79</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>4.90</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>2.0</td>
      <td>True</td>
      <td>377b080cf96e10d50289aa3e1fd79c330265f45a</td>
      <td>Tincando/fiction_story_generator</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Panchovix/airoboros-33b-gpt4-1.2-SuperHOT-8k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Panchovix__airoboros-33b-gpt4-1.2-SuperHOT-8k</td>
      <td>25.35</td>
      <td>24.66</td>
      <td>31.23</td>
      <td>23.13</td>
      <td>47.44</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>0.59</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>32.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>47c14f699cbbc9bd24458edd86eb70d87552b623</td>
      <td>Panchovix/airoboros-33b-gpt4-1.2-SuperHOT-8k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openbmb/UltraRM-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openbmb__UltraRM-13b</td>
      <td>25.35</td>
      <td>28.16</td>
      <td>26.13</td>
      <td>25.96</td>
      <td>47.91</td>
      <td>49.33</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaRewardModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>12.85</td>
      <td>8.0</td>
      <td>True</td>
      <td>4b231ae58c15244e6e15f0d2f4e26ec37b846229</td>
      <td>openbmb/UltraRM-13b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/256_5epoch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__256_5epoch</td>
      <td>25.35</td>
      <td>22.27</td>
      <td>28.99</td>
      <td>26.62</td>
      <td>41.71</td>
      <td>52.72</td>
      <td>0.23</td>
      <td>4.93</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>0.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>b1fe75844a07832acd405a4d989a26f6ab7b1c00</td>
      <td>Corianas/256_5epoch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/vicgalle/alpaca-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vicgalle__alpaca-7b</td>
      <td>25.35</td>
      <td>28.07</td>
      <td>25.83</td>
      <td>25.31</td>
      <td>48.49</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>2.0</td>
      <td>True</td>
      <td>7f22882125208d1f54765c21abf84fd162aa454a</td>
      <td>vicgalle/alpaca-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HWERI/pythia-70m-deduped-cleansharegpt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HWERI__pythia-70m-deduped-cleansharegpt</td>
      <td>25.34</td>
      <td>25.68</td>
      <td>25.40</td>
      <td>23.12</td>
      <td>51.15</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ea42abd94cb0017918f6fe5e71d78bcb7c75548</td>
      <td>HWERI/pythia-70m-deduped-cleansharegpt</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/bigcode/santacoder</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigcode__santacoder</td>
      <td>25.33</td>
      <td>26.28</td>
      <td>25.60</td>
      <td>25.89</td>
      <td>51.24</td>
      <td>48.07</td>
      <td>0.00</td>
      <td>0.21</td>
      <td>GPT2LMHeadCustomModel</td>
      <td>torch.float16</td>
      <td>bigcode-openrail-m</td>
      <td>1.31</td>
      <td>300.0</td>
      <td>True</td>
      <td>132eb6b6cedaf579c2f333f1ecd78a16d7e45978</td>
      <td>bigcode/santacoder</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Abe13/juniper-certificate-Llama-2-7b-chat-hf</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Abe13__juniper-certificate-Llama-2-7b-chat-hf</td>
      <td>25.33</td>
      <td>29.10</td>
      <td>27.63</td>
      <td>24.02</td>
      <td>48.23</td>
      <td>48.30</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>90ed388e5503c02f5e6ba8dbc7286687a85ce1c1</td>
      <td>Abe13/juniper-certificate-Llama-2-7b-chat-hf</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-70m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-70m</td>
      <td>25.28</td>
      <td>21.59</td>
      <td>27.29</td>
      <td>25.90</td>
      <td>47.06</td>
      <td>51.46</td>
      <td>0.30</td>
      <td>3.33</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>14.0</td>
      <td>True</td>
      <td>2ab25ed47af79376eed2baaf8bbb7a192a0c73ff</td>
      <td>EleutherAI/pythia-70m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/marcchew/test1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_marcchew__test1</td>
      <td>25.27</td>
      <td>27.65</td>
      <td>26.17</td>
      <td>24.55</td>
      <td>48.33</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>?</td>
      <td>8bit</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>False</td>
      <td>7444355ad764584ef05805f58ccf174bb03e0f46</td>
      <td>marcchew/test1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/MusePy-1-2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__MusePy-1-2</td>
      <td>25.26</td>
      <td>25.77</td>
      <td>25.94</td>
      <td>25.22</td>
      <td>49.33</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>0.07</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.04</td>
      <td>0.0</td>
      <td>True</td>
      <td>6c1725158a74a41a10f21696a48510d45b4b425b</td>
      <td>BreadAi/MusePy-1-2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/porkorbeef/Llama-2-13b-public</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_porkorbeef__Llama-2-13b-public</td>
      <td>25.25</td>
      <td>29.95</td>
      <td>26.65</td>
      <td>22.74</td>
      <td>49.01</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>e1b32a8fcfc0f37fd5f50cf765151897574c73c7</td>
      <td>porkorbeef/Llama-2-13b-public</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/openbmb/UltraLM-13b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_openbmb__UltraLM-13b</td>
      <td>25.25</td>
      <td>29.44</td>
      <td>25.99</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>67.0</td>
      <td>True</td>
      <td>2c732c2899fc329036d97e5c6f0a61eaff19d97d</td>
      <td>openbmb/UltraLM-13b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cyberagent/open-calm-large</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cyberagent__open-calm-large</td>
      <td>25.24</td>
      <td>20.73</td>
      <td>29.56</td>
      <td>25.23</td>
      <td>46.52</td>
      <td>51.14</td>
      <td>0.08</td>
      <td>3.42</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>0.76</td>
      <td>9.0</td>
      <td>True</td>
      <td>f9b7a3222967b15169a09bcc86b118ac68a1ad62</td>
      <td>cyberagent/open-calm-large</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/microsoft/CodeGPT-small-py</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_microsoft__CodeGPT-small-py</td>
      <td>25.24</td>
      <td>22.70</td>
      <td>27.26</td>
      <td>25.05</td>
      <td>51.23</td>
      <td>48.78</td>
      <td>0.00</td>
      <td>1.64</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>18.0</td>
      <td>True</td>
      <td>e5f31df92bfb7b7a808ea8d1c7557488e1bdff7f</td>
      <td>microsoft/CodeGPT-small-py</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/nthngdy/pythia-owt2-70m-50k</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nthngdy__pythia-owt2-70m-50k</td>
      <td>25.22</td>
      <td>21.50</td>
      <td>28.15</td>
      <td>25.70</td>
      <td>44.50</td>
      <td>52.41</td>
      <td>0.00</td>
      <td>4.28</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>False</td>
      <td>9fce9b8252f7891dbd50299a8c3bd71cd25454db</td>
      <td>nthngdy/pythia-owt2-70m-50k</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TaylorAI/Flash-Llama-30M-20001</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TaylorAI__Flash-Llama-30M-20001</td>
      <td>25.22</td>
      <td>23.89</td>
      <td>25.76</td>
      <td>24.09</td>
      <td>51.29</td>
      <td>50.83</td>
      <td>0.00</td>
      <td>0.68</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>6ff84442217565875450bd7a0457121dcedf6b0b</td>
      <td>TaylorAI/Flash-Llama-30M-20001</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/breadlicker45/dough-base-001</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_breadlicker45__dough-base-001</td>
      <td>25.22</td>
      <td>23.89</td>
      <td>24.76</td>
      <td>23.13</td>
      <td>53.40</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>0.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.15</td>
      <td>0.0</td>
      <td>True</td>
      <td>e42b65191f97d786eadaba450f1d34baea470734</td>
      <td>breadlicker45/dough-base-001</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/breadlicker45/dough-instruct-base-001</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_breadlicker45__dough-instruct-base-001</td>
      <td>25.22</td>
      <td>23.89</td>
      <td>24.76</td>
      <td>23.13</td>
      <td>53.40</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>0.29</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.19</td>
      <td>0.0</td>
      <td>True</td>
      <td>3e1b0bf0a887feeb342982eee4f6d8041772a7dd</td>
      <td>breadlicker45/dough-instruct-base-001</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/gpt-YA-1-1_160M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__gpt-YA-1-1_160M</td>
      <td>25.21</td>
      <td>22.95</td>
      <td>27.29</td>
      <td>26.25</td>
      <td>47.02</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>2.32</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>b9b3577df726f7984721e4d73741296db50fa782</td>
      <td>BreadAi/gpt-YA-1-1_160M</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/pszemraj/pythia-31m-KI_v1-2048-scratch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pszemraj__pythia-31m-KI_v1-2048-scratch</td>
      <td>25.21</td>
      <td>23.12</td>
      <td>25.23</td>
      <td>23.12</td>
      <td>51.67</td>
      <td>51.78</td>
      <td>0.00</td>
      <td>1.52</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>b29a3229f8d5317adeabafeb20677ec7bea9d703</td>
      <td>pszemraj/pythia-31m-KI_v1-2048-scratch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MayaPH/FinOPT-Lincoln</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MayaPH__FinOPT-Lincoln</td>
      <td>25.20</td>
      <td>26.71</td>
      <td>25.60</td>
      <td>23.00</td>
      <td>50.59</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.76</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>0.33</td>
      <td>1.0</td>
      <td>True</td>
      <td>7ddc381fa3968df22f72acb6cf03b75d3ac49661</td>
      <td>MayaPH/FinOPT-Lincoln</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/klosax/pythia-70m-deduped-step44k-92bt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_klosax__pythia-70m-deduped-step44k-92bt</td>
      <td>25.19</td>
      <td>22.10</td>
      <td>28.21</td>
      <td>26.03</td>
      <td>46.12</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>2.37</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.04</td>
      <td>0.0</td>
      <td>True</td>
      <td>aac86fff08965d84d8bfc3e7c14559d48b8c4c99</td>
      <td>klosax/pythia-70m-deduped-step44k-92bt</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/nicholasKluge/Aira-2-1B1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_nicholasKluge__Aira-2-1B1</td>
      <td>25.19</td>
      <td>23.21</td>
      <td>26.97</td>
      <td>24.86</td>
      <td>50.63</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>0.39</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.10</td>
      <td>0.0</td>
      <td>True</td>
      <td>a53eb20b72ae86441566f99acc204d9bb527bf32</td>
      <td>nicholasKluge/Aira-2-1B1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/DiscordPy</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__DiscordPy</td>
      <td>25.19</td>
      <td>23.29</td>
      <td>26.15</td>
      <td>25.04</td>
      <td>48.16</td>
      <td>50.99</td>
      <td>0.00</td>
      <td>2.68</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.26</td>
      <td>0.0</td>
      <td>True</td>
      <td>a5405585aec0b60c5de7d942ccd58421fe9239be</td>
      <td>BreadAi/DiscordPy</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/microsoft/DialoGPT-large</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_microsoft__DialoGPT-large</td>
      <td>25.17</td>
      <td>23.38</td>
      <td>25.77</td>
      <td>23.81</td>
      <td>50.27</td>
      <td>52.41</td>
      <td>0.00</td>
      <td>0.58</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.77</td>
      <td>203.0</td>
      <td>True</td>
      <td>04e3e47b52dadbcf7688aa61a7ed0438ecf9184c</td>
      <td>microsoft/DialoGPT-large</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/yhyhy3/med-orca-instruct-33b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yhyhy3__med-orca-instruct-33b</td>
      <td>25.17</td>
      <td>27.39</td>
      <td>25.89</td>
      <td>25.37</td>
      <td>49.60</td>
      <td>47.91</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>32.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>1d636881854338e571825226c712180da06be72c</td>
      <td>yhyhy3/med-orca-instruct-33b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2_open-platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2_open-platypus</td>
      <td>25.16</td>
      <td>22.18</td>
      <td>31.29</td>
      <td>26.19</td>
      <td>40.35</td>
      <td>51.30</td>
      <td>0.15</td>
      <td>4.64</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>745c1864b752525789cad2b75166c519a327325e</td>
      <td>lgaalves/gpt2_open-platypus</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2_guanaco-dolly-platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2_guanaco-dolly-platypus</td>
      <td>25.15</td>
      <td>23.55</td>
      <td>31.03</td>
      <td>26.40</td>
      <td>40.02</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>4.96</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>6bf0a8146cf255c829ec2ad83926c8b80945b431</td>
      <td>lgaalves/gpt2_guanaco-dolly-platypus</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2_platypus-dolly-guanaco</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2_platypus-dolly-guanaco</td>
      <td>25.15</td>
      <td>23.21</td>
      <td>31.04</td>
      <td>26.16</td>
      <td>40.31</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>4.98</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>bfa144d3eb087e54f1798fd2e2fb17e894cc39d3</td>
      <td>lgaalves/gpt2_platypus-dolly-guanaco</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/concedo/Pythia-70M-ChatSalad</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_concedo__Pythia-70M-ChatSalad</td>
      <td>25.15</td>
      <td>20.99</td>
      <td>27.28</td>
      <td>24.78</td>
      <td>49.74</td>
      <td>52.41</td>
      <td>0.00</td>
      <td>0.84</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.10</td>
      <td>5.0</td>
      <td>True</td>
      <td>692289413c47c219cf83b1596783a8e9223541eb</td>
      <td>concedo/Pythia-70M-ChatSalad</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/budecosystem/boomer-1b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_budecosystem__boomer-1b</td>
      <td>25.12</td>
      <td>22.78</td>
      <td>31.58</td>
      <td>25.66</td>
      <td>39.17</td>
      <td>50.51</td>
      <td>0.91</td>
      <td>5.21</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.94</td>
      <td>1.0</td>
      <td>True</td>
      <td>f8f24b5480fa43f23d858f0eb8d1af1b7ad0af59</td>
      <td>budecosystem/boomer-1b</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/Harshvir/LaMini-Neo-1.3B-Mental-Health_lora</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Harshvir__LaMini-Neo-1.3B-Mental-Health_lora</td>
      <td>25.12</td>
      <td>25.77</td>
      <td>25.67</td>
      <td>27.00</td>
      <td>48.21</td>
      <td>49.17</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>9f1c45d5ce88a8eaf7ec03b760a4adfb5fda07eb</td>
      <td>Harshvir/LaMini-Neo-1.3B-Mental-Health_lora</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/lamini-cerebras-256m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__lamini-cerebras-256m</td>
      <td>25.11</td>
      <td>21.76</td>
      <td>28.70</td>
      <td>26.66</td>
      <td>41.81</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>4.86</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.26</td>
      <td>3.0</td>
      <td>True</td>
      <td>72df0b6d62d64002575687ea2edbb0df05712678</td>
      <td>MBZUAI/lamini-cerebras-256m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/LaMini-GPT-124M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__LaMini-GPT-124M</td>
      <td>25.11</td>
      <td>24.32</td>
      <td>30.82</td>
      <td>24.99</td>
      <td>36.57</td>
      <td>51.38</td>
      <td>0.00</td>
      <td>7.70</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.12</td>
      <td>14.0</td>
      <td>True</td>
      <td>5c67c8c03c08e82d6138ce2a1eddf5317fac3a6b</td>
      <td>MBZUAI/LaMini-GPT-124M</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/bsp-albz/llama2-13b-platypus-ckpt-1000</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bsp-albz__llama2-13b-platypus-ckpt-1000</td>
      <td>25.10</td>
      <td>28.16</td>
      <td>26.55</td>
      <td>23.17</td>
      <td>48.79</td>
      <td>49.01</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>0.0</td>
      <td>True</td>
      <td>d9f3e490df2134784afc3a86f5c617a9bab8db4d</td>
      <td>bsp-albz/llama2-13b-platypus-ckpt-1000</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/euclaise/gpt-neox-122m-minipile-digits</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_euclaise__gpt-neox-122m-minipile-digits</td>
      <td>25.10</td>
      <td>20.73</td>
      <td>27.03</td>
      <td>25.31</td>
      <td>49.19</td>
      <td>52.33</td>
      <td>0.00</td>
      <td>1.09</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc0-1.0</td>
      <td>0.17</td>
      <td>2.0</td>
      <td>True</td>
      <td>3e9187385d31234b04021ddc8b03cbd5cfef9fb4</td>
      <td>euclaise/gpt-neox-122m-minipile-digits</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/voidful/changpt-bart</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_voidful__changpt-bart</td>
      <td>25.09</td>
      <td>28.67</td>
      <td>26.41</td>
      <td>23.12</td>
      <td>47.94</td>
      <td>49.49</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>BartForConditionalGeneration</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.18</td>
      <td>0.0</td>
      <td>True</td>
      <td>e3d26f736b8b47d5275421be6133b81bef84db7d</td>
      <td>voidful/changpt-bart</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Locutusque/gpt2-conversational-or-qa</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Locutusque__gpt2-conversational-or-qa</td>
      <td>25.09</td>
      <td>21.42</td>
      <td>27.61</td>
      <td>26.51</td>
      <td>47.31</td>
      <td>51.14</td>
      <td>0.08</td>
      <td>1.55</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>0.14</td>
      <td>1.0</td>
      <td>True</td>
      <td>f881c740c82ee9bc3191b886ad53f18d741960ea</td>
      <td>Locutusque/gpt2-conversational-or-qa</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/victor123/WizardLM-13B-1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_victor123__WizardLM-13B-1.0</td>
      <td>25.09</td>
      <td>28.50</td>
      <td>25.97</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>49.41</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>66.0</td>
      <td>True</td>
      <td>2ea86d3c02ca0c2abb086a2145e1e85eaea4a23e</td>
      <td>victor123/WizardLM-13B-1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/alibidaran/medical_transcription_generator</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_alibidaran__medical_transcription_generator</td>
      <td>25.08</td>
      <td>22.78</td>
      <td>30.60</td>
      <td>23.84</td>
      <td>46.50</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>1.42</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.14</td>
      <td>1.0</td>
      <td>True</td>
      <td>f622239151c89c2db0f1cef495d1b42afd16ce64</td>
      <td>alibidaran/medical_transcription_generator</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/RWKV/rwkv-4-169m-pile</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_RWKV__rwkv-4-169m-pile</td>
      <td>25.07</td>
      <td>23.63</td>
      <td>31.74</td>
      <td>23.18</td>
      <td>41.92</td>
      <td>50.91</td>
      <td>0.45</td>
      <td>3.65</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.13</td>
      <td>5.0</td>
      <td>True</td>
      <td>46bdc280eb97b6141d5d51a935e0c4870ecaefcc</td>
      <td>RWKV/rwkv-4-169m-pile</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/HWERI/pythia-70m-deduped-cleansharegpt-en</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_HWERI__pythia-70m-deduped-cleansharegpt-en</td>
      <td>25.06</td>
      <td>21.16</td>
      <td>27.16</td>
      <td>25.24</td>
      <td>48.57</td>
      <td>50.12</td>
      <td>0.00</td>
      <td>3.15</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.04</td>
      <td>0.0</td>
      <td>True</td>
      <td>a97ff56bc68a81a9f6147f1590e53511246d1040</td>
      <td>HWERI/pythia-70m-deduped-cleansharegpt-en</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2_platypus-camel_physics</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2_platypus-camel_physics</td>
      <td>25.04</td>
      <td>23.04</td>
      <td>31.32</td>
      <td>26.91</td>
      <td>39.56</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>4.79</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>66165ff32ed8de6c39f3524a810f5e97ba6d3347</td>
      <td>lgaalves/gpt2_platypus-camel_physics</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2_camel_physics-platypus</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2_camel_physics-platypus</td>
      <td>25.04</td>
      <td>23.04</td>
      <td>31.32</td>
      <td>26.91</td>
      <td>39.56</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>4.79</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>66165ff32ed8de6c39f3524a810f5e97ba6d3347</td>
      <td>lgaalves/gpt2_camel_physics-platypus</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/FabbriSimo01/GPT_Large_Quantized</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FabbriSimo01__GPT_Large_Quantized</td>
      <td>25.03</td>
      <td>27.05</td>
      <td>26.29</td>
      <td>24.12</td>
      <td>48.46</td>
      <td>49.33</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>unknown</td>
      <td>0.77</td>
      <td>0.0</td>
      <td>True</td>
      <td>c2df1904aa18de22d03ba0fee925e831d8468898</td>
      <td>FabbriSimo01/GPT_Large_Quantized</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/behnamsh/gpt2_platypus-camel_physics</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_behnamsh__gpt2_platypus-camel_physics</td>
      <td>25.03</td>
      <td>22.78</td>
      <td>31.24</td>
      <td>25.87</td>
      <td>38.95</td>
      <td>51.54</td>
      <td>0.00</td>
      <td>4.85</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>cd4d700d13b3bc9371bf45616ef74ac20d165c3d</td>
      <td>behnamsh/gpt2_platypus-camel_physics</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/BEE-spoke-data/verysmol_llama-v11-KIx2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BEE-spoke-data__verysmol_llama-v11-KIx2</td>
      <td>25.03</td>
      <td>22.70</td>
      <td>27.60</td>
      <td>25.28</td>
      <td>44.75</td>
      <td>51.54</td>
      <td>0.30</td>
      <td>3.03</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>1cd271d3d62a9e1dc4b7c2978e54806d74705439</td>
      <td>BEE-spoke-data/verysmol_llama-v11-KIx2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-7b-longlora-32k-ft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-7b-longlora-32k-ft</td>
      <td>25.03</td>
      <td>27.90</td>
      <td>25.61</td>
      <td>23.08</td>
      <td>49.57</td>
      <td>49.01</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>True</td>
      <td>ab48674ffc55568ffe2a1207ef0e711c2febbaaf</td>
      <td>Yukang/Llama-2-7b-longlora-32k-ft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dpv/finetuned-gpt2-tiny</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dpv__finetuned-gpt2-tiny</td>
      <td>25.02</td>
      <td>21.84</td>
      <td>31.60</td>
      <td>25.86</td>
      <td>40.67</td>
      <td>50.12</td>
      <td>0.30</td>
      <td>4.78</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>379e02101b4dccba48e7ae792708d2fe7f0bbca2</td>
      <td>dpv/finetuned-gpt2-tiny</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/SaylorTwift/gpt2_test</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_SaylorTwift__gpt2_test</td>
      <td>25.02</td>
      <td>21.84</td>
      <td>31.60</td>
      <td>25.86</td>
      <td>40.67</td>
      <td>50.12</td>
      <td>0.30</td>
      <td>4.78</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>ef61310a16ffda93bf8f6132e02658482ffc2bcc</td>
      <td>SaylorTwift/gpt2_test</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/roneneldan/TinyStories-1M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_roneneldan__TinyStories-1M</td>
      <td>25.02</td>
      <td>23.46</td>
      <td>25.23</td>
      <td>24.57</td>
      <td>49.40</td>
      <td>52.17</td>
      <td>0.00</td>
      <td>0.32</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.00</td>
      <td>20.0</td>
      <td>True</td>
      <td>8cd14d5339178f1b285f55baee14a0deff7103ac</td>
      <td>roneneldan/TinyStories-1M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/microsoft/DialoGPT-small</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_microsoft__DialoGPT-small</td>
      <td>25.02</td>
      <td>25.77</td>
      <td>25.79</td>
      <td>25.81</td>
      <td>47.49</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.18</td>
      <td>55.0</td>
      <td>True</td>
      <td>97d0fec744c2cb4d48f5db51d17e3258e185858e</td>
      <td>microsoft/DialoGPT-small</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/Quokka_590m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__Quokka_590m</td>
      <td>25.02</td>
      <td>24.40</td>
      <td>31.61</td>
      <td>25.36</td>
      <td>39.59</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>3.96</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.67</td>
      <td>0.0</td>
      <td>True</td>
      <td>ae0ac41e9be016f6dceac06821fbf6ebacc7edb9</td>
      <td>Corianas/Quokka_590m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v2-124m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v2-124m</td>
      <td>25.01</td>
      <td>23.98</td>
      <td>31.10</td>
      <td>25.29</td>
      <td>38.98</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>5.29</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>4.0</td>
      <td>True</td>
      <td>bc719f990748ea72be4b6c270df34fc3d37291dc</td>
      <td>aisquared/dlite-v2-124m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-13b-longlora-16k-ft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-13b-longlora-16k-ft</td>
      <td>25.00</td>
      <td>25.85</td>
      <td>27.60</td>
      <td>23.10</td>
      <td>48.89</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>12.85</td>
      <td>1.0</td>
      <td>True</td>
      <td>5f0cfdef590fc9bd7642042fb5f1ed9679260b93</td>
      <td>Yukang/Llama-2-13b-longlora-16k-ft</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/beomi/KoRWKV-6B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beomi__KoRWKV-6B</td>
      <td>25.00</td>
      <td>22.10</td>
      <td>32.18</td>
      <td>24.69</td>
      <td>39.05</td>
      <td>51.14</td>
      <td>0.00</td>
      <td>5.83</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>6.53</td>
      <td>3.0</td>
      <td>True</td>
      <td>541600070459baf0f1be9560181d5ceb77794085</td>
      <td>beomi/KoRWKV-6B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/KnutJaegersberg/RWKV-4-PilePlus-169M-20230520-done-ctx4096</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__RWKV-4-PilePlus-169M-20230520-done-ctx4096</td>
      <td>25.00</td>
      <td>23.98</td>
      <td>32.25</td>
      <td>23.37</td>
      <td>42.29</td>
      <td>49.17</td>
      <td>0.38</td>
      <td>3.53</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.13</td>
      <td>0.0</td>
      <td>True</td>
      <td>1134d31db1aee9fc970d3e9dc4e7314fb8bba500</td>
      <td>KnutJaegersberg/RWKV-4-PilePlus-169M-20230520-done-ctx4096</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/vicgalle/gpt2-alpaca-gpt4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vicgalle__gpt2-alpaca-gpt4</td>
      <td>24.98</td>
      <td>22.61</td>
      <td>31.17</td>
      <td>25.76</td>
      <td>38.04</td>
      <td>52.17</td>
      <td>0.30</td>
      <td>4.83</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.14</td>
      <td>14.0</td>
      <td>True</td>
      <td>282e9bd56f0cab5d48e6954793647eecaa0871d9</td>
      <td>vicgalle/gpt2-alpaca-gpt4</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/TheBloke/Llama-2-7b-Chat-AWQ</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheBloke__Llama-2-7b-Chat-AWQ</td>
      <td>24.98</td>
      <td>27.22</td>
      <td>25.48</td>
      <td>24.67</td>
      <td>49.95</td>
      <td>47.51</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>1.13</td>
      <td>1.0</td>
      <td>True</td>
      <td>a065961fd627aa3b3e6dde21e77fd5e20f712189</td>
      <td>TheBloke/Llama-2-7b-Chat-AWQ</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/bigcode/gpt_bigcode-santacoder</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_bigcode__gpt_bigcode-santacoder</td>
      <td>24.95</td>
      <td>21.16</td>
      <td>30.84</td>
      <td>24.97</td>
      <td>45.64</td>
      <td>47.83</td>
      <td>0.53</td>
      <td>3.72</td>
      <td>GPTBigCodeForCausalLM</td>
      <td>torch.float16</td>
      <td>openrail</td>
      <td>1.12</td>
      <td>21.0</td>
      <td>True</td>
      <td>291931872cae83498cf984b16319f47f5e9e7a07</td>
      <td>bigcode/gpt_bigcode-santacoder</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Aspik101/tulu-7b-instruct-pl-lora_unload</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Aspik101__tulu-7b-instruct-pl-lora_unload</td>
      <td>24.95</td>
      <td>28.67</td>
      <td>26.05</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>48.22</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>962d4e5d8da5a4ec0ec047b6f8f08f1bb9e509fe</td>
      <td>Aspik101/tulu-7b-instruct-pl-lora_unload</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/beomi/KoAlpaca-KoRWKV-6B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_beomi__KoAlpaca-KoRWKV-6B</td>
      <td>24.94</td>
      <td>23.46</td>
      <td>31.65</td>
      <td>24.89</td>
      <td>39.83</td>
      <td>51.62</td>
      <td>0.00</td>
      <td>3.16</td>
      <td>RwkvForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.53</td>
      <td>7.0</td>
      <td>True</td>
      <td>427ee72c4350f26de1b287a0c07b842e7d168dbc</td>
      <td>beomi/KoAlpaca-KoRWKV-6B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-7b-longlora-100k-ft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-7b-longlora-100k-ft</td>
      <td>24.93</td>
      <td>28.16</td>
      <td>25.43</td>
      <td>23.48</td>
      <td>49.06</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>37.0</td>
      <td>True</td>
      <td>242c6469cab41b41d30826e850afa4687e422f24</td>
      <td>Yukang/Llama-2-7b-longlora-100k-ft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/anas-awadalla/mpt-1b-redpajama-200b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_anas-awadalla__mpt-1b-redpajama-200b</td>
      <td>24.90</td>
      <td>25.77</td>
      <td>26.08</td>
      <td>24.50</td>
      <td>47.57</td>
      <td>50.36</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>MosaicGPT</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.00</td>
      <td>2.0</td>
      <td>True</td>
      <td>fc98636655efb7c091bbe5d8014eb138ddfc5471</td>
      <td>anas-awadalla/mpt-1b-redpajama-200b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/StoryPy</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__StoryPy</td>
      <td>24.89</td>
      <td>22.35</td>
      <td>26.19</td>
      <td>24.37</td>
      <td>49.10</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>1.18</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.10</td>
      <td>1.0</td>
      <td>True</td>
      <td>5c32081bd3bc1404c2f5b8dbb6f888048bcb7cd7</td>
      <td>BreadAi/StoryPy</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/ethzanalytics/pythia-31m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ethzanalytics__pythia-31m</td>
      <td>24.89</td>
      <td>21.84</td>
      <td>27.00</td>
      <td>24.97</td>
      <td>49.10</td>
      <td>49.72</td>
      <td>0.23</td>
      <td>1.37</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>eeea0b6b80603d162fe7de4e80a5bf4a8e9c6207</td>
      <td>ethzanalytics/pythia-31m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/postbot/distilgpt2-emailgen</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_postbot__distilgpt2-emailgen</td>
      <td>24.89</td>
      <td>21.76</td>
      <td>27.52</td>
      <td>25.97</td>
      <td>46.17</td>
      <td>51.62</td>
      <td>0.00</td>
      <td>1.16</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.09</td>
      <td>2.0</td>
      <td>True</td>
      <td>fe96d63cc2edcbd1ae444ada293cc59d1e01a6ad</td>
      <td>postbot/distilgpt2-emailgen</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/Quokka_256m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__Quokka_256m</td>
      <td>24.88</td>
      <td>22.87</td>
      <td>28.84</td>
      <td>26.48</td>
      <td>39.47</td>
      <td>52.25</td>
      <td>0.00</td>
      <td>4.27</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.32</td>
      <td>1.0</td>
      <td>True</td>
      <td>d4e69f714d360d39979eb7b8cbc9decdb7190c88</td>
      <td>Corianas/Quokka_256m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MayaPH/FinOPT-Washington</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MayaPH__FinOPT-Washington</td>
      <td>24.87</td>
      <td>25.17</td>
      <td>26.25</td>
      <td>24.83</td>
      <td>45.80</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>cdd8a6cde7902de39757cf31d73af1f51df0d8e8</td>
      <td>MayaPH/FinOPT-Washington</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/gpt-Youtube</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__gpt-Youtube</td>
      <td>24.86</td>
      <td>23.29</td>
      <td>26.34</td>
      <td>23.54</td>
      <td>48.63</td>
      <td>48.93</td>
      <td>0.00</td>
      <td>3.32</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>de88554a0212c16fdfeda030afb58f831ebcd895</td>
      <td>BreadAi/gpt-Youtube</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/concedo/OPT-19M-ChatSalad</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_concedo__OPT-19M-ChatSalad</td>
      <td>24.86</td>
      <td>24.40</td>
      <td>25.15</td>
      <td>23.12</td>
      <td>51.36</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.25</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>0.02</td>
      <td>15.0</td>
      <td>True</td>
      <td>3930ca6bf3976e9b603815403cb373398ae509e5</td>
      <td>concedo/OPT-19M-ChatSalad</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pszemraj__pythia-31m-goodwiki-deduped-2048-scratch</td>
      <td>24.85</td>
      <td>23.12</td>
      <td>25.66</td>
      <td>23.11</td>
      <td>51.32</td>
      <td>49.88</td>
      <td>0.00</td>
      <td>0.86</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>01a3cd918dd7c233bc0c3c0c948a9a462a5359d1</td>
      <td>pszemraj/pythia-31m-goodwiki-deduped-2048-scratch</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/crumb/gpt2023</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_crumb__gpt2023</td>
      <td>24.85</td>
      <td>21.93</td>
      <td>31.11</td>
      <td>25.05</td>
      <td>40.71</td>
      <td>50.12</td>
      <td>0.30</td>
      <td>4.73</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.14</td>
      <td>12.0</td>
      <td>True</td>
      <td>e3620b53d164529575db66d9d4f4382311dd713c</td>
      <td>crumb/gpt2023</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardLM-30B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardLM-30B-V1.0</td>
      <td>24.82</td>
      <td>27.39</td>
      <td>25.94</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>48.70</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>?</td>
      <td>32.32</td>
      <td>70.0</td>
      <td>True</td>
      <td>815e2dd7daabe446c429f3c9f70ef01582528f81</td>
      <td>WizardLM/WizardLM-30B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/WizardLM/WizardLM-30B-V1.0</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_WizardLM__WizardLM-30B-V1.0</td>
      <td>24.81</td>
      <td>27.39</td>
      <td>25.94</td>
      <td>23.12</td>
      <td>48.61</td>
      <td>48.62</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>8bit</td>
      <td>?</td>
      <td>32.32</td>
      <td>70.0</td>
      <td>True</td>
      <td>815e2dd7daabe446c429f3c9f70ef01582528f81</td>
      <td>WizardLM/WizardLM-30B-V1.0</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/anton-l/gpt-j-tiny-random</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_anton-l__gpt-j-tiny-random</td>
      <td>24.79</td>
      <td>26.37</td>
      <td>25.76</td>
      <td>24.46</td>
      <td>47.44</td>
      <td>49.49</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>GPTJForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.05</td>
      <td>1.0</td>
      <td>True</td>
      <td>feea91564dac0081f73aeb6744979c6cfe553fff</td>
      <td>anton-l/gpt-j-tiny-random</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/cyberagent/open-calm-7b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cyberagent__open-calm-7b</td>
      <td>24.74</td>
      <td>20.48</td>
      <td>30.65</td>
      <td>25.22</td>
      <td>44.15</td>
      <td>48.54</td>
      <td>0.23</td>
      <td>3.93</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-sa-4.0</td>
      <td>6.66</td>
      <td>188.0</td>
      <td>True</td>
      <td>276a5fb67510554e11ef191a2da44c919acccdf5</td>
      <td>cyberagent/open-calm-7b</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/microsoft/DialoGPT-medium</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_microsoft__DialoGPT-medium</td>
      <td>24.74</td>
      <td>24.49</td>
      <td>26.21</td>
      <td>25.84</td>
      <td>47.06</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.36</td>
      <td>240.0</td>
      <td>True</td>
      <td>9d5c5fadcc072b693fb5a5e29416bbf3f503c26c</td>
      <td>microsoft/DialoGPT-medium</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Quake24/easyTermsSummerizer</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Quake24__easyTermsSummerizer</td>
      <td>24.73</td>
      <td>25.77</td>
      <td>25.81</td>
      <td>23.12</td>
      <td>47.69</td>
      <td>50.75</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>BartForConditionalGeneration</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.41</td>
      <td>0.0</td>
      <td>True</td>
      <td>8df9f96cc14be8f681c40bd1672b3f3540b70e31</td>
      <td>Quake24/easyTermsSummerizer</td>
    </tr>
    <tr>
      <td>instruction-tuned</td>
      <td>https://huggingface.co/lgaalves/gpt2-dolly</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_lgaalves__gpt2-dolly</td>
      <td>24.73</td>
      <td>21.76</td>
      <td>30.77</td>
      <td>24.66</td>
      <td>42.22</td>
      <td>49.57</td>
      <td>0.08</td>
      <td>4.08</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>52fcf61a8eef255a981be6efde187481086e1a48</td>
      <td>lgaalves/gpt2-dolly</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Mikivis/xuanxuan</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Mikivis__xuanxuan</td>
      <td>24.72</td>
      <td>23.46</td>
      <td>31.12</td>
      <td>26.27</td>
      <td>35.97</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>5.74</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.14</td>
      <td>0.0</td>
      <td>True</td>
      <td>ba6ae2b347bc613ae38980e059ec8c5ec8b26038</td>
      <td>Mikivis/xuanxuan</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/EleutherAI/pythia-70m-deduped</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__pythia-70m-deduped</td>
      <td>24.71</td>
      <td>21.08</td>
      <td>27.17</td>
      <td>25.26</td>
      <td>47.51</td>
      <td>49.64</td>
      <td>0.00</td>
      <td>2.30</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.10</td>
      <td>14.0</td>
      <td>True</td>
      <td>e93a9faa9c77e5d09219f6c868bfc7a1bd65593c</td>
      <td>EleutherAI/pythia-70m-deduped</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pszemraj__pythia-31m-simplepile-lite-2048-scratch-2e</td>
      <td>24.70</td>
      <td>21.59</td>
      <td>25.79</td>
      <td>24.99</td>
      <td>50.62</td>
      <td>48.62</td>
      <td>0.00</td>
      <td>1.32</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>91f011eb99502e667ebc2803f354ce5f5209ccf1</td>
      <td>pszemraj/pythia-31m-simplepile-lite-2048-scratch-2e</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Yukang/Llama-2-7b-longlora-16k-ft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Yukang__Llama-2-7b-longlora-16k-ft</td>
      <td>24.69</td>
      <td>26.37</td>
      <td>26.37</td>
      <td>23.75</td>
      <td>47.76</td>
      <td>48.62</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>c86de31b80866d047e680e08dbd3572e2965d4c5</td>
      <td>Yukang/Llama-2-7b-longlora-16k-ft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/vicgalle/gpt2-alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_vicgalle__gpt2-alpaca</td>
      <td>24.66</td>
      <td>22.87</td>
      <td>31.14</td>
      <td>26.26</td>
      <td>36.22</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>5.46</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>0.14</td>
      <td>8.0</td>
      <td>True</td>
      <td>e06875a588f7b3386c18a6efdc8cc7583d95b21b</td>
      <td>vicgalle/gpt2-alpaca</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/pszemraj/pythia-31m-simplewiki-scratch-bf16</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pszemraj__pythia-31m-simplewiki-scratch-bf16</td>
      <td>24.63</td>
      <td>22.78</td>
      <td>25.61</td>
      <td>23.12</td>
      <td>49.65</td>
      <td>50.51</td>
      <td>0.00</td>
      <td>0.72</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>4eaec0542e7609fd3f364cb34491f05d7c61a3d0</td>
      <td>pszemraj/pythia-31m-simplewiki-scratch-bf16</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/abhiramtirumala/DialoGPT-sarcastic-medium</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_abhiramtirumala__DialoGPT-sarcastic-medium</td>
      <td>24.63</td>
      <td>23.29</td>
      <td>25.93</td>
      <td>23.76</td>
      <td>46.04</td>
      <td>53.35</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.14</td>
      <td>1.0</td>
      <td>True</td>
      <td>292596e120591887383011c4520bc5b57e7e8993</td>
      <td>abhiramtirumala/DialoGPT-sarcastic-medium</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aisquared/dlite-v1-124m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aisquared__dlite-v1-124m</td>
      <td>24.62</td>
      <td>24.32</td>
      <td>31.16</td>
      <td>25.08</td>
      <td>36.38</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>5.20</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>0.0</td>
      <td>True</td>
      <td>f6fd5f3960f31881e6cee23f5a872ecc80b40283</td>
      <td>aisquared/dlite-v1-124m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/postbot/distilgpt2-emailgen-V2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_postbot__distilgpt2-emailgen-V2</td>
      <td>24.59</td>
      <td>20.99</td>
      <td>26.78</td>
      <td>25.53</td>
      <td>46.51</td>
      <td>52.01</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.09</td>
      <td>2.0</td>
      <td>True</td>
      <td>9750ba00e79a02e1bf98d3faa3d49b8ae0f8ae63</td>
      <td>postbot/distilgpt2-emailgen-V2</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/MBZUAI/lamini-cerebras-111m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_MBZUAI__lamini-cerebras-111m</td>
      <td>24.57</td>
      <td>22.10</td>
      <td>27.12</td>
      <td>25.51</td>
      <td>43.79</td>
      <td>51.22</td>
      <td>0.00</td>
      <td>2.22</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.11</td>
      <td>3.0</td>
      <td>True</td>
      <td>e8e347b02f9305e4bc144eb9be2821c518d43183</td>
      <td>MBZUAI/lamini-cerebras-111m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/blueapple8259/TinyStories-Alpaca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_blueapple8259__TinyStories-Alpaca</td>
      <td>24.51</td>
      <td>23.98</td>
      <td>24.92</td>
      <td>23.35</td>
      <td>46.68</td>
      <td>51.85</td>
      <td>0.00</td>
      <td>0.81</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>18e0bde7e72e477757832f0624a0410efc066216</td>
      <td>blueapple8259/TinyStories-Alpaca</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/yeen214/llama2_7b_small_tuning_v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_yeen214__llama2_7b_small_tuning_v1</td>
      <td>24.48</td>
      <td>22.44</td>
      <td>25.00</td>
      <td>25.51</td>
      <td>48.70</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>6.61</td>
      <td>0.0</td>
      <td>True</td>
      <td>3f9b43b4db2da4fe3785071dd52c9fc92aa0801d</td>
      <td>yeen214/llama2_7b_small_tuning_v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/huggingtweets/gladosystem</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huggingtweets__gladosystem</td>
      <td>24.46</td>
      <td>24.40</td>
      <td>29.71</td>
      <td>23.18</td>
      <td>41.78</td>
      <td>50.67</td>
      <td>0.00</td>
      <td>1.49</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>02a1bbcee7b584ace743b2fe4885cc0eaf2179ac</td>
      <td>huggingtweets/gladosystem</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/roneneldan/TinyStories-28M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_roneneldan__TinyStories-28M</td>
      <td>24.39</td>
      <td>22.78</td>
      <td>25.83</td>
      <td>23.53</td>
      <td>48.08</td>
      <td>50.43</td>
      <td>0.00</td>
      <td>0.07</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.05</td>
      <td>5.0</td>
      <td>True</td>
      <td>52dabea9997faf578489d619249616926e54ed18</td>
      <td>roneneldan/TinyStories-28M</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/roneneldan/TinyStories-33M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_roneneldan__TinyStories-33M</td>
      <td>24.38</td>
      <td>24.23</td>
      <td>25.69</td>
      <td>23.82</td>
      <td>47.64</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>0.19</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.07</td>
      <td>61.0</td>
      <td>True</td>
      <td>190d22e37cba4b12ddae57d6738a0c65f6ab1aa5</td>
      <td>roneneldan/TinyStories-33M</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/ethzanalytics/pythia-31m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ethzanalytics__pythia-31m</td>
      <td>24.35</td>
      <td>19.97</td>
      <td>26.34</td>
      <td>24.27</td>
      <td>50.12</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>0.66</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>8a3c2f1555de8a3c53d67d73b5d0d53a66a6c6c2</td>
      <td>ethzanalytics/pythia-31m</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/pszemraj/pythia-31m-simplewiki-2048</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_pszemraj__pythia-31m-simplewiki-2048</td>
      <td>24.35</td>
      <td>22.18</td>
      <td>25.55</td>
      <td>23.12</td>
      <td>49.37</td>
      <td>49.41</td>
      <td>0.00</td>
      <td>0.81</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>apache-2.0</td>
      <td>0.03</td>
      <td>0.0</td>
      <td>True</td>
      <td>95d47818055661250b55144c7d9beaf05dc126d8</td>
      <td>pszemraj/pythia-31m-simplewiki-2048</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/mncai/SGPT-1.3B-insurance-epoch10</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_mncai__SGPT-1.3B-insurance-epoch10</td>
      <td>24.31</td>
      <td>24.57</td>
      <td>24.25</td>
      <td>25.23</td>
      <td>45.24</td>
      <td>50.91</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.27</td>
      <td>0.0</td>
      <td>True</td>
      <td>df685c0bbf838f0627383c28f48e577ee901ba68</td>
      <td>mncai/SGPT-1.3B-insurance-epoch10</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/roneneldan/TinyStories-8M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_roneneldan__TinyStories-8M</td>
      <td>24.31</td>
      <td>24.66</td>
      <td>25.03</td>
      <td>23.33</td>
      <td>46.54</td>
      <td>50.28</td>
      <td>0.00</td>
      <td>0.31</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.02</td>
      <td>3.0</td>
      <td>True</td>
      <td>8612e3b15c66ffa94eaa6ee0de5c96edd2d630af</td>
      <td>roneneldan/TinyStories-8M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/psyche/kogpt</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_psyche__kogpt</td>
      <td>24.27</td>
      <td>21.16</td>
      <td>28.11</td>
      <td>26.56</td>
      <td>42.06</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>2.89</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.39</td>
      <td>3.0</td>
      <td>True</td>
      <td>4c02d48f548103ba53a5e481b8aa81bf7a259287</td>
      <td>psyche/kogpt</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/TurkuNLP/gpt3-finnish-small</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TurkuNLP__gpt3-finnish-small</td>
      <td>24.25</td>
      <td>20.48</td>
      <td>28.09</td>
      <td>24.47</td>
      <td>46.47</td>
      <td>48.22</td>
      <td>0.00</td>
      <td>2.02</td>
      <td>BloomModel</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.19</td>
      <td>9.0</td>
      <td>True</td>
      <td>20a19af481bf59f38610a2977b2b513e9df51e3a</td>
      <td>TurkuNLP/gpt3-finnish-small</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/roneneldan/TinyStories-3M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_roneneldan__TinyStories-3M</td>
      <td>24.18</td>
      <td>22.01</td>
      <td>25.58</td>
      <td>24.99</td>
      <td>47.33</td>
      <td>49.25</td>
      <td>0.00</td>
      <td>0.10</td>
      <td>GPTNeoForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.01</td>
      <td>2.0</td>
      <td>True</td>
      <td>cfaf26ec85ecdfc1bd7c2638104cce55cb67f894</td>
      <td>roneneldan/TinyStories-3M</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/cerebras/Cerebras-GPT-111M</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_cerebras__Cerebras-GPT-111M</td>
      <td>24.10</td>
      <td>20.22</td>
      <td>26.73</td>
      <td>25.51</td>
      <td>46.31</td>
      <td>47.75</td>
      <td>0.00</td>
      <td>2.14</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.11</td>
      <td>62.0</td>
      <td>True</td>
      <td>d2b54d7af419055f204690fe0385959616a1723e</td>
      <td>cerebras/Cerebras-GPT-111M</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/huashiyiqike/testmodel</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_huashiyiqike__testmodel</td>
      <td>24.04</td>
      <td>19.71</td>
      <td>26.68</td>
      <td>25.28</td>
      <td>43.72</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>2.69</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>0.15</td>
      <td>1.0</td>
      <td>True</td>
      <td>1ac5d244402e2433b6abfcff1fe65e84af15766b</td>
      <td>huashiyiqike/testmodel</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Corianas/111m</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Corianas__111m</td>
      <td>24.04</td>
      <td>19.71</td>
      <td>26.68</td>
      <td>25.28</td>
      <td>43.72</td>
      <td>50.20</td>
      <td>0.00</td>
      <td>2.69</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>cc-by-nc-sa-4.0</td>
      <td>0.15</td>
      <td>2.0</td>
      <td>True</td>
      <td>ee58d79e27f8b9e3984aab29235c5851d2be01d4</td>
      <td>Corianas/111m</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/databricks/dolly-v2-3b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_databricks__dolly-v2-3b</td>
      <td>20.31</td>
      <td>25.26</td>
      <td>26.55</td>
      <td>24.70</td>
      <td>0.00</td>
      <td>59.43</td>
      <td>1.06</td>
      <td>5.17</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>2.65</td>
      <td>232.0</td>
      <td>True</td>
      <td>f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df</td>
      <td>databricks/dolly-v2-3b</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/FabbriSimo01/Facebook_opt_1.3b_Quantized</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_FabbriSimo01__Facebook_opt_1.3b_Quantized</td>
      <td>19.40</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>59.67</td>
      <td>0.15</td>
      <td>5.11</td>
      <td>OPTForCausalLM</td>
      <td>torch.float16</td>
      <td>mit</td>
      <td>1.32</td>
      <td>0.0</td>
      <td>True</td>
      <td>7ef72ccee9d91d06967809e4e63ffbef62a9ad4a</td>
      <td>FabbriSimo01/Facebook_opt_1.3b_Quantized</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/jslin09/bloom-560m-finetuned-fraud</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_jslin09__bloom-560m-finetuned-fraud</td>
      <td>18.37</td>
      <td>26.96</td>
      <td>28.87</td>
      <td>24.03</td>
      <td>0.00</td>
      <td>48.38</td>
      <td>0.00</td>
      <td>0.33</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>bigscience-bloom-rail-1.0</td>
      <td>0.56</td>
      <td>1.0</td>
      <td>True</td>
      <td>5571f87f557b909e863005c6e3870bc2e77341a7</td>
      <td>jslin09/bloom-560m-finetuned-fraud</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/ahnyeonchan/OpenOrca-AYT-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_ahnyeonchan__OpenOrca-AYT-13B</td>
      <td>18.30</td>
      <td>27.22</td>
      <td>26.03</td>
      <td>25.11</td>
      <td>0.00</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.01</td>
      <td>LlamaForCausalLM</td>
      <td>torch.bfloat16</td>
      <td>llama2</td>
      <td>13.02</td>
      <td>0.0</td>
      <td>True</td>
      <td>1357abceda30e8389007a023907824cc3a11e397</td>
      <td>ahnyeonchan/OpenOrca-AYT-13B</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Andron00e/YetAnother_Open-Llama-3B-LoRA</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Andron00e__YetAnother_Open-Llama-3B-LoRA</td>
      <td>18.25</td>
      <td>25.94</td>
      <td>25.76</td>
      <td>24.65</td>
      <td>0.00</td>
      <td>51.38</td>
      <td>0.00</td>
      <td>0.05</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>52c5cb0178831908ed0571f1750fcb0f0fb125f9</td>
      <td>Andron00e/YetAnother_Open-Llama-3B-LoRA</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/Andron00e/YetAnother_Open-Llama-3B-LoRA-OpenOrca</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Andron00e__YetAnother_Open-Llama-3B-LoRA-OpenOrca</td>
      <td>18.18</td>
      <td>25.94</td>
      <td>25.76</td>
      <td>24.65</td>
      <td>0.00</td>
      <td>50.83</td>
      <td>0.00</td>
      <td>0.04</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>3.43</td>
      <td>0.0</td>
      <td>True</td>
      <td>07d9d32cd091148295d4e13802ba63486599aff4</td>
      <td>Andron00e/YetAnother_Open-Llama-3B-LoRA-OpenOrca</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Dampish/Dante-2.8B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Dampish__Dante-2.8B</td>
      <td>18.13</td>
      <td>25.09</td>
      <td>26.05</td>
      <td>24.51</td>
      <td>0.00</td>
      <td>51.07</td>
      <td>0.00</td>
      <td>0.17</td>
      <td>GPTNeoXForCausalLM</td>
      <td>torch.float16</td>
      <td>cc-by-nc-4.0</td>
      <td>2.65</td>
      <td>0.0</td>
      <td>True</td>
      <td>fb2a8f95c0286f957c830af640fd5c989081e8e4</td>
      <td>Dampish/Dante-2.8B</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/BreadAi/MuseCan</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_BreadAi__MuseCan</td>
      <td>18.06</td>
      <td>28.07</td>
      <td>25.00</td>
      <td>24.19</td>
      <td>0.00</td>
      <td>49.09</td>
      <td>0.00</td>
      <td>0.03</td>
      <td>GPT2LMHeadModel</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>0.07</td>
      <td>0.0</td>
      <td>True</td>
      <td>f441866d78feaead3dede6efd9e23990bb74c21e</td>
      <td>BreadAi/MuseCan</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/team-lucid/mptk-1b</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_team-lucid__mptk-1b</td>
      <td>17.88</td>
      <td>22.70</td>
      <td>25.48</td>
      <td>27.11</td>
      <td>0.00</td>
      <td>49.72</td>
      <td>0.00</td>
      <td>0.17</td>
      <td>MptForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>1.31</td>
      <td>0.0</td>
      <td>True</td>
      <td>aea467410ae0cead4fded6b98a3575e92b22862f</td>
      <td>team-lucid/mptk-1b</td>
    </tr>
    <tr>
      <td>RL-tuned</td>
      <td>https://huggingface.co/TheTravellingEngineer/bloom-1b1-RLHF-v2</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_TheTravellingEngineer__bloom-1b1-RLHF-v2</td>
      <td>17.20</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>BloomForCausalLM</td>
      <td>torch.float16</td>
      <td>?</td>
      <td>1.06</td>
      <td>0.0</td>
      <td>True</td>
      <td>05f7f0fd82fb3a5798d4bb284b6c10dd9d380f22</td>
      <td>TheTravellingEngineer/bloom-1b1-RLHF-v2</td>
    </tr>
    <tr>
      <td>pretrained</td>
      <td>https://huggingface.co/wtang06/mpt-125m-c4</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_wtang06__mpt-125m-c4</td>
      <td>17.20</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>MPTForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>0.12</td>
      <td>1.0</td>
      <td>True</td>
      <td>55f8f1874aa8bf4fc28c0abc92c7fbd1271ff7d7</td>
      <td>wtang06/mpt-125m-c4</td>
    </tr>
    <tr>
      <td></td>
      <td>https://huggingface.co/Rardilit/Panther_v1</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_Rardilit__Panther_v1</td>
      <td>17.20</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>other</td>
      <td>6.61</td>
      <td>1.0</td>
      <td>False</td>
      <td></td>
      <td>Rardilit/Panther_v1</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/dfurman/llama-2-13b-dolphin-peft</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_dfurman__llama-2-13b-dolphin-peft</td>
      <td>17.20</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>?</td>
      <td>torch.float16</td>
      <td>llama2</td>
      <td>13.00</td>
      <td>10.0</td>
      <td>False</td>
      <td>5d17f6b5f394f0745bd4377c8a1290c68051e351</td>
      <td>dfurman/llama-2-13b-dolphin-peft</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/clibrain/Llama-2-ft-instruct-es</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_clibrain__Llama-2-ft-instruct-es</td>
      <td>17.20</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>torch.float16</td>
      <td>apache-2.0</td>
      <td>6.61</td>
      <td>16.0</td>
      <td>True</td>
      <td>42f07d6a86fac5574febb7b8fa13c3b1e14fcebd</td>
      <td>clibrain/Llama-2-ft-instruct-es</td>
    </tr>
    <tr>
      <td>fine-tuned</td>
      <td>https://huggingface.co/aiplanet/panda-coder-13B</td>
      <td>https://huggingface.co/datasets/open-llm-leaderboard/details_aiplanet__panda-coder-13B</td>
      <td>17.20</td>
      <td>22.70</td>
      <td>25.04</td>
      <td>23.12</td>
      <td>0.00</td>
      <td>49.57</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>LlamaForCausalLM</td>
      <td>4bit</td>
      <td>apache-2.0</td>
      <td>12.85</td>
      <td>2.0</td>
      <td>True</td>
      <td>823a8320224cdac88e927aee00338ffa79395faa</td>
      <td>aiplanet/panda-coder-13B</td>
    </tr>
  </tbody>
</table>